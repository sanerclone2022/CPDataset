[{"authorTime":"2020-11-04 10:54:31","codes":[{"authorDate":"2020-11-04 10:54:31","commitOrder":1,"curCode":"\tpublic DynamicTableSource createDynamicTableSource(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\t\tDecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tDecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tString keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tProperties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\t\t\r\n\t\tStartupMode earliest = StartupMode.EARLIEST;\n\n\t\treturn new KafkaDynamicSource(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyDecodingFormat,\n\t\t\t\tnew DecodingFormatWrapper(valueDecodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\tKafkaOptions.getSourceTopics(tableOptions),\n\t\t\t\tKafkaOptions.getSourceTopicPattern(tableOptions),\n\t\t\t\tproperties,\n\t\t\t\tearliest,\n\t\t\t\tCollections.emptyMap(),\n\t\t\t\t0,\n\t\t\t\ttrue);\n\t}\n","date":"2020-11-05 21:08:26","endLine":131,"groupId":"48691","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/58/a94efd3f801d1033f12d9fc318a6ca82adbf97.src","preCode":"\tpublic DynamicTableSource createDynamicTableSource(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\t\tDecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tDecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tString keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tProperties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\t\t\r\n\t\tStartupMode earliest = StartupMode.EARLIEST;\n\n\t\treturn new KafkaDynamicSource(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyDecodingFormat,\n\t\t\t\tnew DecodingFormatWrapper(valueDecodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\tKafkaOptions.getSourceTopics(tableOptions),\n\t\t\t\tKafkaOptions.getSourceTopicPattern(tableOptions),\n\t\t\t\tproperties,\n\t\t\t\tearliest,\n\t\t\t\tCollections.emptyMap(),\n\t\t\t\t0,\n\t\t\t\ttrue);\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"B"},{"authorDate":"2020-11-04 10:54:31","commitOrder":1,"curCode":"\tpublic DynamicTableSink createDynamicTableSink(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tfinal String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n\t\t\r\n\t\t\r\n\t\treturn new KafkaDynamicSink(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyEncodingFormat,\n\t\t\t\tnew EncodingFormatWrapper(valueEncodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\ttableOptions.get(TOPIC).get(0),\n\t\t\t\tproperties,\n\t\t\t\tnull,\n\t\t\t\tKafkaSinkSemantic.AT_LEAST_ONCE,\n\t\t\t\ttrue\n\t\t);\n\t}\n","date":"2020-11-05 21:08:26","endLine":170,"groupId":"48691","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/58/a94efd3f801d1033f12d9fc318a6ca82adbf97.src","preCode":"\tpublic DynamicTableSink createDynamicTableSink(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tfinal String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n\t\t\r\n\t\t\r\n\t\treturn new KafkaDynamicSink(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyEncodingFormat,\n\t\t\t\tnew EncodingFormatWrapper(valueEncodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\ttableOptions.get(TOPIC).get(0),\n\t\t\t\tproperties,\n\t\t\t\tnull,\n\t\t\t\tKafkaSinkSemantic.AT_LEAST_ONCE,\n\t\t\t\ttrue\n\t\t);\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":134,"status":"B"}],"commitId":"9cc5e07c43fdae37c5f2afd617091931c92626d6","commitMessage":"@@@[FLINK-19858][upsert-kafka] Introduce the upsert-kafka table factory\n\nThis closes #13850\n","date":"2020-11-05 21:08:26","modifiedFileCount":"10","status":"B","submitter":"Shengkai"},{"authorTime":"2020-11-05 14:14:21","codes":[{"authorDate":"2020-11-04 10:54:31","commitOrder":2,"curCode":"\tpublic DynamicTableSource createDynamicTableSource(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\t\tDecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tDecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tString keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tProperties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\t\t\r\n\t\tStartupMode earliest = StartupMode.EARLIEST;\n\n\t\treturn new KafkaDynamicSource(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyDecodingFormat,\n\t\t\t\tnew DecodingFormatWrapper(valueDecodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\tKafkaOptions.getSourceTopics(tableOptions),\n\t\t\t\tKafkaOptions.getSourceTopicPattern(tableOptions),\n\t\t\t\tproperties,\n\t\t\t\tearliest,\n\t\t\t\tCollections.emptyMap(),\n\t\t\t\t0,\n\t\t\t\ttrue);\n\t}\n","date":"2020-11-05 21:08:26","endLine":131,"groupId":"48691","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/58/a94efd3f801d1033f12d9fc318a6ca82adbf97.src","preCode":"\tpublic DynamicTableSource createDynamicTableSource(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\t\tDecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tDecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tString keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tProperties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\t\t\r\n\t\tStartupMode earliest = StartupMode.EARLIEST;\n\n\t\treturn new KafkaDynamicSource(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyDecodingFormat,\n\t\t\t\tnew DecodingFormatWrapper(valueDecodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\tKafkaOptions.getSourceTopics(tableOptions),\n\t\t\t\tKafkaOptions.getSourceTopicPattern(tableOptions),\n\t\t\t\tproperties,\n\t\t\t\tearliest,\n\t\t\t\tCollections.emptyMap(),\n\t\t\t\t0,\n\t\t\t\ttrue);\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"N"},{"authorDate":"2020-11-05 14:14:21","commitOrder":2,"curCode":"\tpublic DynamicTableSink createDynamicTableSink(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tfinal String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n\t\tInteger parallelism = tableOptions.get(FactoryUtil.SINK_PARALLELISM);\n\n\t\t\r\n\t\t\r\n\t\treturn new KafkaDynamicSink(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyEncodingFormat,\n\t\t\t\tnew EncodingFormatWrapper(valueEncodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\ttableOptions.get(TOPIC).get(0),\n\t\t\t\tproperties,\n\t\t\t\tnull,\n\t\t\t\tKafkaSinkSemantic.AT_LEAST_ONCE,\n\t\t\t\ttrue,\n\t\t\t\tparallelism\n\t\t);\n\t}\n","date":"2020-11-05 21:08:28","endLine":174,"groupId":"49969","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/91/18f3ea0ae6a987b3cb78660b5f3fc0fc59c763.src","preCode":"\tpublic DynamicTableSink createDynamicTableSink(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat = helper.discoverEncodingFormat(\n\t\t\t\tSerializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tfinal String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n\t\t\r\n\t\t\r\n\t\treturn new KafkaDynamicSink(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyEncodingFormat,\n\t\t\t\tnew EncodingFormatWrapper(valueEncodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\ttableOptions.get(TOPIC).get(0),\n\t\t\t\tproperties,\n\t\t\t\tnull,\n\t\t\t\tKafkaSinkSemantic.AT_LEAST_ONCE,\n\t\t\t\ttrue\n\t\t);\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"M"}],"commitId":"f504bf576863442bab21f4293d763246cc5fe94e","commitMessage":"@@@[hotfix][upsert-kafka] Support sink parallelism on upsert-kafka sink\n","date":"2020-11-05 21:08:28","modifiedFileCount":"4","status":"M","submitter":"Jark Wu"},{"authorTime":"2021-01-15 22:42:20","codes":[{"authorDate":"2020-11-04 10:54:31","commitOrder":3,"curCode":"\tpublic DynamicTableSource createDynamicTableSource(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\t\tDecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tDecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tString keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tProperties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\t\t\r\n\t\tStartupMode earliest = StartupMode.EARLIEST;\n\n\t\treturn new KafkaDynamicSource(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyDecodingFormat,\n\t\t\t\tnew DecodingFormatWrapper(valueDecodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\tKafkaOptions.getSourceTopics(tableOptions),\n\t\t\t\tKafkaOptions.getSourceTopicPattern(tableOptions),\n\t\t\t\tproperties,\n\t\t\t\tearliest,\n\t\t\t\tCollections.emptyMap(),\n\t\t\t\t0,\n\t\t\t\ttrue);\n\t}\n","date":"2020-11-05 21:08:26","endLine":131,"groupId":"48691","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/58/a94efd3f801d1033f12d9fc318a6ca82adbf97.src","preCode":"\tpublic DynamicTableSource createDynamicTableSource(Context context) {\n\t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n\t\tReadableConfig tableOptions = helper.getOptions();\n\t\tDecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tKEY_FORMAT);\n\t\tDecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat = helper.discoverDecodingFormat(\n\t\t\t\tDeserializationFormatFactory.class,\n\t\t\t\tVALUE_FORMAT);\n\n\t\t\r\n\t\thelper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n\t\tTableSchema schema = context.getCatalogTable().getSchema();\n\t\tvalidateTableOptions(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n\t\tTuple2<int[], int[]> keyValueProjections = createKeyValueProjections(context.getCatalogTable());\n\t\tString keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n\t\tProperties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\t\t\r\n\t\tStartupMode earliest = StartupMode.EARLIEST;\n\n\t\treturn new KafkaDynamicSource(\n\t\t\t\tschema.toPhysicalRowDataType(),\n\t\t\t\tkeyDecodingFormat,\n\t\t\t\tnew DecodingFormatWrapper(valueDecodingFormat),\n\t\t\t\tkeyValueProjections.f0,\n\t\t\t\tkeyValueProjections.f1,\n\t\t\t\tkeyPrefix,\n\t\t\t\tKafkaOptions.getSourceTopics(tableOptions),\n\t\t\t\tKafkaOptions.getSourceTopicPattern(tableOptions),\n\t\t\t\tproperties,\n\t\t\t\tearliest,\n\t\t\t\tCollections.emptyMap(),\n\t\t\t\t0,\n\t\t\t\ttrue);\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"N"},{"authorDate":"2021-01-15 22:42:20","commitOrder":3,"curCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateTableOptions(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(FactoryUtil.SINK_PARALLELISM);\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                KafkaSinkSemantic.AT_LEAST_ONCE,\n                true,\n                parallelism);\n    }\n","date":"2021-01-15 22:42:20","endLine":173,"groupId":"49969","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/fb/92586fa669867d31be1bce159f75cb5489b409.src","preCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateTableOptions(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(FactoryUtil.SINK_PARALLELISM);\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                KafkaSinkSemantic.AT_LEAST_ONCE,\n                true,\n                parallelism);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":134,"status":"M"}],"commitId":"3786d3b1e55e063c7453d9813335dc5c2906bf7b","commitMessage":"@@@[FLINK-20348][kafka] Make \"schema-registry.subject\" optional for avro-confluent format when used with kafka\n\nThis closes #14530\n\nCo-authored-by: zhuxiaoshang <zhushang@qutoutiao.net>","date":"2021-01-15 22:42:20","modifiedFileCount":"7","status":"M","submitter":"zhuxiaoshang"},{"authorTime":"2021-03-30 22:40:31","codes":[{"authorDate":"2021-03-30 22:40:31","commitOrder":4,"curCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                KafkaOptions.getSourceTopics(tableOptions),\n                KafkaOptions.getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","date":"2021-04-01 10:16:59","endLine":136,"groupId":"0","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/1d/2860bc1f799738033d549944ae33e1ba413475.src","preCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateTableOptions(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                KafkaOptions.getSourceTopics(tableOptions),\n                KafkaOptions.getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"M"},{"authorDate":"2021-03-30 22:40:31","commitOrder":4,"curCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(FactoryUtil.SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                KafkaSinkSemantic.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism);\n    }\n","date":"2021-04-01 10:16:59","endLine":185,"groupId":"49969","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/1d/2860bc1f799738033d549944ae33e1ba413475.src","preCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateTableOptions(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(FactoryUtil.SINK_PARALLELISM);\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                KafkaSinkSemantic.AT_LEAST_ONCE,\n                true,\n                parallelism);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":139,"status":"M"}],"commitId":"ec9b0c5b60290697769415eb3e1b1ed2052460ac","commitMessage":"@@@[FLINK-21191][upsert-kafka] Support buffered sink function for upsert-kafka\n\nThis closes #15434\n","date":"2021-04-01 10:16:59","modifiedFileCount":"7","status":"M","submitter":"Shengkai"},{"authorTime":"2021-06-30 19:55:22","codes":[{"authorDate":"2021-06-30 19:55:22","commitOrder":5,"curCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                getSourceTopics(tableOptions),\n                getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","date":"2021-07-12 18:56:18","endLine":140,"groupId":"49105","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/01/2014c6eea6bda706cff7eb0123b7a53a80a71b.src","preCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                KafkaOptions.getSourceTopics(tableOptions),\n                KafkaOptions.getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":105,"status":"M"},{"authorDate":"2021-06-30 19:55:22","commitOrder":5,"curCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                KafkaSinkSemantic.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism);\n    }\n","date":"2021-07-12 18:56:18","endLine":189,"groupId":"49969","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/01/2014c6eea6bda706cff7eb0123b7a53a80a71b.src","preCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(KafkaOptions.PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(FactoryUtil.SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                KafkaSinkSemantic.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":143,"status":"M"}],"commitId":"5703e2f40150782943429092c8c3cb00ba719124","commitMessage":"@@@[FLINK-23064][connector-kafka] Expose connector options as PublicEvolving\n\nThis closes #16334.\n","date":"2021-07-12 18:56:18","modifiedFileCount":"4","status":"M","submitter":"Ingo B?rk"},{"authorTime":"2021-07-14 13:54:56","codes":[{"authorDate":"2021-06-30 19:55:22","commitOrder":6,"curCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                getSourceTopics(tableOptions),\n                getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","date":"2021-07-12 18:56:18","endLine":140,"groupId":"49105","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/01/2014c6eea6bda706cff7eb0123b7a53a80a71b.src","preCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                getSourceTopics(tableOptions),\n                getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":105,"status":"N"},{"authorDate":"2021-07-14 13:54:56","commitOrder":6,"curCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                SinkSemantic.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism);\n    }\n","date":"2021-07-26 22:20:08","endLine":190,"groupId":"49969","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/95/569e61cf9060d11177e27f3d2ef70aae3eeae2.src","preCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                KafkaSinkSemantic.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":144,"status":"M"}],"commitId":"abbc658f1d1898e63e7bb0d9bfdbbf42fc70ad8b","commitMessage":"@@@[FLINK-23369][connector-kafka] Use enums for options\n\nThis closes #16482.\n","date":"2021-07-26 22:20:08","modifiedFileCount":"8","status":"M","submitter":"Ingo B?rk"},{"authorTime":"2021-08-10 19:34:26","codes":[{"authorDate":"2021-06-30 19:55:22","commitOrder":7,"curCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                getSourceTopics(tableOptions),\n                getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","date":"2021-07-12 18:56:18","endLine":140,"groupId":"49105","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/01/2014c6eea6bda706cff7eb0123b7a53a80a71b.src","preCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                getSourceTopics(tableOptions),\n                getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":105,"status":"N"},{"authorDate":"2021-08-10 19:34:26","commitOrder":7,"curCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                DeliveryGuarantee.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism,\n                tableOptions.get(TRANSACTIONAL_ID_PREFIX));\n    }\n","date":"2021-08-14 00:05:49","endLine":192,"groupId":"49969","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/8c/e7ae322fb2733bb7cf67a3709192abbb5fd897.src","preCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                SinkSemantic.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":145,"status":"M"}],"commitId":"1948446eeda0bcd93471b2aad61a953b6a2e36e3","commitMessage":"@@@[FLINK-23639][connectors/kafka] Migrate Table API Kafka connector to use FLIP-143 KafkaSink\n","date":"2021-08-14 00:05:49","modifiedFileCount":"9","status":"M","submitter":"Fabian Paul"},{"authorTime":"2021-08-10 19:34:26","codes":[{"authorDate":"2021-08-16 11:28:39","commitOrder":8,"curCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                getSourceTopics(tableOptions),\n                getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true,\n                context.getObjectIdentifier().asSummaryString());\n    }\n","date":"2021-08-16 23:23:35","endLine":143,"groupId":"10319","id":15,"instanceNumber":1,"isCurCommit":1,"methodName":"createDynamicTableSource","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/6c/0d88e416ed7f0313f78d03a2bf0391a38eef01.src","preCode":"    public DynamicTableSource createDynamicTableSource(Context context) {\n        FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n\n        ReadableConfig tableOptions = helper.getOptions();\n        DecodingFormat<DeserializationSchema<RowData>> keyDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, KEY_FORMAT);\n        DecodingFormat<DeserializationSchema<RowData>> valueDecodingFormat =\n                helper.discoverDecodingFormat(DeserializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSource(tableOptions, keyDecodingFormat, valueDecodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n        \r\n        StartupMode earliest = StartupMode.EARLIEST;\n\n        return new KafkaDynamicSource(\n                schema.toPhysicalRowDataType(),\n                keyDecodingFormat,\n                new DecodingFormatWrapper(valueDecodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                getSourceTopics(tableOptions),\n                getSourceTopicPattern(tableOptions),\n                properties,\n                earliest,\n                Collections.emptyMap(),\n                0,\n                true);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":107,"status":"M"},{"authorDate":"2021-08-10 19:34:26","commitOrder":8,"curCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                DeliveryGuarantee.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism,\n                tableOptions.get(TRANSACTIONAL_ID_PREFIX));\n    }\n","date":"2021-08-14 00:05:49","endLine":192,"groupId":"10319","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"createDynamicTableSink","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/8c/e7ae322fb2733bb7cf67a3709192abbb5fd897.src","preCode":"    public DynamicTableSink createDynamicTableSink(Context context) {\n        FactoryUtil.TableFactoryHelper helper =\n                FactoryUtil.createTableFactoryHelper(\n                        this, autoCompleteSchemaRegistrySubject(context));\n\n        final ReadableConfig tableOptions = helper.getOptions();\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, KEY_FORMAT);\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                helper.discoverEncodingFormat(SerializationFormatFactory.class, VALUE_FORMAT);\n\n        \r\n        helper.validateExcept(PROPERTIES_PREFIX);\n        TableSchema schema = context.getCatalogTable().getSchema();\n        validateSink(tableOptions, keyEncodingFormat, valueEncodingFormat, schema);\n\n        Tuple2<int[], int[]> keyValueProjections =\n                createKeyValueProjections(context.getCatalogTable());\n        final String keyPrefix = tableOptions.getOptional(KEY_FIELDS_PREFIX).orElse(null);\n        final Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n\n        Integer parallelism = tableOptions.get(SINK_PARALLELISM);\n\n        int batchSize = tableOptions.get(SINK_BUFFER_FLUSH_MAX_ROWS);\n        Duration batchInterval = tableOptions.get(SINK_BUFFER_FLUSH_INTERVAL);\n        SinkBufferFlushMode flushMode =\n                new SinkBufferFlushMode(batchSize, batchInterval.toMillis());\n\n        \r\n        \r\n        return new KafkaDynamicSink(\n                schema.toPhysicalRowDataType(),\n                schema.toPhysicalRowDataType(),\n                keyEncodingFormat,\n                new EncodingFormatWrapper(valueEncodingFormat),\n                keyValueProjections.f0,\n                keyValueProjections.f1,\n                keyPrefix,\n                tableOptions.get(TOPIC).get(0),\n                properties,\n                null,\n                DeliveryGuarantee.AT_LEAST_ONCE,\n                true,\n                flushMode,\n                parallelism,\n                tableOptions.get(TRANSACTIONAL_ID_PREFIX));\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactory.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":145,"status":"N"}],"commitId":"4c8f3a7036f5aecfba88381c50b285566bcbcbd5","commitMessage":"@@@[FLINK-22914][connector/kafka] Use FLIP-27 KafkaSource in table connector\n\nThis closes #16809\n","date":"2021-08-16 23:23:35","modifiedFileCount":"5","status":"M","submitter":"Qingsheng Ren"}]
