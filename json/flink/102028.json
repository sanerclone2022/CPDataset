[{"authorTime":"2021-04-25 14:14:21","codes":[{"authorDate":"2021-05-12 09:35:57","commitOrder":2,"curCode":"    private String initNestedTypesFile(List<RowData> data) throws Exception {\n        LogicalType[] fieldTypes = new LogicalType[4];\n        fieldTypes[0] = new VarCharType();\n        fieldTypes[1] = new IntType();\n        List<RowType.RowField> arrayRowFieldList =\n                Collections.singletonList(new RowType.RowField(\"_col2_col0\", new VarCharType()));\n        fieldTypes[2] = new ArrayType(new RowType(arrayRowFieldList));\n        List<RowType.RowField> mapRowFieldList =\n                Arrays.asList(\n                        new RowType.RowField(\"_col3_col0\", new VarCharType()),\n                        new RowType.RowField(\"_col3_col1\", new TimestampType()));\n        fieldTypes[3] = new MapType(new VarCharType(), new RowType(mapRowFieldList));\n        String schema =\n                \"struct<_col0:string,_col1:int,_col2:array<struct<_col2_col0:string>>,\"\n                        + \"_col3:map<string,struct<_col3_col0:string,_col3_col1:timestamp>>>\";\n\n        File outDir = TEMPORARY_FOLDER.newFolder();\n        Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(\n                                new org.apache.flink.core.fs.Path(outDir.toURI()), writer)\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : data) {\n                testHarness.processElement(record, ++time);\n            }\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n        }\n        return outDir.getAbsolutePath();\n    }\n","date":"2021-06-04 17:45:08","endLine":331,"groupId":"29865","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"initNestedTypesFile","params":"(List<RowData>data)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/3c3bd489824655fa711d9af54e89a0fa1057ea.src","preCode":"    private String initNestedTypesFile(List<RowData> data) throws Exception {\n        LogicalType[] fieldTypes = new LogicalType[4];\n        fieldTypes[0] = new VarCharType();\n        fieldTypes[1] = new IntType();\n        List<RowType.RowField> arrayRowFieldList =\n                Collections.singletonList(new RowType.RowField(\"_col2_col0\", new VarCharType()));\n        fieldTypes[2] = new ArrayType(new RowType(arrayRowFieldList));\n        List<RowType.RowField> mapRowFieldList =\n                Arrays.asList(\n                        new RowType.RowField(\"_col3_col0\", new VarCharType()),\n                        new RowType.RowField(\"_col3_col1\", new TimestampType()));\n        fieldTypes[3] = new MapType(new VarCharType(), new RowType(mapRowFieldList));\n        String schema =\n                \"struct<_col0:string,_col1:int,_col2:array<struct<_col2_col0:string>>,\"\n                        + \"_col3:map<string,struct<_col3_col0:string,_col3_col1:timestamp>>>\";\n\n        File outDir = TEMPORARY_FOLDER.newFolder();\n        Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(\n                                new org.apache.flink.core.fs.Path(outDir.toURI()), writer)\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : data) {\n                testHarness.processElement(record, ++time);\n            }\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n        }\n        return outDir.getAbsolutePath();\n    }\n","realPath":"flink-formats/flink-orc/src/test/java/org/apache/flink/orc/OrcFileSystemITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":285,"status":"B"},{"authorDate":"2021-04-25 14:14:21","commitOrder":2,"curCode":"    public void testOrcBulkWriterWithRowData() throws Exception {\n        final File outDir = TEMPORARY_FOLDER.newFolder();\n        final Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(new Path(outDir.toURI()), writer)\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : input) {\n                testHarness.processElement(record, ++time);\n            }\n\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n\n            validate(outDir, input);\n        }\n    }\n","date":"2021-04-28 20:39:40","endLine":122,"groupId":"29865","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testOrcBulkWriterWithRowData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/6f/fa51e76202a259ea2e3296f6f7ce7ce4cd36ba.src","preCode":"    public void testOrcBulkWriterWithRowData() throws Exception {\n        final File outDir = TEMPORARY_FOLDER.newFolder();\n        final Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(new Path(outDir.toURI()), writer)\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : input) {\n                testHarness.processElement(record, ++time);\n            }\n\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n\n            validate(outDir, input);\n        }\n    }\n","realPath":"flink-formats/flink-orc/src/test/java/org/apache/flink/orc/writer/OrcBulkRowDataWriterTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":90,"status":"NB"}],"commitId":"6340c7b1a409f9ad2f4fee7318c5634515ff2459","commitMessage":"@@@[FLINK-15390][orc] List/Map/Struct types support for vectorized orc reader\n\nThis closes #15939\n","date":"2021-06-04 17:45:08","modifiedFileCount":"5","status":"M","submitter":"wangwei1025"},{"authorTime":"2021-08-25 10:43:17","codes":[{"authorDate":"2021-05-12 09:35:57","commitOrder":3,"curCode":"    private String initNestedTypesFile(List<RowData> data) throws Exception {\n        LogicalType[] fieldTypes = new LogicalType[4];\n        fieldTypes[0] = new VarCharType();\n        fieldTypes[1] = new IntType();\n        List<RowType.RowField> arrayRowFieldList =\n                Collections.singletonList(new RowType.RowField(\"_col2_col0\", new VarCharType()));\n        fieldTypes[2] = new ArrayType(new RowType(arrayRowFieldList));\n        List<RowType.RowField> mapRowFieldList =\n                Arrays.asList(\n                        new RowType.RowField(\"_col3_col0\", new VarCharType()),\n                        new RowType.RowField(\"_col3_col1\", new TimestampType()));\n        fieldTypes[3] = new MapType(new VarCharType(), new RowType(mapRowFieldList));\n        String schema =\n                \"struct<_col0:string,_col1:int,_col2:array<struct<_col2_col0:string>>,\"\n                        + \"_col3:map<string,struct<_col3_col0:string,_col3_col1:timestamp>>>\";\n\n        File outDir = TEMPORARY_FOLDER.newFolder();\n        Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(\n                                new org.apache.flink.core.fs.Path(outDir.toURI()), writer)\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : data) {\n                testHarness.processElement(record, ++time);\n            }\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n        }\n        return outDir.getAbsolutePath();\n    }\n","date":"2021-06-04 17:45:08","endLine":331,"groupId":"102028","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"initNestedTypesFile","params":"(List<RowData>data)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/3c3bd489824655fa711d9af54e89a0fa1057ea.src","preCode":"    private String initNestedTypesFile(List<RowData> data) throws Exception {\n        LogicalType[] fieldTypes = new LogicalType[4];\n        fieldTypes[0] = new VarCharType();\n        fieldTypes[1] = new IntType();\n        List<RowType.RowField> arrayRowFieldList =\n                Collections.singletonList(new RowType.RowField(\"_col2_col0\", new VarCharType()));\n        fieldTypes[2] = new ArrayType(new RowType(arrayRowFieldList));\n        List<RowType.RowField> mapRowFieldList =\n                Arrays.asList(\n                        new RowType.RowField(\"_col3_col0\", new VarCharType()),\n                        new RowType.RowField(\"_col3_col1\", new TimestampType()));\n        fieldTypes[3] = new MapType(new VarCharType(), new RowType(mapRowFieldList));\n        String schema =\n                \"struct<_col0:string,_col1:int,_col2:array<struct<_col2_col0:string>>,\"\n                        + \"_col3:map<string,struct<_col3_col0:string,_col3_col1:timestamp>>>\";\n\n        File outDir = TEMPORARY_FOLDER.newFolder();\n        Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(\n                                new org.apache.flink.core.fs.Path(outDir.toURI()), writer)\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : data) {\n                testHarness.processElement(record, ++time);\n            }\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n        }\n        return outDir.getAbsolutePath();\n    }\n","realPath":"flink-formats/flink-orc/src/test/java/org/apache/flink/orc/OrcFileSystemITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":285,"status":"N"},{"authorDate":"2021-08-25 10:43:17","commitOrder":3,"curCode":"    public void testOrcBulkWriterWithRowData() throws Exception {\n        final File outDir = TEMPORARY_FOLDER.newFolder();\n        final Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(new Path(outDir.toURI()), writer)\n                        .withBucketAssigner(new UniqueBucketAssigner<>(\"test\"))\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : input) {\n                testHarness.processElement(record, ++time);\n            }\n\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n\n            validate(outDir, input);\n        }\n    }\n","date":"2021-08-25 19:29:37","endLine":124,"groupId":"102028","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testOrcBulkWriterWithRowData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/60/7fe152ee259b1fccbce3009178d8d695141232.src","preCode":"    public void testOrcBulkWriterWithRowData() throws Exception {\n        final File outDir = TEMPORARY_FOLDER.newFolder();\n        final Properties writerProps = new Properties();\n        writerProps.setProperty(\"orc.compress\", \"LZ4\");\n\n        final OrcBulkWriterFactory<RowData> writer =\n                new OrcBulkWriterFactory<>(\n                        new RowDataVectorizer(schema, fieldTypes),\n                        writerProps,\n                        new Configuration());\n\n        StreamingFileSink<RowData> sink =\n                StreamingFileSink.forBulkFormat(new Path(outDir.toURI()), writer)\n                        .withBucketCheckInterval(10000)\n                        .build();\n\n        try (OneInputStreamOperatorTestHarness<RowData, Object> testHarness =\n                new OneInputStreamOperatorTestHarness<>(new StreamSink<>(sink), 1, 1, 0)) {\n\n            testHarness.setup();\n            testHarness.open();\n\n            int time = 0;\n            for (final RowData record : input) {\n                testHarness.processElement(record, ++time);\n            }\n\n            testHarness.snapshot(1, ++time);\n            testHarness.notifyOfCompletedCheckpoint(1);\n\n            validate(outDir, input);\n        }\n    }\n","realPath":"flink-formats/flink-orc/src/test/java/org/apache/flink/orc/writer/OrcBulkRowDataWriterTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"}],"commitId":"5390e91bd47219adde15d5d515a4f5baf4231fc2","commitMessage":"@@@[FLINK-22710][formats] Explicitly set the bucket assigner to avoid writing into two buckets in the tests\n\nThis closes #16950.\n","date":"2021-08-25 19:29:37","modifiedFileCount":"9","status":"M","submitter":"Yun Gao"}]
