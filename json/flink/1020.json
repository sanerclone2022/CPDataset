[{"authorTime":"2016-10-07 06:21:42","codes":[{"authorDate":"2016-10-07 06:21:42","commitOrder":3,"curCode":"\tprivate Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (hdfs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = hdfs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor (int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx + \": \" + sampleLine + \" \" + i + \"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes());\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","date":"2016-10-27 20:22:02","endLine":301,"groupId":"47299","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"fillWithData","params":"(Stringbase@StringfileName@intfileIdx@StringsampleLine)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/32/11a208a36275137bfe88099bfd6873d33ca246.src","preCode":"\tprivate Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (hdfs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = hdfs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor (int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx + \": \" + sampleLine + \" \" + i + \"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes());\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":284,"status":"B"},{"authorDate":"2016-10-07 06:21:42","commitOrder":3,"curCode":"\tprivate Tuple2<Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (localFs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = localFs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor(int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx +\": \"+ sampleLine + \" \" + i +\"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes());\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","date":"2016-10-27 20:22:02","endLine":345,"groupId":"47299","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"fillWithData","params":"(Stringbase@StringfileName@intfileIdx@StringsampleLine)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0e/9b054af64072175ff2bccd908abd7131293384.src","preCode":"\tprivate Tuple2<Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (localFs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = localFs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor(int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx +\": \"+ sampleLine + \" \" + i +\"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes());\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/ContinuousFileProcessingCheckpointITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":328,"status":"MB"}],"commitId":"b410c393c960f55c09fadd4f22732d06f801b938","commitMessage":"@@@[FLINK-4800] Introduce the TimestampedFileInputSplit for Continuous File Processing\n\nThis commit mainly introduces the TimestampedFileInputSplit. \nwhich extends the class FileInputSplit and also contains:\ni) the modification time of the file it belongs to and also.  and\nii) when checkpointing.  the point the reader is currently reading\n    from in the split the reader.\n\nThis will be useful for rescaling. With this addition.  the\nContinuousFileMonitoringFunction sends TimestampedFileInputSplit\nto the Readers.  and the Readers' state now contain only\nTimestampedFileInputSplit.\n\nIn addition.  it refactors the code of the ContinuousFileMonitoringFunction\nand that of the ContinuousFileReaderOperator along with the related\ntests.\n\nThis closes #2618.\n","date":"2016-10-27 20:22:02","modifiedFileCount":"5","status":"M","submitter":"kl0u"},{"authorTime":"2017-03-03 20:24:49","codes":[{"authorDate":"2017-03-03 20:24:49","commitOrder":4,"curCode":"\tprivate Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (hdfs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = hdfs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor (int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx + \": \" + sampleLine + \" \" + i + \"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","date":"2017-03-09 20:00:55","endLine":302,"groupId":"1020","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"fillWithData","params":"(Stringbase@StringfileName@intfileIdx@StringsampleLine)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/bc/42838c00c68c751b6be596ae9b44e137ecf5d0.src","preCode":"\tprivate Tuple2<org.apache.hadoop.fs.Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (hdfs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = hdfs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor (int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx + \": \" + sampleLine + \" \" + i + \"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes());\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":285,"status":"M"},{"authorDate":"2017-03-03 20:24:49","commitOrder":4,"curCode":"\tprivate Tuple2<Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (localFs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = localFs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor(int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx +\": \"+ sampleLine + \" \" + i +\"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","date":"2017-03-09 20:00:55","endLine":348,"groupId":"1020","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"fillWithData","params":"(Stringbase@StringfileName@intfileIdx@StringsampleLine)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0a/d42bb522b4c204780709942dc0f45afeea03f1.src","preCode":"\tprivate Tuple2<Path, String> fillWithData(\n\t\tString base, String fileName, int fileIdx, String sampleLine) throws IOException, InterruptedException {\n\n\t\tassert (localFs != null);\n\n\t\torg.apache.hadoop.fs.Path tmp =\n\t\t\tnew org.apache.hadoop.fs.Path(base + \"/.\" + fileName + fileIdx);\n\n\t\tFSDataOutputStream stream = localFs.create(tmp);\n\t\tStringBuilder str = new StringBuilder();\n\t\tfor(int i = 0; i < LINES_PER_FILE; i++) {\n\t\t\tString line = fileIdx +\": \"+ sampleLine + \" \" + i +\"\\n\";\n\t\t\tstr.append(line);\n\t\t\tstream.write(line.getBytes());\n\t\t}\n\t\tstream.close();\n\t\treturn new Tuple2<>(tmp, str.toString());\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/ContinuousFileProcessingCheckpointITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":331,"status":"M"}],"commitId":"53fedbd2894c6c7b839d8fdcc0dbf1e6e21e631a","commitMessage":"@@@[FLINK-5824] Fix String/byte conversions without explicit encoding\n\nThis closes #3468\n","date":"2017-03-09 20:00:55","modifiedFileCount":"70","status":"M","submitter":"Dawid Wysakowicz"}]
