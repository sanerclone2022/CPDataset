[{"authorTime":"2019-12-03 17:21:29","codes":[{"authorDate":"2020-03-09 12:05:53","commitOrder":4,"curCode":"\tpublic HiveVectorizedParquetSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\thiveVersion.startsWith(\"3\"),\n\t\t\t\tconf,\n\t\t\t\tfieldNames,\n\t\t\t\tfieldTypes,\n\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\tselectedFields,\n\t\t\t\tDEFAULT_SIZE,\n\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\tfileSplit.getStart(),\n\t\t\t\tfileSplit.getLength());\n\t}\n","date":"2020-03-09 12:05:53","endLine":75,"groupId":"1991","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"HiveVectorizedParquetSplitReader","params":"(StringhiveVersion@JobConfjobConf@String[]fieldNames@DataType[]fieldTypes@int[]selectedFields@HiveTableInputSplitsplit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cd/119475b37198d41431ed20b5b98a0ed3ed9b5a.src","preCode":"\tpublic HiveVectorizedParquetSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\thiveVersion.startsWith(\"3\"),\n\t\t\t\tconf,\n\t\t\t\tfieldNames,\n\t\t\t\tfieldTypes,\n\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\tselectedFields,\n\t\t\t\tDEFAULT_SIZE,\n\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\tfileSplit.getStart(),\n\t\t\t\tfileSplit.getLength());\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveVectorizedParquetSplitReader.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"B"},{"authorDate":"2019-12-03 17:21:29","commitOrder":4,"curCode":"\tpublic HiveVectorizedOrcSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = hiveVersion.startsWith(\"1.\") ?\n\t\t\t\tOrcNoHiveSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength()) :\n\t\t\t\tOrcSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\thiveVersion,\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength());\n\t}\n","date":"2020-02-15 14:07:13","endLine":90,"groupId":"52581","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"HiveVectorizedOrcSplitReader","params":"(StringhiveVersion@JobConfjobConf@String[]fieldNames@DataType[]fieldTypes@int[]selectedFields@HiveTableInputSplitsplit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/29/259d9e3f6f5e6ef018fdd0a409ac5f789474cb.src","preCode":"\tpublic HiveVectorizedOrcSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = hiveVersion.startsWith(\"1.\") ?\n\t\t\t\tOrcNoHiveSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength()) :\n\t\t\t\tOrcSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\thiveVersion,\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength());\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveVectorizedOrcSplitReader.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"NB"}],"commitId":"a68f5fde472de8a351726239a936ca2acf8073a3","commitMessage":"@@@[FLINK-16450][hive] Integrate parquet columnar row reader to hive\n\n\nThis closes #11327","date":"2020-03-09 12:05:53","modifiedFileCount":"2","status":"M","submitter":"Jingsong Lee"},{"authorTime":"2019-12-03 17:21:29","codes":[{"authorDate":"2020-05-21 16:21:22","commitOrder":5,"curCode":"\tpublic HiveVectorizedParquetSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\thiveVersion.startsWith(\"3\"),\n\t\t\t\tfalse, \r\n\t\t\t\tconf,\n\t\t\t\tfieldNames,\n\t\t\t\tfieldTypes,\n\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\tselectedFields,\n\t\t\t\tDEFAULT_SIZE,\n\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\tfileSplit.getStart(),\n\t\t\t\tfileSplit.getLength());\n\t}\n","date":"2020-05-21 16:21:22","endLine":76,"groupId":"23572","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"HiveVectorizedParquetSplitReader","params":"(StringhiveVersion@JobConfjobConf@String[]fieldNames@DataType[]fieldTypes@int[]selectedFields@HiveTableInputSplitsplit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/78/23b713d4faa8b70272b1648bbf63a1838dbf6e.src","preCode":"\tpublic HiveVectorizedParquetSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\thiveVersion.startsWith(\"3\"),\n\t\t\t\tconf,\n\t\t\t\tfieldNames,\n\t\t\t\tfieldTypes,\n\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\tselectedFields,\n\t\t\t\tDEFAULT_SIZE,\n\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\tfileSplit.getStart(),\n\t\t\t\tfileSplit.getLength());\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveVectorizedParquetSplitReader.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"M"},{"authorDate":"2019-12-03 17:21:29","commitOrder":5,"curCode":"\tpublic HiveVectorizedOrcSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = hiveVersion.startsWith(\"1.\") ?\n\t\t\t\tOrcNoHiveSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength()) :\n\t\t\t\tOrcSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\thiveVersion,\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength());\n\t}\n","date":"2020-02-15 14:07:13","endLine":90,"groupId":"52581","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"HiveVectorizedOrcSplitReader","params":"(StringhiveVersion@JobConfjobConf@String[]fieldNames@DataType[]fieldTypes@int[]selectedFields@HiveTableInputSplitsplit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/29/259d9e3f6f5e6ef018fdd0a409ac5f789474cb.src","preCode":"\tpublic HiveVectorizedOrcSplitReader(\n\t\t\tString hiveVersion,\n\t\t\tJobConf jobConf,\n\t\t\tString[] fieldNames,\n\t\t\tDataType[] fieldTypes,\n\t\t\tint[] selectedFields,\n\t\t\tHiveTableInputSplit split) throws IOException {\n\t\tStorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n\t\tConfiguration conf = new Configuration(jobConf);\n\t\tsd.getSerdeInfo().getParameters().forEach(conf::set);\n\n\t\tInputSplit hadoopSplit = split.getHadoopInputSplit();\n\t\tFileSplit fileSplit;\n\t\tif (hadoopSplit instanceof FileSplit) {\n\t\t\tfileSplit = (FileSplit) hadoopSplit;\n\t\t} else {\n\t\t\tthrow new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n\t\t}\n\n\t\tthis.reader = hiveVersion.startsWith(\"1.\") ?\n\t\t\t\tOrcNoHiveSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength()) :\n\t\t\t\tOrcSplitReaderUtil.genPartColumnarRowReader(\n\t\t\t\t\t\thiveVersion,\n\t\t\t\t\t\tconf,\n\t\t\t\t\t\tfieldNames,\n\t\t\t\t\t\tfieldTypes,\n\t\t\t\t\t\tsplit.getHiveTablePartition().getPartitionSpec(),\n\t\t\t\t\t\tselectedFields,\n\t\t\t\t\t\tnew ArrayList<>(),\n\t\t\t\t\t\tDEFAULT_SIZE,\n\t\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n\t\t\t\t\t\tfileSplit.getStart(),\n\t\t\t\t\t\tfileSplit.getLength());\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveVectorizedOrcSplitReader.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"N"}],"commitId":"4d1a2f5da980f79b62dacf836e32ad1487a78eb0","commitMessage":"@@@[FLINK-17474][parquet][hive] Parquet reader should be case insensitive for hive\n\n\nThis closes #12241","date":"2020-05-21 16:21:22","modifiedFileCount":"8","status":"M","submitter":"Jingsong Lee"},{"authorTime":"2021-03-22 21:57:57","codes":[{"authorDate":"2021-03-22 21:57:57","commitOrder":6,"curCode":"    public HiveVectorizedParquetSplitReader(\n            String hiveVersion,\n            JobConf jobConf,\n            String[] fieldNames,\n            DataType[] fieldTypes,\n            int[] selectedFields,\n            HiveTableInputSplit split)\n            throws IOException {\n        StorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n        Configuration conf = new Configuration(jobConf);\n        sd.getSerdeInfo().getParameters().forEach(conf::set);\n\n        InputSplit hadoopSplit = split.getHadoopInputSplit();\n        FileSplit fileSplit;\n        if (hadoopSplit instanceof FileSplit) {\n            fileSplit = (FileSplit) hadoopSplit;\n        } else {\n            throw new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n        }\n\n        Map<String, Object> partitionValues =\n                HivePartitionUtils.parsePartitionValues(\n                        split.getHiveTablePartition().getPartitionSpec(),\n                        fieldNames,\n                        fieldTypes,\n                        JobConfUtils.getDefaultPartitionName(jobConf),\n                        HiveShimLoader.loadHiveShim(hiveVersion));\n        this.reader =\n                ParquetSplitReaderUtil.genPartColumnarRowReader(\n                        hiveVersion.startsWith(\"3\"),\n                        false, \r\n                        conf,\n                        fieldNames,\n                        fieldTypes,\n                        partitionValues,\n                        selectedFields,\n                        DEFAULT_SIZE,\n                        new Path(fileSplit.getPath().toString()),\n                        fileSplit.getStart(),\n                        fileSplit.getLength());\n    }\n","date":"2021-03-22 21:57:57","endLine":87,"groupId":"101140","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"HiveVectorizedParquetSplitReader","params":"(StringhiveVersion@JobConfjobConf@String[]fieldNames@DataType[]fieldTypes@int[]selectedFields@HiveTableInputSplitsplit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/c0/fd8e858a61958fdda533ee8c745ff30d56dedd.src","preCode":"    public HiveVectorizedParquetSplitReader(\n            String hiveVersion,\n            JobConf jobConf,\n            String[] fieldNames,\n            DataType[] fieldTypes,\n            int[] selectedFields,\n            HiveTableInputSplit split)\n            throws IOException {\n        StorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n        Configuration conf = new Configuration(jobConf);\n        sd.getSerdeInfo().getParameters().forEach(conf::set);\n\n        InputSplit hadoopSplit = split.getHadoopInputSplit();\n        FileSplit fileSplit;\n        if (hadoopSplit instanceof FileSplit) {\n            fileSplit = (FileSplit) hadoopSplit;\n        } else {\n            throw new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n        }\n\n        this.reader =\n                ParquetSplitReaderUtil.genPartColumnarRowReader(\n                        hiveVersion.startsWith(\"3\"),\n                        false, \r\n                        conf,\n                        fieldNames,\n                        fieldTypes,\n                        split.getHiveTablePartition().getPartitionSpec(),\n                        selectedFields,\n                        DEFAULT_SIZE,\n                        new Path(fileSplit.getPath().toString()),\n                        fileSplit.getStart(),\n                        fileSplit.getLength());\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveVectorizedParquetSplitReader.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"},{"authorDate":"2021-03-22 21:57:57","commitOrder":6,"curCode":"    public HiveVectorizedOrcSplitReader(\n            String hiveVersion,\n            JobConf jobConf,\n            String[] fieldNames,\n            DataType[] fieldTypes,\n            int[] selectedFields,\n            HiveTableInputSplit split)\n            throws IOException {\n        StorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n        Configuration conf = new Configuration(jobConf);\n        sd.getSerdeInfo().getParameters().forEach(conf::set);\n\n        InputSplit hadoopSplit = split.getHadoopInputSplit();\n        FileSplit fileSplit;\n        if (hadoopSplit instanceof FileSplit) {\n            fileSplit = (FileSplit) hadoopSplit;\n        } else {\n            throw new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n        }\n\n        Map<String, Object> partitionValues =\n                HivePartitionUtils.parsePartitionValues(\n                        split.getHiveTablePartition().getPartitionSpec(),\n                        fieldNames,\n                        fieldTypes,\n                        JobConfUtils.getDefaultPartitionName(jobConf),\n                        HiveShimLoader.loadHiveShim(hiveVersion));\n        this.reader =\n                hiveVersion.startsWith(\"1.\")\n                        ? OrcNoHiveSplitReaderUtil.genPartColumnarRowReader(\n                                conf,\n                                fieldNames,\n                                fieldTypes,\n                                partitionValues,\n                                selectedFields,\n                                new ArrayList<>(),\n                                DEFAULT_SIZE,\n                                new Path(fileSplit.getPath().toString()),\n                                fileSplit.getStart(),\n                                fileSplit.getLength())\n                        : OrcSplitReaderUtil.genPartColumnarRowReader(\n                                hiveVersion,\n                                conf,\n                                fieldNames,\n                                fieldTypes,\n                                partitionValues,\n                                selectedFields,\n                                new ArrayList<>(),\n                                DEFAULT_SIZE,\n                                new Path(fileSplit.getPath().toString()),\n                                fileSplit.getStart(),\n                                fileSplit.getLength());\n    }\n","date":"2021-03-22 21:57:57","endLine":101,"groupId":"101140","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"HiveVectorizedOrcSplitReader","params":"(StringhiveVersion@JobConfjobConf@String[]fieldNames@DataType[]fieldTypes@int[]selectedFields@HiveTableInputSplitsplit)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cf/de044bc4248bfb1a9117bab898b35719785048.src","preCode":"    public HiveVectorizedOrcSplitReader(\n            String hiveVersion,\n            JobConf jobConf,\n            String[] fieldNames,\n            DataType[] fieldTypes,\n            int[] selectedFields,\n            HiveTableInputSplit split)\n            throws IOException {\n        StorageDescriptor sd = split.getHiveTablePartition().getStorageDescriptor();\n\n        Configuration conf = new Configuration(jobConf);\n        sd.getSerdeInfo().getParameters().forEach(conf::set);\n\n        InputSplit hadoopSplit = split.getHadoopInputSplit();\n        FileSplit fileSplit;\n        if (hadoopSplit instanceof FileSplit) {\n            fileSplit = (FileSplit) hadoopSplit;\n        } else {\n            throw new IllegalArgumentException(\"Unknown split type: \" + hadoopSplit);\n        }\n\n        this.reader =\n                hiveVersion.startsWith(\"1.\")\n                        ? OrcNoHiveSplitReaderUtil.genPartColumnarRowReader(\n                                conf,\n                                fieldNames,\n                                fieldTypes,\n                                split.getHiveTablePartition().getPartitionSpec(),\n                                selectedFields,\n                                new ArrayList<>(),\n                                DEFAULT_SIZE,\n                                new Path(fileSplit.getPath().toString()),\n                                fileSplit.getStart(),\n                                fileSplit.getLength())\n                        : OrcSplitReaderUtil.genPartColumnarRowReader(\n                                hiveVersion,\n                                conf,\n                                fieldNames,\n                                fieldTypes,\n                                split.getHiveTablePartition().getPartitionSpec(),\n                                selectedFields,\n                                new ArrayList<>(),\n                                DEFAULT_SIZE,\n                                new Path(fileSplit.getPath().toString()),\n                                fileSplit.getStart(),\n                                fileSplit.getLength());\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveVectorizedOrcSplitReader.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":48,"status":"M"}],"commitId":"c0cf91ce6fbe55f9f1df1fb1626a50e79cb9fc50","commitMessage":"@@@[FLINK-21894][hive] Update type of HiveTablePartition#partitionSpec from Map<String.  Object> to Map<String.  String>\n\nThis closes #15308","date":"2021-03-22 21:57:57","modifiedFileCount":"12","status":"M","submitter":"Jark Wu"}]
