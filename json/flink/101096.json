[{"authorTime":"2021-03-29 10:49:30","codes":[{"authorDate":"2021-03-29 10:49:30","commitOrder":1,"curCode":"    private static NotNullConstraint processNotNull(\n            HiveParserASTNode node, String dbName, String tblName, String colName)\n            throws SemanticException {\n        boolean enable = true;\n        boolean validate = false;\n        boolean rely = false;\n        for (int i = 0; i < node.getChildCount(); i++) {\n            HiveParserASTNode child = (HiveParserASTNode) node.getChild(i);\n            switch (child.getToken().getType()) {\n                case HiveASTParser.TOK_ENABLE:\n                case HiveASTParser.TOK_NOVALIDATE:\n                case HiveASTParser.TOK_NORELY:\n                    break;\n                case HiveASTParser.TOK_DISABLE:\n                    enable = false;\n                    break;\n                case HiveASTParser.TOK_VALIDATE:\n                    validate = true;\n                    break;\n                case HiveASTParser.TOK_RELY:\n                    rely = true;\n                    break;\n                default:\n                    throw new SemanticException(\n                            \"Unexpected node for NOT NULL constraint: \" + child);\n            }\n        }\n        return new NotNullConstraint(dbName, tblName, colName, null, enable, validate, rely);\n    }\n","date":"2021-03-30 13:48:32","endLine":249,"groupId":"913","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"processNotNull","params":"(HiveParserASTNodenode@StringdbName@StringtblName@StringcolName)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/72/093088e39060bae13c5ea31d838c506b003b0f.src","preCode":"    private static NotNullConstraint processNotNull(\n            HiveParserASTNode node, String dbName, String tblName, String colName)\n            throws SemanticException {\n        boolean enable = true;\n        boolean validate = false;\n        boolean rely = false;\n        for (int i = 0; i < node.getChildCount(); i++) {\n            HiveParserASTNode child = (HiveParserASTNode) node.getChild(i);\n            switch (child.getToken().getType()) {\n                case HiveASTParser.TOK_ENABLE:\n                case HiveASTParser.TOK_NOVALIDATE:\n                case HiveASTParser.TOK_NORELY:\n                    break;\n                case HiveASTParser.TOK_DISABLE:\n                    enable = false;\n                    break;\n                case HiveASTParser.TOK_VALIDATE:\n                    validate = true;\n                    break;\n                case HiveASTParser.TOK_RELY:\n                    rely = true;\n                    break;\n                default:\n                    throw new SemanticException(\n                            \"Unexpected node for NOT NULL constraint: \" + child);\n            }\n        }\n        return new NotNullConstraint(dbName, tblName, colName, null, enable, validate, rely);\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/copy/HiveParserBaseSemanticAnalyzer.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":221,"status":"B"},{"authorDate":"2021-03-29 10:49:30","commitOrder":1,"curCode":"    private static void processPrimaryKeyInfos(HiveParserASTNode child, List<PKInfo> pkInfos)\n            throws SemanticException {\n        if (child.getChildCount() < 4) {\n            throw new SemanticException(\"Invalid Primary Key syntax\");\n        }\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        boolean userSpecifiedConstraintName = child.getChildCount() == 5;\n        int relyIndex = child.getChildCount() == 5 ? 2 : 1;\n        for (int j = 0; j < child.getChild(0).getChildCount(); j++) {\n            Tree grandChild = child.getChild(0).getChild(j);\n            boolean rely = child.getChild(relyIndex).getType() == HiveASTParser.TOK_VALIDATE;\n            boolean enable = child.getChild(relyIndex + 1).getType() == HiveASTParser.TOK_ENABLE;\n            boolean validate =\n                    child.getChild(relyIndex + 2).getType() == HiveASTParser.TOK_VALIDATE;\n            if (enable) {\n                throw new SemanticException(\n                        \"Invalid Primary Key syntax ENABLE feature not supported yet\");\n            }\n            if (validate) {\n                throw new SemanticException(\n                        \"Invalid Primary Key syntax VALIDATE feature not supported yet\");\n            }\n            checkColumnName(grandChild.getText());\n            pkInfos.add(\n                    new PKInfo(\n                            unescapeIdentifier(grandChild.getText().toLowerCase()),\n                            (userSpecifiedConstraintName\n                                    ? unescapeIdentifier(child.getChild(1).getText().toLowerCase())\n                                    : null),\n                            rely));\n        }\n    }\n","date":"2021-03-30 13:48:32","endLine":320,"groupId":"106","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"processPrimaryKeyInfos","params":"(HiveParserASTNodechild@List<PKInfo>pkInfos)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/72/093088e39060bae13c5ea31d838c506b003b0f.src","preCode":"    private static void processPrimaryKeyInfos(HiveParserASTNode child, List<PKInfo> pkInfos)\n            throws SemanticException {\n        if (child.getChildCount() < 4) {\n            throw new SemanticException(\"Invalid Primary Key syntax\");\n        }\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        boolean userSpecifiedConstraintName = child.getChildCount() == 5;\n        int relyIndex = child.getChildCount() == 5 ? 2 : 1;\n        for (int j = 0; j < child.getChild(0).getChildCount(); j++) {\n            Tree grandChild = child.getChild(0).getChild(j);\n            boolean rely = child.getChild(relyIndex).getType() == HiveASTParser.TOK_VALIDATE;\n            boolean enable = child.getChild(relyIndex + 1).getType() == HiveASTParser.TOK_ENABLE;\n            boolean validate =\n                    child.getChild(relyIndex + 2).getType() == HiveASTParser.TOK_VALIDATE;\n            if (enable) {\n                throw new SemanticException(\n                        \"Invalid Primary Key syntax ENABLE feature not supported yet\");\n            }\n            if (validate) {\n                throw new SemanticException(\n                        \"Invalid Primary Key syntax VALIDATE feature not supported yet\");\n            }\n            checkColumnName(grandChild.getText());\n            pkInfos.add(\n                    new PKInfo(\n                            unescapeIdentifier(grandChild.getText().toLowerCase()),\n                            (userSpecifiedConstraintName\n                                    ? unescapeIdentifier(child.getChild(1).getText().toLowerCase())\n                                    : null),\n                            rely));\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/copy/HiveParserBaseSemanticAnalyzer.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":279,"status":"B"}],"commitId":"4bc28730b4dd0d1cb83441b80fa5e267fd447f32","commitMessage":"@@@[FLINK-21998][hive] Move copied hive classes to a separate package\n","date":"2021-03-30 13:48:32","modifiedFileCount":"9","status":"B","submitter":"Rui Li"},{"authorTime":"2021-03-30 21:56:18","codes":[{"authorDate":"2021-03-30 21:56:18","commitOrder":2,"curCode":"    private static NotNullConstraint processNotNull(\n            HiveParserASTNode nnNode, String dbName, String tblName, String colName)\n            throws SemanticException {\n        boolean enable = true;\n        boolean validate = false;\n        boolean rely = false;\n        for (int i = 0; i < nnNode.getChildCount(); i++) {\n            HiveParserASTNode child = (HiveParserASTNode) nnNode.getChild(i);\n            switch (child.getToken().getType()) {\n                case HiveASTParser.TOK_ENABLE:\n                case HiveASTParser.TOK_NOVALIDATE:\n                case HiveASTParser.TOK_NORELY:\n                    break;\n                case HiveASTParser.TOK_DISABLE:\n                    enable = false;\n                    break;\n                case HiveASTParser.TOK_VALIDATE:\n                    validate = true;\n                    break;\n                case HiveASTParser.TOK_RELY:\n                    rely = true;\n                    break;\n                default:\n                    throw new SemanticException(\n                            \"Unexpected node for NOT NULL constraint: \" + child);\n            }\n        }\n        return new NotNullConstraint(dbName, tblName, colName, null, enable, validate, rely);\n    }\n","date":"2021-04-01 10:31:50","endLine":309,"groupId":"101096","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"processNotNull","params":"(HiveParserASTNodennNode@StringdbName@StringtblName@StringcolName)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/2c/4d3cb60c8fa9d88ff7c3ee24fd2ed460dd947d.src","preCode":"    private static NotNullConstraint processNotNull(\n            HiveParserASTNode node, String dbName, String tblName, String colName)\n            throws SemanticException {\n        boolean enable = true;\n        boolean validate = false;\n        boolean rely = false;\n        for (int i = 0; i < node.getChildCount(); i++) {\n            HiveParserASTNode child = (HiveParserASTNode) node.getChild(i);\n            switch (child.getToken().getType()) {\n                case HiveASTParser.TOK_ENABLE:\n                case HiveASTParser.TOK_NOVALIDATE:\n                case HiveASTParser.TOK_NORELY:\n                    break;\n                case HiveASTParser.TOK_DISABLE:\n                    enable = false;\n                    break;\n                case HiveASTParser.TOK_VALIDATE:\n                    validate = true;\n                    break;\n                case HiveASTParser.TOK_RELY:\n                    rely = true;\n                    break;\n                default:\n                    throw new SemanticException(\n                            \"Unexpected node for NOT NULL constraint: \" + child);\n            }\n        }\n        return new NotNullConstraint(dbName, tblName, colName, null, enable, validate, rely);\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/copy/HiveParserBaseSemanticAnalyzer.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":281,"status":"M"},{"authorDate":"2021-03-30 21:56:18","commitOrder":2,"curCode":"    private static void processPrimaryKeyInfos(HiveParserASTNode pkNode, List<PKInfo> pkInfos)\n            throws SemanticException {\n        String userSpecifiedName = null;\n        boolean enable = true;\n        boolean validate = false;\n        boolean rely = false;\n        for (int i = 0; i < pkNode.getChildCount(); i++) {\n            HiveParserASTNode child = (HiveParserASTNode) pkNode.getChild(i);\n            switch (child.getType()) {\n                case HiveASTParser.TOK_ENABLE:\n                case HiveASTParser.TOK_NOVALIDATE:\n                case HiveASTParser.TOK_NORELY:\n                    break;\n                case HiveASTParser.TOK_DISABLE:\n                    enable = false;\n                    break;\n                case HiveASTParser.TOK_VALIDATE:\n                    validate = true;\n                    break;\n                case HiveASTParser.TOK_RELY:\n                    rely = true;\n                    break;\n                case HiveASTParser.TOK_CONSTRAINT_NAME:\n                    userSpecifiedName =\n                            unescapeIdentifier(child.getChild(0).getText().toLowerCase());\n                    break;\n                case HiveASTParser.TOK_TABCOLNAME:\n                    for (int j = 0; j < child.getChildCount(); j++) {\n                        String colName = child.getChild(j).getText();\n                        checkColumnName(colName);\n                        pkInfos.add(new PKInfo(unescapeIdentifier(colName.toLowerCase())));\n                    }\n                    break;\n                default:\n                    throw new SemanticException(\n                            \"Unexpected node for PRIMARY KEY constraint: \" + child);\n            }\n        }\n        if (enable) {\n            throw new SemanticException(\n                    \"Invalid Primary Key syntax ENABLE feature not supported yet\");\n        }\n        if (validate) {\n            throw new SemanticException(\n                    \"Invalid Primary Key syntax VALIDATE feature not supported yet\");\n        }\n        if (pkInfos.isEmpty()) {\n            throw new SemanticException(\"No column specified as the primary key\");\n        }\n        for (PKInfo pkInfo : pkInfos) {\n            pkInfo.constraintName = userSpecifiedName;\n            pkInfo.rely = rely;\n        }\n    }\n","date":"2021-04-01 10:31:50","endLine":392,"groupId":"101096","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"processPrimaryKeyInfos","params":"(HiveParserASTNodepkNode@List<PKInfo>pkInfos)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/2c/4d3cb60c8fa9d88ff7c3ee24fd2ed460dd947d.src","preCode":"    private static void processPrimaryKeyInfos(HiveParserASTNode child, List<PKInfo> pkInfos)\n            throws SemanticException {\n        if (child.getChildCount() < 4) {\n            throw new SemanticException(\"Invalid Primary Key syntax\");\n        }\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        boolean userSpecifiedConstraintName = child.getChildCount() == 5;\n        int relyIndex = child.getChildCount() == 5 ? 2 : 1;\n        for (int j = 0; j < child.getChild(0).getChildCount(); j++) {\n            Tree grandChild = child.getChild(0).getChild(j);\n            boolean rely = child.getChild(relyIndex).getType() == HiveASTParser.TOK_VALIDATE;\n            boolean enable = child.getChild(relyIndex + 1).getType() == HiveASTParser.TOK_ENABLE;\n            boolean validate =\n                    child.getChild(relyIndex + 2).getType() == HiveASTParser.TOK_VALIDATE;\n            if (enable) {\n                throw new SemanticException(\n                        \"Invalid Primary Key syntax ENABLE feature not supported yet\");\n            }\n            if (validate) {\n                throw new SemanticException(\n                        \"Invalid Primary Key syntax VALIDATE feature not supported yet\");\n            }\n            checkColumnName(grandChild.getText());\n            pkInfos.add(\n                    new PKInfo(\n                            unescapeIdentifier(grandChild.getText().toLowerCase()),\n                            (userSpecifiedConstraintName\n                                    ? unescapeIdentifier(child.getChild(1).getText().toLowerCase())\n                                    : null),\n                            rely));\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/copy/HiveParserBaseSemanticAnalyzer.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":339,"status":"M"}],"commitId":"04bbf03a0cdb2f455c1b06569dea95ace6fa7e7c","commitMessage":"@@@[FLINK-21808][hive] Support DQL/DML in HiveParser\n\nThis closes #15253\n","date":"2021-04-01 10:31:50","modifiedFileCount":"21","status":"M","submitter":"Rui Li"}]
