[{"authorTime":"2016-10-07 06:21:42","codes":[{"authorDate":"2016-10-07 06:21:42","commitOrder":1,"curCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(latch, monitoringFunction, 1, -1);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":504,"groupId":"3495","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testProcessOnce","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/02/83f68178013da87b98a2223506d11540630117.src","preCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(latch, monitoringFunction, 1, -1);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":447,"status":"B"},{"authorDate":"2016-10-07 06:21:42","commitOrder":1,"curCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":566,"groupId":"3496","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testProcessContinuously","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/02/83f68178013da87b98a2223506d11540630117.src","preCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":507,"status":"B"}],"commitId":"b410c393c960f55c09fadd4f22732d06f801b938","commitMessage":"@@@[FLINK-4800] Introduce the TimestampedFileInputSplit for Continuous File Processing\n\nThis commit mainly introduces the TimestampedFileInputSplit. \nwhich extends the class FileInputSplit and also contains:\ni) the modification time of the file it belongs to and also.  and\nii) when checkpointing.  the point the reader is currently reading\n    from in the split the reader.\n\nThis will be useful for rescaling. With this addition.  the\nContinuousFileMonitoringFunction sends TimestampedFileInputSplit\nto the Readers.  and the Readers' state now contain only\nTimestampedFileInputSplit.\n\nIn addition.  it refactors the code of the ContinuousFileMonitoringFunction\nand that of the ContinuousFileReaderOperator along with the related\ntests.\n\nThis closes #2618.\n","date":"2016-10-27 20:22:02","modifiedFileCount":"5","status":"B","submitter":"kl0u"},{"authorTime":"2016-10-07 06:21:42","codes":[{"authorDate":"2016-11-03 18:21:08","commitOrder":2,"curCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch, monitoringFunction);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\n\t\t\t\t\t\r\n\t\t\t\t\tcontext.close();\n\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2016-11-11 21:05:58","endLine":683,"groupId":"3495","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testProcessOnce","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/64/54c11ad39c86831a8165bcfd6d5633e7319156.src","preCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(latch, monitoringFunction, 1, -1);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":620,"status":"M"},{"authorDate":"2016-10-07 06:21:42","commitOrder":2,"curCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":566,"groupId":"3496","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testProcessContinuously","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/02/83f68178013da87b98a2223506d11540630117.src","preCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":507,"status":"N"}],"commitId":"b2e8792b8ef07f5e9f676fd83137b3ce4d72cdb0","commitMessage":"@@@[FLINK-5021] Make the ContinuousFileReaderOperator rescalable.\n\nThis is the last commit that completes the refactoring of the\nContinuousFileReaderOperator so that it can be rescalable.\nWith this.  the reader can restart from a savepoint with a\ndifferent parallelism without compromising the provided\nexactly-once guarantees.\n","date":"2016-11-11 21:05:58","modifiedFileCount":"3","status":"M","submitter":"kl0u"},{"authorTime":"2016-11-17 21:54:08","codes":[{"authorDate":"2016-11-17 21:54:08","commitOrder":3,"curCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch, monitoringFunction);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\n\t\t\t\t\t\r\n\t\t\t\t\tcontext.close();\n\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2016-12-13 20:38:18","endLine":690,"groupId":"25869","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testProcessOnce","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0c/b1bad1f55a122deb2cc09e0c48c1d79b172071.src","preCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch, monitoringFunction);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\n\t\t\t\t\t\r\n\t\t\t\t\tcontext.close();\n\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":627,"status":"M"},{"authorDate":"2016-11-17 21:54:08","commitOrder":3,"curCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2016-12-13 20:38:18","endLine":826,"groupId":"25867","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testProcessContinuously","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0c/b1bad1f55a122deb2cc09e0c48c1d79b172071.src","preCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format, hdfsURI,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":767,"status":"M"}],"commitId":"685c4f836bdb79181fd1f62642736606eb81d847","commitMessage":"@@@[FLINK-5163] Port the ContinuousFileMonitoringFunction to the new state abstractions.\n","date":"2016-12-13 20:38:18","modifiedFileCount":"4","status":"M","submitter":"kl0u"},{"authorTime":"2017-02-10 22:14:34","codes":[{"authorDate":"2017-02-10 22:14:34","commitOrder":4,"curCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch, monitoringFunction);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\n\t\t\t\t\t\r\n\t\t\t\t\tcontext.close();\n\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2017-02-11 00:07:39","endLine":752,"groupId":"14132","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testProcessOnce","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch, monitoringFunction);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\n\t\t\t\t\t\r\n\t\t\t\t\tcontext.close();\n\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":687,"status":"M"},{"authorDate":"2017-02-10 22:14:34","commitOrder":4,"curCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2017-02-11 00:07:39","endLine":899,"groupId":"14131","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testProcessContinuously","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":838,"status":"M"}],"commitId":"f6709b4a48a843a0a1818fd59b98d32f82d6184f","commitMessage":"@@@[FLINK-5415] Harden ContinuousFileProcessingTest\n\n- Use TemporaryFolder @ClassRule instead of manually managing HDFS base\ndir.\n- Place files for each test in own sub-directory\n- Harden completeness condition in testFunctionRestore()\n","date":"2017-02-11 00:07:39","modifiedFileCount":"1","status":"M","submitter":"Aljoscha Krettek"},{"authorTime":"2018-07-05 12:39:23","codes":[{"authorDate":"2018-07-05 12:39:23","commitOrder":5,"curCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tcreateTestContinuousFileMonitoringFunction(format, FileProcessingMode.PROCESS_ONCE);\n\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch, monitoringFunction);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\n\t\t\t\t\t\r\n\t\t\t\t\tcontext.close();\n\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2018-07-16 02:38:07","endLine":756,"groupId":"104","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testProcessOnce","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cb/ea871fe92a11b9ee1970e8eecfdaaa582480ac.src","preCode":"\tpublic void testProcessOnce() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\t\r\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch, monitoringFunction);\n\n\t\tfinal Thread t = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\n\t\t\t\t\t\r\n\t\t\t\t\tcontext.close();\n\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> ignoredFile =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = ignoredFile.f0;\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":692,"status":"M"},{"authorDate":"2018-07-05 12:39:23","commitOrder":5,"curCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tcreateTestContinuousFileMonitoringFunction(format, FileProcessingMode.PROCESS_CONTINUOUSLY);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","date":"2018-07-16 02:38:07","endLine":901,"groupId":"104","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testProcessContinuously","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cb/ea871fe92a11b9ee1970e8eecfdaaa582480ac.src","preCode":"\tpublic void testProcessContinuously() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal OneShotLatch latch = new OneShotLatch();\n\n\t\t\r\n\t\tTuple2<org.apache.hadoop.fs.Path, String> bootstrap =\n\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", NO_OF_FILES + 1, \"This is test line.\");\n\t\tAssert.assertTrue(hdfs.exists(bootstrap.f0));\n\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\t\tfilesToBeRead.add(bootstrap.f0.getName());\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\n\t\tfinal ContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_CONTINUOUSLY, 1, INTERVAL);\n\n\t\tfinal int totalNoOfFilesToBeRead = NO_OF_FILES + 1; \r\n\t\tfinal FileVerifyingSourceContext context = new FileVerifyingSourceContext(latch,\n\t\t\tmonitoringFunction, 1, totalNoOfFilesToBeRead);\n\n\t\tfinal Thread t = new Thread() {\n\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tmonitoringFunction.open(new Configuration());\n\t\t\t\t\tmonitoringFunction.run(context);\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tAssert.fail(e.getMessage());\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tt.start();\n\n\t\tif (!latch.isTriggered()) {\n\t\t\tlatch.await();\n\t\t}\n\n\t\t\r\n\t\tfinal org.apache.hadoop.fs.Path[] filesCreated = new org.apache.hadoop.fs.Path[NO_OF_FILES];\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated[i] = file.f0;\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\t\r\n\t\tt.join();\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\thdfs.delete(bootstrap.f0, false);\n\t\tfor (org.apache.hadoop.fs.Path path: filesCreated) {\n\t\t\thdfs.delete(path, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":841,"status":"M"}],"commitId":"a7be2e188b7eebe825a8608950b0c1addbfa536c","commitMessage":"@@@[FLINK-9758] Fix ContinuousFileProcessingTest failure due to not setting runtimeContext\n\nThis closes #6260\n","date":"2018-07-16 02:38:07","modifiedFileCount":"1","status":"M","submitter":"Yun Tang"}]
