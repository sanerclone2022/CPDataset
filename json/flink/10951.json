[{"authorTime":"2017-08-22 01:55:57","codes":[{"authorDate":"2017-08-22 01:55:57","commitOrder":1,"curCode":"\tpublic HadoopInputSplit(int splitNumber, org.apache.hadoop.mapreduce.InputSplit mapreduceInputSplit, JobContext jobContext) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (mapreduceInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\t\tif (!(mapreduceInputSplit instanceof Writable)) {\n\t\t\tthrow new IllegalArgumentException(\"InputSplit must implement Writable interface.\");\n\t\t}\n\t\tthis.splitType = mapreduceInputSplit.getClass();\n\t\tthis.mapreduceInputSplit = mapreduceInputSplit;\n\t}\n","date":"2017-09-27 16:05:11","endLine":55,"groupId":"18179","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"HadoopInputSplit","params":"(intsplitNumber@org.apache.hadoop.mapreduce.InputSplitmapreduceInputSplit@JobContextjobContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/8f/3d13ca18eb0a4d3f4b2afc438f999fc3947e50.src","preCode":"\tpublic HadoopInputSplit(int splitNumber, org.apache.hadoop.mapreduce.InputSplit mapreduceInputSplit, JobContext jobContext) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (mapreduceInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\t\tif (!(mapreduceInputSplit instanceof Writable)) {\n\t\t\tthrow new IllegalArgumentException(\"InputSplit must implement Writable interface.\");\n\t\t}\n\t\tthis.splitType = mapreduceInputSplit.getClass();\n\t\tthis.mapreduceInputSplit = mapreduceInputSplit;\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/main/java/org/apache/flink/api/java/hadoop/mapreduce/wrapper/HadoopInputSplit.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"B"},{"authorDate":"2017-08-22 01:55:57","commitOrder":1,"curCode":"\tpublic HadoopInputSplit(int splitNumber, org.apache.hadoop.mapred.InputSplit hInputSplit, JobConf jobconf) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (hInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\t\tif (jobconf == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop JobConf must not be null\");\n\t\t}\n\n\t\tthis.splitType = hInputSplit.getClass();\n\n\t\tthis.jobConf = jobconf;\n\t\tthis.hadoopInputSplit = hInputSplit;\n\t}\n","date":"2017-09-27 16:05:11","endLine":63,"groupId":"38921","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"HadoopInputSplit","params":"(intsplitNumber@org.apache.hadoop.mapred.InputSplithInputSplit@JobConfjobconf)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0c/ca5581eb5e18ab7067120c714454b467c5aa3f.src","preCode":"\tpublic HadoopInputSplit(int splitNumber, org.apache.hadoop.mapred.InputSplit hInputSplit, JobConf jobconf) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (hInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\t\tif (jobconf == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop JobConf must not be null\");\n\t\t}\n\n\t\tthis.splitType = hInputSplit.getClass();\n\n\t\tthis.jobConf = jobconf;\n\t\tthis.hadoopInputSplit = hInputSplit;\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/main/java/org/apache/flink/api/java/hadoop/mapred/wrapper/HadoopInputSplit.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":49,"status":"B"}],"commitId":"4beff13e07625d8b24119ea15d828481a4bb30d7","commitMessage":"@@@[FLINK-4048] Remove Hadoop from DataSet API\n\nThis removes all Hadoop-related methods from ExecutionEnvironment (there\nare already equivalent methods in flink-hadoop-compatibility (see\nHadoopUtils and HadoopInputs.  etc.). This also removes Hadoop-specific\ntests from flink-tests because these are duplicated by tests in\nflink-hadoop-compatibility.\n\nThis also removes Hadoop-specic example code from flink-examples: the\nDistCp example and related code.\n","date":"2017-09-27 16:05:11","modifiedFileCount":"1","status":"B","submitter":"Aljoscha Krettek"},{"authorTime":"2019-11-22 21:39:16","codes":[{"authorDate":"2017-08-22 01:55:57","commitOrder":2,"curCode":"\tpublic HadoopInputSplit(int splitNumber, org.apache.hadoop.mapreduce.InputSplit mapreduceInputSplit, JobContext jobContext) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (mapreduceInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\t\tif (!(mapreduceInputSplit instanceof Writable)) {\n\t\t\tthrow new IllegalArgumentException(\"InputSplit must implement Writable interface.\");\n\t\t}\n\t\tthis.splitType = mapreduceInputSplit.getClass();\n\t\tthis.mapreduceInputSplit = mapreduceInputSplit;\n\t}\n","date":"2017-09-27 16:05:11","endLine":55,"groupId":"10951","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"HadoopInputSplit","params":"(intsplitNumber@org.apache.hadoop.mapreduce.InputSplitmapreduceInputSplit@JobContextjobContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/8f/3d13ca18eb0a4d3f4b2afc438f999fc3947e50.src","preCode":"\tpublic HadoopInputSplit(int splitNumber, org.apache.hadoop.mapreduce.InputSplit mapreduceInputSplit, JobContext jobContext) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (mapreduceInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\t\tif (!(mapreduceInputSplit instanceof Writable)) {\n\t\t\tthrow new IllegalArgumentException(\"InputSplit must implement Writable interface.\");\n\t\t}\n\t\tthis.splitType = mapreduceInputSplit.getClass();\n\t\tthis.mapreduceInputSplit = mapreduceInputSplit;\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/main/java/org/apache/flink/api/java/hadoop/mapreduce/wrapper/HadoopInputSplit.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"N"},{"authorDate":"2019-11-22 21:39:16","commitOrder":2,"curCode":"\tpublic HadoopInputSplit(\n\t\t\tint splitNumber,\n\t\t\torg.apache.hadoop.mapred.InputSplit hInputSplit,\n\t\t\t@Nullable JobConf jobconf) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (hInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\n\t\tif (needsJobConf(hInputSplit) && jobconf == null) {\n\t\t\tthrow new NullPointerException(\n\t\t\t\t\t\"Hadoop JobConf must not be null when input split is configurable.\");\n\t\t}\n\n\t\tthis.splitType = hInputSplit.getClass();\n\n\t\tthis.jobConf = jobconf;\n\t\tthis.hadoopInputSplit = hInputSplit;\n\t}\n","date":"2019-11-22 21:39:16","endLine":70,"groupId":"10951","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"HadoopInputSplit","params":"(intsplitNumber@org.apache.hadoop.mapred.InputSplithInputSplit@@NullableJobConfjobconf)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/d0/626a0966d8d920a4c1b02750dfae40cda7050d.src","preCode":"\tpublic HadoopInputSplit(int splitNumber, org.apache.hadoop.mapred.InputSplit hInputSplit, JobConf jobconf) {\n\t\tsuper(splitNumber, (String) null);\n\n\t\tif (hInputSplit == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop input split must not be null\");\n\t\t}\n\t\tif (jobconf == null) {\n\t\t\tthrow new NullPointerException(\"Hadoop JobConf must not be null\");\n\t\t}\n\n\t\tthis.splitType = hInputSplit.getClass();\n\n\t\tthis.jobConf = jobconf;\n\t\tthis.hadoopInputSplit = hInputSplit;\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/main/java/org/apache/flink/api/java/hadoop/mapred/wrapper/HadoopInputSplit.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":51,"status":"M"}],"commitId":"c7ae2b8559c0fe4cf17613249db0c2857bb91d94","commitMessage":"@@@[FLINK-14722][hadoop] Optimize mapred.HadoopInputSplit to serialize conf only when necessary\n\nThis closes #10170","date":"2019-11-22 21:39:16","modifiedFileCount":"1","status":"M","submitter":"Jingsong Lee"}]
