[{"authorTime":"2017-01-11 08:43:19","codes":[{"authorDate":"2016-11-17 21:54:08","commitOrder":4,"curCode":"\tpublic void testFilePathFiltering() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tSet<String> filesKept = new TreeSet<>();\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"**file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t}\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesKept.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(new FilePathFilter() {\n\n\t\t\tprivate static final long serialVersionUID = 2611449927338589804L;\n\n\t\t\t@Override\n\t\t\tpublic boolean filterPath(Path filePath) {\n\t\t\t\treturn filePath.getName().startsWith(\"**\");\n\t\t\t}\n\t\t});\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesKept.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2016-12-13 20:38:18","endLine":588,"groupId":"4691","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testFilePathFiltering","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0c/b1bad1f55a122deb2cc09e0c48c1d79b172071.src","preCode":"\tpublic void testFilePathFiltering() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tSet<String> filesKept = new TreeSet<>();\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"**file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t}\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesKept.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(new FilePathFilter() {\n\n\t\t\tprivate static final long serialVersionUID = 2611449927338589804L;\n\n\t\t\t@Override\n\t\t\tpublic boolean filterPath(Path filePath) {\n\t\t\t\treturn filePath.getName().startsWith(\"**\");\n\t\t\t}\n\t\t});\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesKept.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":543,"status":"NB"},{"authorDate":"2017-01-11 08:43:19","commitOrder":4,"curCode":"\tpublic void testNestedFilesProcessing() throws Exception {\n\t\tfinal Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\n\t\t\r\n\t\torg.apache.hadoop.fs.Path firstLevelDir = new org.apache.hadoop.fs.Path(hdfsURI + \"/\" + \"firstLevelDir\");\n\t\torg.apache.hadoop.fs.Path secondLevelDir = new org.apache.hadoop.fs.Path(hdfsURI + \"/\" + \"firstLevelDir\" + \"/\" + \"secondLevelDir\");\n\t\tAssert.assertFalse(hdfs.exists(firstLevelDir));\n\t\thdfs.mkdirs(firstLevelDir);\n\t\thdfs.mkdirs(secondLevelDir);\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"firstLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(firstLevelDir.toString(), \"secondLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(secondLevelDir.toString(), \"thirdLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\t\tformat.setNestedFileEnumeration(true);\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t\thdfs.delete(secondLevelDir, false);\n\t\thdfs.delete(firstLevelDir, false);\n\t}\n","date":"2017-01-20 06:57:19","endLine":641,"groupId":"4956","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedFilesProcessing","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/c2/9dd27a08ae62ad6c8425a6bccf470e907b4907.src","preCode":"\tpublic void testNestedFilesProcessing() throws Exception {\n\t\tfinal Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\n\t\t\r\n\t\torg.apache.hadoop.fs.Path firstLevelDir = new org.apache.hadoop.fs.Path(hdfsURI + \"/\" + \"firstLevelDir\");\n\t\torg.apache.hadoop.fs.Path secondLevelDir = new org.apache.hadoop.fs.Path(hdfsURI + \"/\" + \"firstLevelDir\" + \"/\" + \"secondLevelDir\");\n\t\tAssert.assertFalse(hdfs.exists(firstLevelDir));\n\t\thdfs.mkdirs(firstLevelDir);\n\t\thdfs.mkdirs(secondLevelDir);\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"firstLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(firstLevelDir.toString(), \"secondLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(secondLevelDir.toString(), \"thirdLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\t\tformat.setNestedFileEnumeration(true);\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t\thdfs.delete(secondLevelDir, false);\n\t\thdfs.delete(firstLevelDir, false);\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":591,"status":"B"}],"commitId":"9945904e2251e7c0e218e2766bf07778d1307277","commitMessage":"@@@[FLINK-5432] recursively scan nested files in ContinuousFileMonitoringFunction\n\nThis closes #3090.\n","date":"2017-01-20 06:57:19","modifiedFileCount":"3","status":"M","submitter":"Yassine Marzougui"},{"authorTime":"2017-02-10 22:14:34","codes":[{"authorDate":"2017-02-10 22:14:34","commitOrder":5,"curCode":"\tpublic void testFilePathFiltering() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tSet<String> filesKept = new TreeSet<>();\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"**file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t}\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesKept.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(new FilePathFilter() {\n\n\t\t\tprivate static final long serialVersionUID = 2611449927338589804L;\n\n\t\t\t@Override\n\t\t\tpublic boolean filterPath(Path filePath) {\n\t\t\t\treturn filePath.getName().startsWith(\"**\");\n\t\t\t}\n\t\t});\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesKept.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2017-02-11 00:07:39","endLine":591,"groupId":"4691","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testFilePathFiltering","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic void testFilePathFiltering() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tSet<String> filesKept = new TreeSet<>();\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"**file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t}\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesKept.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(new FilePathFilter() {\n\n\t\t\tprivate static final long serialVersionUID = 2611449927338589804L;\n\n\t\t\t@Override\n\t\t\tpublic boolean filterPath(Path filePath) {\n\t\t\t\treturn filePath.getName().startsWith(\"**\");\n\t\t\t}\n\t\t});\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesKept.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":544,"status":"M"},{"authorDate":"2017-02-10 22:14:34","commitOrder":5,"curCode":"\tpublic void testNestedFilesProcessing() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\n\t\t\r\n\t\torg.apache.hadoop.fs.Path firstLevelDir = new org.apache.hadoop.fs.Path(testBasePath + \"/\" + \"firstLevelDir\");\n\t\torg.apache.hadoop.fs.Path secondLevelDir = new org.apache.hadoop.fs.Path(testBasePath + \"/\" + \"firstLevelDir\" + \"/\" + \"secondLevelDir\");\n\t\tAssert.assertFalse(hdfs.exists(firstLevelDir));\n\t\thdfs.mkdirs(firstLevelDir);\n\t\thdfs.mkdirs(secondLevelDir);\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"firstLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(firstLevelDir.toString(), \"secondLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(secondLevelDir.toString(), \"thirdLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\t\tformat.setNestedFileEnumeration(true);\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t\thdfs.delete(secondLevelDir, false);\n\t\thdfs.delete(firstLevelDir, false);\n\t}\n","date":"2017-02-11 00:07:39","endLine":646,"groupId":"4956","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedFilesProcessing","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic void testNestedFilesProcessing() throws Exception {\n\t\tfinal Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\n\t\t\r\n\t\torg.apache.hadoop.fs.Path firstLevelDir = new org.apache.hadoop.fs.Path(hdfsURI + \"/\" + \"firstLevelDir\");\n\t\torg.apache.hadoop.fs.Path secondLevelDir = new org.apache.hadoop.fs.Path(hdfsURI + \"/\" + \"firstLevelDir\" + \"/\" + \"secondLevelDir\");\n\t\tAssert.assertFalse(hdfs.exists(firstLevelDir));\n\t\thdfs.mkdirs(firstLevelDir);\n\t\thdfs.mkdirs(secondLevelDir);\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"firstLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(firstLevelDir.toString(), \"secondLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(secondLevelDir.toString(), \"thirdLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\t\tformat.setNestedFileEnumeration(true);\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t\thdfs.delete(secondLevelDir, false);\n\t\thdfs.delete(firstLevelDir, false);\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":594,"status":"M"}],"commitId":"f6709b4a48a843a0a1818fd59b98d32f82d6184f","commitMessage":"@@@[FLINK-5415] Harden ContinuousFileProcessingTest\n\n- Use TemporaryFolder @ClassRule instead of manually managing HDFS base\ndir.\n- Place files for each test in own sub-directory\n- Harden completeness condition in testFunctionRestore()\n","date":"2017-02-11 00:07:39","modifiedFileCount":"1","status":"M","submitter":"Aljoscha Krettek"},{"authorTime":"2018-07-05 12:39:23","codes":[{"authorDate":"2018-07-05 12:39:23","commitOrder":6,"curCode":"\tpublic void testFilePathFiltering() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tSet<String> filesKept = new TreeSet<>();\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"**file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t}\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesKept.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(new FilePathFilter() {\n\n\t\t\tprivate static final long serialVersionUID = 2611449927338589804L;\n\n\t\t\t@Override\n\t\t\tpublic boolean filterPath(Path filePath) {\n\t\t\t\treturn filePath.getName().startsWith(\"**\");\n\t\t\t}\n\t\t});\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tcreateTestContinuousFileMonitoringFunction(format, FileProcessingMode.PROCESS_ONCE);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesKept.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2018-07-16 02:38:07","endLine":598,"groupId":"103","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testFilePathFiltering","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cb/ea871fe92a11b9ee1970e8eecfdaaa582480ac.src","preCode":"\tpublic void testFilePathFiltering() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tSet<String> filesKept = new TreeSet<>();\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"**file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t}\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file =\n\t\t\t\tcreateFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesKept.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(new FilePathFilter() {\n\n\t\t\tprivate static final long serialVersionUID = 2611449927338589804L;\n\n\t\t\t@Override\n\t\t\tpublic boolean filterPath(Path filePath) {\n\t\t\t\treturn filePath.getName().startsWith(\"**\");\n\t\t\t}\n\t\t});\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesKept.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":552,"status":"M"},{"authorDate":"2018-07-05 12:39:23","commitOrder":6,"curCode":"\tpublic void testNestedFilesProcessing() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\n\t\t\r\n\t\torg.apache.hadoop.fs.Path firstLevelDir = new org.apache.hadoop.fs.Path(testBasePath + \"/\" + \"firstLevelDir\");\n\t\torg.apache.hadoop.fs.Path secondLevelDir = new org.apache.hadoop.fs.Path(testBasePath + \"/\" + \"firstLevelDir\" + \"/\" + \"secondLevelDir\");\n\t\tAssert.assertFalse(hdfs.exists(firstLevelDir));\n\t\thdfs.mkdirs(firstLevelDir);\n\t\thdfs.mkdirs(secondLevelDir);\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"firstLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(firstLevelDir.toString(), \"secondLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(secondLevelDir.toString(), \"thirdLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\t\tformat.setNestedFileEnumeration(true);\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tcreateTestContinuousFileMonitoringFunction(format, FileProcessingMode.PROCESS_ONCE);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t\thdfs.delete(secondLevelDir, false);\n\t\thdfs.delete(firstLevelDir, false);\n\t}\n","date":"2018-07-16 02:38:07","endLine":652,"groupId":"103","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedFilesProcessing","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cb/ea871fe92a11b9ee1970e8eecfdaaa582480ac.src","preCode":"\tpublic void testNestedFilesProcessing() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tfinal Set<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tfinal Set<String> filesToBeRead = new TreeSet<>();\n\n\t\t\r\n\t\torg.apache.hadoop.fs.Path firstLevelDir = new org.apache.hadoop.fs.Path(testBasePath + \"/\" + \"firstLevelDir\");\n\t\torg.apache.hadoop.fs.Path secondLevelDir = new org.apache.hadoop.fs.Path(testBasePath + \"/\" + \"firstLevelDir\" + \"/\" + \"secondLevelDir\");\n\t\tAssert.assertFalse(hdfs.exists(firstLevelDir));\n\t\thdfs.mkdirs(firstLevelDir);\n\t\thdfs.mkdirs(secondLevelDir);\n\n\t\t\r\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"firstLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(firstLevelDir.toString(), \"secondLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(secondLevelDir.toString(), \"thirdLevelFile\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tfilesToBeRead.add(file.f0.getName());\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\t\tformat.setNestedFileEnumeration(true);\n\n\t\tContinuousFileMonitoringFunction<String> monitoringFunction =\n\t\t\tnew ContinuousFileMonitoringFunction<>(format,\n\t\t\t\tFileProcessingMode.PROCESS_ONCE, 1, INTERVAL);\n\n\t\tfinal FileVerifyingSourceContext context =\n\t\t\tnew FileVerifyingSourceContext(new OneShotLatch(), monitoringFunction);\n\n\t\tmonitoringFunction.open(new Configuration());\n\t\tmonitoringFunction.run(context);\n\n\t\tAssert.assertArrayEquals(filesToBeRead.toArray(), context.getSeenFiles().toArray());\n\n\t\t\r\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t\thdfs.delete(secondLevelDir, false);\n\t\thdfs.delete(firstLevelDir, false);\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":601,"status":"M"}],"commitId":"a7be2e188b7eebe825a8608950b0c1addbfa536c","commitMessage":"@@@[FLINK-9758] Fix ContinuousFileProcessingTest failure due to not setting runtimeContext\n\nThis closes #6260\n","date":"2018-07-16 02:38:07","modifiedFileCount":"1","status":"M","submitter":"Yun Tang"}]
