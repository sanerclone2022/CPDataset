[{"authorTime":"2020-03-06 08:04:08","codes":[{"authorDate":"2020-05-15 20:39:37","commitOrder":2,"curCode":"\tpublic void testWriteAvroSpecific() throws Exception {\n\t\tFile folder = TEMPORARY_FOLDER.newFolder();\n\n\t\tList<Address> data = Arrays.asList(\n\t\t\tnew Address(1, \"a\", \"b\", \"c\", \"12345\"),\n\t\t\tnew Address(2, \"p\", \"q\", \"r\", \"12345\"),\n\t\t\tnew Address(3, \"x\", \"y\", \"z\", \"12345\"));\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.enableCheckpointing(100);\n\n\t\tAvroWriterFactory<Address> avroWriterFactory = AvroWriters.forSpecificRecord(Address.class);\n\t\tDataStream<Address> stream = env.addSource(\n\t\t\tnew FiniteTestSource<>(data),\n\t\t\tTypeInformation.of(Address.class));\n\t\tstream.addSink(StreamingFileSink.forBulkFormat(\n\t\t\tPath.fromLocalFile(folder),\n\t\t\tavroWriterFactory).build());\n\t\tenv.execute();\n\n\t\tvalidateResults(folder, new SpecificDatumReader<>(Address.class), data);\n\t}\n","date":"2020-05-15 20:39:37","endLine":89,"groupId":"17370","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteAvroSpecific","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/a6/efaaea1c4df12a65ac123c3ab371edb832c154.src","preCode":"\tpublic void testWriteAvroSpecific() throws Exception {\n\t\tFile folder = TEMPORARY_FOLDER.newFolder();\n\n\t\tList<Address> data = Arrays.asList(\n\t\t\tnew Address(1, \"a\", \"b\", \"c\", \"12345\"),\n\t\t\tnew Address(2, \"p\", \"q\", \"r\", \"12345\"),\n\t\t\tnew Address(3, \"x\", \"y\", \"z\", \"12345\"));\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.enableCheckpointing(100);\n\n\t\tAvroWriterFactory<Address> avroWriterFactory = AvroWriters.forSpecificRecord(Address.class);\n\t\tDataStream<Address> stream = env.addSource(\n\t\t\tnew FiniteTestSource<>(data),\n\t\t\tTypeInformation.of(Address.class));\n\t\tstream.addSink(StreamingFileSink.forBulkFormat(\n\t\t\tPath.fromLocalFile(folder),\n\t\t\tavroWriterFactory).build());\n\t\tenv.execute();\n\n\t\tvalidateResults(folder, new SpecificDatumReader<>(Address.class), data);\n\t}\n","realPath":"flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroStreamingFileSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":67,"status":"NB"},{"authorDate":"2020-03-06 08:04:08","commitOrder":2,"curCode":"\tpublic void testWriteParquetAvroSpecific() throws Exception {\n\n\t\tfinal File folder = TEMPORARY_FOLDER.newFolder();\n\n\t\tfinal List<Address> data = Arrays.asList(\n\t\t\t\tnew Address(1, \"a\", \"b\", \"c\", \"12345\"),\n\t\t\t\tnew Address(2, \"p\", \"q\", \"r\", \"12345\"),\n\t\t\t\tnew Address(3, \"x\", \"y\", \"z\", \"12345\")\n\t\t);\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.enableCheckpointing(100);\n\n\t\tDataStream<Address> stream = env.addSource(\n\t\t\t\tnew FiniteTestSource<>(data), TypeInformation.of(Address.class));\n\n\t\tstream.addSink(\n\t\t\t\tStreamingFileSink.forBulkFormat(\n\t\t\t\t\t\tPath.fromLocalFile(folder),\n\t\t\t\t\t\tParquetAvroWriters.forSpecificRecord(Address.class))\n\t\t\t\t.build());\n\n\t\tenv.execute();\n\n\t\tvalidateResults(folder, SpecificData.get(), data);\n\t}\n","date":"2020-10-12 18:46:58","endLine":97,"groupId":"17370","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteParquetAvroSpecific","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/4e/c747e4db5acfba81ec7cab3b83310bcc96d6fc.src","preCode":"\tpublic void testWriteParquetAvroSpecific() throws Exception {\n\n\t\tfinal File folder = TEMPORARY_FOLDER.newFolder();\n\n\t\tfinal List<Address> data = Arrays.asList(\n\t\t\t\tnew Address(1, \"a\", \"b\", \"c\", \"12345\"),\n\t\t\t\tnew Address(2, \"p\", \"q\", \"r\", \"12345\"),\n\t\t\t\tnew Address(3, \"x\", \"y\", \"z\", \"12345\")\n\t\t);\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.enableCheckpointing(100);\n\n\t\tDataStream<Address> stream = env.addSource(\n\t\t\t\tnew FiniteTestSource<>(data), TypeInformation.of(Address.class));\n\n\t\tstream.addSink(\n\t\t\t\tStreamingFileSink.forBulkFormat(\n\t\t\t\t\t\tPath.fromLocalFile(folder),\n\t\t\t\t\t\tParquetAvroWriters.forSpecificRecord(Address.class))\n\t\t\t\t.build());\n\n\t\tenv.execute();\n\n\t\tvalidateResults(folder, SpecificData.get(), data);\n\t}\n","realPath":"flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/avro/ParquetAvroStreamingFileSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"B"}],"commitId":"e0565444a08b1d176fcb3133b8f70c3629072370","commitMessage":"@@@[hotfix] Rename ParquetStreamingFileSinkITCase\n","date":"2020-10-12 18:46:58","modifiedFileCount":"0","status":"M","submitter":"Gao Yun"},{"authorTime":"2021-08-25 10:43:17","codes":[{"authorDate":"2021-08-25 10:43:17","commitOrder":3,"curCode":"    public void testWriteAvroSpecific() throws Exception {\n        File folder = TEMPORARY_FOLDER.newFolder();\n\n        List<Address> data =\n                Arrays.asList(\n                        new Address(1, \"a\", \"b\", \"c\", \"12345\"),\n                        new Address(2, \"p\", \"q\", \"r\", \"12345\"),\n                        new Address(3, \"x\", \"y\", \"z\", \"12345\"));\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.enableCheckpointing(100);\n\n        AvroWriterFactory<Address> avroWriterFactory = AvroWriters.forSpecificRecord(Address.class);\n        DataStream<Address> stream =\n                env.addSource(new FiniteTestSource<>(data), TypeInformation.of(Address.class));\n        stream.addSink(\n                StreamingFileSink.forBulkFormat(Path.fromLocalFile(folder), avroWriterFactory)\n                        .withBucketAssigner(new UniqueBucketAssigner<>(\"test\"))\n                        .build());\n        env.execute();\n\n        validateResults(folder, new SpecificDatumReader<>(Address.class), data);\n    }\n","date":"2021-08-25 19:29:37","endLine":90,"groupId":"101929","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"testWriteAvroSpecific","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/c0/5e381c816cb4d3104223c88551a09d420452ce.src","preCode":"    public void testWriteAvroSpecific() throws Exception {\n        File folder = TEMPORARY_FOLDER.newFolder();\n\n        List<Address> data =\n                Arrays.asList(\n                        new Address(1, \"a\", \"b\", \"c\", \"12345\"),\n                        new Address(2, \"p\", \"q\", \"r\", \"12345\"),\n                        new Address(3, \"x\", \"y\", \"z\", \"12345\"));\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.enableCheckpointing(100);\n\n        AvroWriterFactory<Address> avroWriterFactory = AvroWriters.forSpecificRecord(Address.class);\n        DataStream<Address> stream =\n                env.addSource(new FiniteTestSource<>(data), TypeInformation.of(Address.class));\n        stream.addSink(\n                StreamingFileSink.forBulkFormat(Path.fromLocalFile(folder), avroWriterFactory)\n                        .build());\n        env.execute();\n\n        validateResults(folder, new SpecificDatumReader<>(Address.class), data);\n    }\n","realPath":"flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroStreamingFileSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":67,"status":"M"},{"authorDate":"2021-08-25 10:43:17","commitOrder":3,"curCode":"    public void testWriteParquetAvroSpecific() throws Exception {\n\n        final File folder = TEMPORARY_FOLDER.newFolder();\n\n        final List<Address> data =\n                Arrays.asList(\n                        new Address(1, \"a\", \"b\", \"c\", \"12345\"),\n                        new Address(2, \"p\", \"q\", \"r\", \"12345\"),\n                        new Address(3, \"x\", \"y\", \"z\", \"12345\"));\n\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.enableCheckpointing(100);\n\n        DataStream<Address> stream =\n                env.addSource(new FiniteTestSource<>(data), TypeInformation.of(Address.class));\n\n        stream.addSink(\n                StreamingFileSink.forBulkFormat(\n                                Path.fromLocalFile(folder),\n                                ParquetAvroWriters.forSpecificRecord(Address.class))\n                        .withBucketAssigner(new UniqueBucketAssigner<>(\"test\"))\n                        .build());\n\n        env.execute();\n\n        validateResults(folder, SpecificData.get(), data);\n    }\n","date":"2021-08-25 19:29:37","endLine":97,"groupId":"101929","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testWriteParquetAvroSpecific","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/bf/a10f12d929991b909ef74132460e56d7a0d86e.src","preCode":"    public void testWriteParquetAvroSpecific() throws Exception {\n\n        final File folder = TEMPORARY_FOLDER.newFolder();\n\n        final List<Address> data =\n                Arrays.asList(\n                        new Address(1, \"a\", \"b\", \"c\", \"12345\"),\n                        new Address(2, \"p\", \"q\", \"r\", \"12345\"),\n                        new Address(3, \"x\", \"y\", \"z\", \"12345\"));\n\n        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.enableCheckpointing(100);\n\n        DataStream<Address> stream =\n                env.addSource(new FiniteTestSource<>(data), TypeInformation.of(Address.class));\n\n        stream.addSink(\n                StreamingFileSink.forBulkFormat(\n                                Path.fromLocalFile(folder),\n                                ParquetAvroWriters.forSpecificRecord(Address.class))\n                        .build());\n\n        env.execute();\n\n        validateResults(folder, SpecificData.get(), data);\n    }\n","realPath":"flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/avro/ParquetAvroStreamingFileSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"M"}],"commitId":"5390e91bd47219adde15d5d515a4f5baf4231fc2","commitMessage":"@@@[FLINK-22710][formats] Explicitly set the bucket assigner to avoid writing into two buckets in the tests\n\nThis closes #16950.\n","date":"2021-08-25 19:29:37","modifiedFileCount":"9","status":"M","submitter":"Yun Gao"}]
