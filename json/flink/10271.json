[{"authorTime":"2020-10-16 20:19:57","codes":[{"authorDate":"2020-10-16 20:19:57","commitOrder":1,"curCode":"\tpublic void runKeyValueTest() throws Exception {\n\t\tfinal String topic = \"keyvaluetest\";\n\t\tcreateTestTopic(topic, 1, 1);\n\t\tfinal int elementCount = 5000;\n\n\t\t\r\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tDataStream<Tuple2<Long, PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<Long, PojoValue>>() {\n\t\t\t@Override\n\t\t\tpublic void run(SourceContext<Tuple2<Long, PojoValue>> ctx) throws Exception {\n\t\t\t\tRandom rnd = new Random(1337);\n\t\t\t\tfor (long i = 0; i < elementCount; i++) {\n\t\t\t\t\tPojoValue pojo = new PojoValue();\n\t\t\t\t\tpojo.when = new Date(rnd.nextLong());\n\t\t\t\t\tpojo.lon = rnd.nextLong();\n\t\t\t\t\tpojo.lat = i;\n\t\t\t\t\t\r\n\t\t\t\t\tLong key = (i % 2 == 0) ? null : i;\n\t\t\t\t\tctx.collect(new Tuple2<>(key, pojo));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void cancel() {\n\t\t\t}\n\t\t});\n\n\t\tKeyedSerializationSchema<Tuple2<Long, PojoValue>> schema = new TypeInformationKeyValueSerializationSchema<>(Long.class, PojoValue.class, env.getConfig());\n\t\tProperties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n\t\tproducerProperties.setProperty(\"retries\", \"3\");\n\t\tkafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n\t\tenv.execute(\"Write KV to Kafka\");\n\n\t\t\r\n\n\t\tenv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tKafkaDeserializationSchema<Tuple2<Long, PojoValue>> readSchema = new TypeInformationKeyValueSerializationSchema<>(Long.class, PojoValue.class, env.getConfig());\n\n\t\tProperties props = new Properties();\n\t\tprops.putAll(standardProps);\n\t\tprops.putAll(secureProps);\n\t\tDataStream<Tuple2<Long, PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, readSchema, props));\n\t\tfromKafka.flatMap(new RichFlatMapFunction<Tuple2<Long, PojoValue>, Object>() {\n\t\t\tlong counter = 0;\n\t\t\t@Override\n\t\t\tpublic void flatMap(Tuple2<Long, PojoValue> value, Collector<Object> out) throws Exception {\n\t\t\t\t\r\n\t\t\t\tAssert.assertTrue(\"Wrong value \" + value.f1.lat, value.f1.lat == counter);\n\t\t\t\tif (value.f1.lat % 2 == 0) {\n\t\t\t\t\tassertNull(\"key was not null\", value.f0);\n\t\t\t\t} else {\n\t\t\t\t\tAssert.assertTrue(\"Wrong value \" + value.f0, value.f0 == counter);\n\t\t\t\t}\n\t\t\t\tcounter++;\n\t\t\t\tif (counter == elementCount) {\n\t\t\t\t\t\r\n\t\t\t\t\tthrow new SuccessException();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\n\t\ttryExecute(env, \"Read KV from Kafka\");\n\n\t\tdeleteTestTopic(topic);\n\t}\n","date":"2020-10-20 05:13:30","endLine":1428,"groupId":"11458","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runKeyValueTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/45/cb862f300a7c0e0f8a396a92a1bfcae40d57c2.src","preCode":"\tpublic void runKeyValueTest() throws Exception {\n\t\tfinal String topic = \"keyvaluetest\";\n\t\tcreateTestTopic(topic, 1, 1);\n\t\tfinal int elementCount = 5000;\n\n\t\t\r\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tDataStream<Tuple2<Long, PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<Long, PojoValue>>() {\n\t\t\t@Override\n\t\t\tpublic void run(SourceContext<Tuple2<Long, PojoValue>> ctx) throws Exception {\n\t\t\t\tRandom rnd = new Random(1337);\n\t\t\t\tfor (long i = 0; i < elementCount; i++) {\n\t\t\t\t\tPojoValue pojo = new PojoValue();\n\t\t\t\t\tpojo.when = new Date(rnd.nextLong());\n\t\t\t\t\tpojo.lon = rnd.nextLong();\n\t\t\t\t\tpojo.lat = i;\n\t\t\t\t\t\r\n\t\t\t\t\tLong key = (i % 2 == 0) ? null : i;\n\t\t\t\t\tctx.collect(new Tuple2<>(key, pojo));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void cancel() {\n\t\t\t}\n\t\t});\n\n\t\tKeyedSerializationSchema<Tuple2<Long, PojoValue>> schema = new TypeInformationKeyValueSerializationSchema<>(Long.class, PojoValue.class, env.getConfig());\n\t\tProperties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n\t\tproducerProperties.setProperty(\"retries\", \"3\");\n\t\tkafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n\t\tenv.execute(\"Write KV to Kafka\");\n\n\t\t\r\n\n\t\tenv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tKafkaDeserializationSchema<Tuple2<Long, PojoValue>> readSchema = new TypeInformationKeyValueSerializationSchema<>(Long.class, PojoValue.class, env.getConfig());\n\n\t\tProperties props = new Properties();\n\t\tprops.putAll(standardProps);\n\t\tprops.putAll(secureProps);\n\t\tDataStream<Tuple2<Long, PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, readSchema, props));\n\t\tfromKafka.flatMap(new RichFlatMapFunction<Tuple2<Long, PojoValue>, Object>() {\n\t\t\tlong counter = 0;\n\t\t\t@Override\n\t\t\tpublic void flatMap(Tuple2<Long, PojoValue> value, Collector<Object> out) throws Exception {\n\t\t\t\t\r\n\t\t\t\tAssert.assertTrue(\"Wrong value \" + value.f1.lat, value.f1.lat == counter);\n\t\t\t\tif (value.f1.lat % 2 == 0) {\n\t\t\t\t\tassertNull(\"key was not null\", value.f0);\n\t\t\t\t} else {\n\t\t\t\t\tAssert.assertTrue(\"Wrong value \" + value.f0, value.f0 == counter);\n\t\t\t\t}\n\t\t\t\tcounter++;\n\t\t\t\tif (counter == elementCount) {\n\t\t\t\t\t\r\n\t\t\t\t\tthrow new SuccessException();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\n\t\ttryExecute(env, \"Read KV from Kafka\");\n\n\t\tdeleteTestTopic(topic);\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":1357,"status":"B"},{"authorDate":"2020-10-16 20:19:57","commitOrder":1,"curCode":"\tpublic void runAllDeletesTest() throws Exception {\n\t\tfinal String topic = \"alldeletestest\";\n\t\tcreateTestTopic(topic, 1, 1);\n\t\tfinal int elementCount = 300;\n\n\t\t\r\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tDataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {\n\t\t\t@Override\n\t\t\tpublic void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {\n\t\t\t\tRandom rnd = new Random(1337);\n\t\t\t\tfor (long i = 0; i < elementCount; i++) {\n\t\t\t\t\tfinal byte[] key = new byte[200];\n\t\t\t\t\trnd.nextBytes(key);\n\t\t\t\t\tctx.collect(new Tuple2<>(key, (PojoValue) null));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void cancel() {\n\t\t\t}\n\t\t});\n\n\t\tTypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig());\n\n\t\tProperties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n\t\tproducerProperties.setProperty(\"retries\", \"3\");\n\t\tproducerProperties.putAll(secureProps);\n\t\tkafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n\n\t\tenv.execute(\"Write deletes to Kafka\");\n\n\t\t\r\n\n\t\tenv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tProperties props = new Properties();\n\t\tprops.putAll(standardProps);\n\t\tprops.putAll(secureProps);\n\t\tDataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props));\n\n\t\tfromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {\n\t\t\tlong counter = 0;\n\t\t\t@Override\n\t\t\tpublic void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {\n\t\t\t\t\r\n\t\t\t\tassertNull(value.f1);\n\t\t\t\tcounter++;\n\t\t\t\tif (counter == elementCount) {\n\t\t\t\t\t\r\n\t\t\t\t\tthrow new SuccessException();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\n\t\ttryExecute(env, \"Read deletes from Kafka\");\n\n\t\tdeleteTestTopic(topic);\n\t}\n","date":"2020-10-20 05:13:30","endLine":1505,"groupId":"50507","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runAllDeletesTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/45/cb862f300a7c0e0f8a396a92a1bfcae40d57c2.src","preCode":"\tpublic void runAllDeletesTest() throws Exception {\n\t\tfinal String topic = \"alldeletestest\";\n\t\tcreateTestTopic(topic, 1, 1);\n\t\tfinal int elementCount = 300;\n\n\t\t\r\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tDataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {\n\t\t\t@Override\n\t\t\tpublic void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {\n\t\t\t\tRandom rnd = new Random(1337);\n\t\t\t\tfor (long i = 0; i < elementCount; i++) {\n\t\t\t\t\tfinal byte[] key = new byte[200];\n\t\t\t\t\trnd.nextBytes(key);\n\t\t\t\t\tctx.collect(new Tuple2<>(key, (PojoValue) null));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void cancel() {\n\t\t\t}\n\t\t});\n\n\t\tTypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig());\n\n\t\tProperties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n\t\tproducerProperties.setProperty(\"retries\", \"3\");\n\t\tproducerProperties.putAll(secureProps);\n\t\tkafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n\n\t\tenv.execute(\"Write deletes to Kafka\");\n\n\t\t\r\n\n\t\tenv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(1);\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n\t\tProperties props = new Properties();\n\t\tprops.putAll(standardProps);\n\t\tprops.putAll(secureProps);\n\t\tDataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props));\n\n\t\tfromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {\n\t\t\tlong counter = 0;\n\t\t\t@Override\n\t\t\tpublic void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {\n\t\t\t\t\r\n\t\t\t\tassertNull(value.f1);\n\t\t\t\tcounter++;\n\t\t\t\tif (counter == elementCount) {\n\t\t\t\t\t\r\n\t\t\t\t\tthrow new SuccessException();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\n\t\ttryExecute(env, \"Read deletes from Kafka\");\n\n\t\tdeleteTestTopic(topic);\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":1441,"status":"B"}],"commitId":"a87407e60be4e69cb18b6d29b9754d740b8243f0","commitMessage":"@@@[FLINK-19672][connector-kafka] Merge connector-kafka-base and connector-kafka\n","date":"2020-10-20 05:13:30","modifiedFileCount":"0","status":"B","submitter":"Timo Walther"},{"authorTime":"2021-03-08 09:35:30","codes":[{"authorDate":"2021-03-08 09:35:30","commitOrder":2,"curCode":"    public void runKeyValueTest() throws Exception {\n        final String topic = \"keyvaluetest\";\n        createTestTopic(topic, 1, 1);\n        final int elementCount = 5000;\n\n        \r\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.setRestartStrategy(RestartStrategies.noRestart());\n\n        DataStream<Tuple2<Long, PojoValue>> kvStream =\n                env.addSource(\n                        new SourceFunction<Tuple2<Long, PojoValue>>() {\n                            @Override\n                            public void run(SourceContext<Tuple2<Long, PojoValue>> ctx)\n                                    throws Exception {\n                                Random rnd = new Random(1337);\n                                for (long i = 0; i < elementCount; i++) {\n                                    PojoValue pojo = new PojoValue();\n                                    pojo.when = new Date(rnd.nextLong());\n                                    pojo.lon = rnd.nextLong();\n                                    pojo.lat = i;\n                                    \r\n                                    \r\n                                    Long key = (i % 2 == 0) ? null : i;\n                                    ctx.collect(new Tuple2<>(key, pojo));\n                                }\n                            }\n\n                            @Override\n                            public void cancel() {}\n                        });\n\n        KeyedSerializationSchema<Tuple2<Long, PojoValue>> schema =\n                new TypeInformationKeyValueSerializationSchema<>(\n                        Long.class, PojoValue.class, env.getConfig());\n        Properties producerProperties =\n                FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n        producerProperties.setProperty(\"retries\", \"3\");\n        kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n        env.execute(\"Write KV to Kafka\");\n\n        \r\n\n        env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.setRestartStrategy(RestartStrategies.noRestart());\n\n        KafkaDeserializationSchema<Tuple2<Long, PojoValue>> readSchema =\n                new TypeInformationKeyValueSerializationSchema<>(\n                        Long.class, PojoValue.class, env.getConfig());\n\n        Properties props = new Properties();\n        props.putAll(standardProps);\n        props.putAll(secureProps);\n        DataStream<Tuple2<Long, PojoValue>> fromKafka = getStream(env, topic, readSchema, props);\n        fromKafka.flatMap(\n                new RichFlatMapFunction<Tuple2<Long, PojoValue>, Object>() {\n                    long counter = 0;\n\n                    @Override\n                    public void flatMap(Tuple2<Long, PojoValue> value, Collector<Object> out)\n                            throws Exception {\n                        \r\n                        Assert.assertTrue(\"Wrong value \" + value.f1.lat, value.f1.lat == counter);\n                        if (value.f1.lat % 2 == 0) {\n                            assertNull(\"key was not null\", value.f0);\n                        } else {\n                            Assert.assertTrue(\"Wrong value \" + value.f0, value.f0 == counter);\n                        }\n                        counter++;\n                        if (counter == elementCount) {\n                            \r\n                            throw new SuccessException();\n                        }\n                    }\n                });\n\n        tryExecute(env, \"Read KV from Kafka\");\n\n        deleteTestTopic(topic);\n    }\n","date":"2021-03-30 07:53:01","endLine":1608,"groupId":"10271","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runKeyValueTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/93/e0cfef96b5b0f0e877c88de0373924f5a0b515.src","preCode":"    public void runKeyValueTest() throws Exception {\n        final String topic = \"keyvaluetest\";\n        createTestTopic(topic, 1, 1);\n        final int elementCount = 5000;\n\n        \r\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.setRestartStrategy(RestartStrategies.noRestart());\n\n        DataStream<Tuple2<Long, PojoValue>> kvStream =\n                env.addSource(\n                        new SourceFunction<Tuple2<Long, PojoValue>>() {\n                            @Override\n                            public void run(SourceContext<Tuple2<Long, PojoValue>> ctx)\n                                    throws Exception {\n                                Random rnd = new Random(1337);\n                                for (long i = 0; i < elementCount; i++) {\n                                    PojoValue pojo = new PojoValue();\n                                    pojo.when = new Date(rnd.nextLong());\n                                    pojo.lon = rnd.nextLong();\n                                    pojo.lat = i;\n                                    \r\n                                    \r\n                                    Long key = (i % 2 == 0) ? null : i;\n                                    ctx.collect(new Tuple2<>(key, pojo));\n                                }\n                            }\n\n                            @Override\n                            public void cancel() {}\n                        });\n\n        KeyedSerializationSchema<Tuple2<Long, PojoValue>> schema =\n                new TypeInformationKeyValueSerializationSchema<>(\n                        Long.class, PojoValue.class, env.getConfig());\n        Properties producerProperties =\n                FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n        producerProperties.setProperty(\"retries\", \"3\");\n        kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n        env.execute(\"Write KV to Kafka\");\n\n        \r\n\n        env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.setRestartStrategy(RestartStrategies.noRestart());\n\n        KafkaDeserializationSchema<Tuple2<Long, PojoValue>> readSchema =\n                new TypeInformationKeyValueSerializationSchema<>(\n                        Long.class, PojoValue.class, env.getConfig());\n\n        Properties props = new Properties();\n        props.putAll(standardProps);\n        props.putAll(secureProps);\n        DataStream<Tuple2<Long, PojoValue>> fromKafka =\n                env.addSource(kafkaServer.getConsumer(topic, readSchema, props));\n        fromKafka.flatMap(\n                new RichFlatMapFunction<Tuple2<Long, PojoValue>, Object>() {\n                    long counter = 0;\n\n                    @Override\n                    public void flatMap(Tuple2<Long, PojoValue> value, Collector<Object> out)\n                            throws Exception {\n                        \r\n                        Assert.assertTrue(\"Wrong value \" + value.f1.lat, value.f1.lat == counter);\n                        if (value.f1.lat % 2 == 0) {\n                            assertNull(\"key was not null\", value.f0);\n                        } else {\n                            Assert.assertTrue(\"Wrong value \" + value.f0, value.f0 == counter);\n                        }\n                        counter++;\n                        if (counter == elementCount) {\n                            \r\n                            throw new SuccessException();\n                        }\n                    }\n                });\n\n        tryExecute(env, \"Read KV from Kafka\");\n\n        deleteTestTopic(topic);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":1526,"status":"M"},{"authorDate":"2021-03-08 09:35:30","commitOrder":2,"curCode":"    public void runAllDeletesTest() throws Exception {\n        final String topic = \"alldeletestest\";\n        createTestTopic(topic, 1, 1);\n        final int elementCount = 300;\n\n        \r\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n        DataStream<Tuple2<byte[], PojoValue>> kvStream =\n                env.addSource(\n                        new SourceFunction<Tuple2<byte[], PojoValue>>() {\n                            @Override\n                            public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx)\n                                    throws Exception {\n                                Random rnd = new Random(1337);\n                                for (long i = 0; i < elementCount; i++) {\n                                    final byte[] key = new byte[200];\n                                    rnd.nextBytes(key);\n                                    ctx.collect(new Tuple2<>(key, (PojoValue) null));\n                                }\n                            }\n\n                            @Override\n                            public void cancel() {}\n                        });\n\n        TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema =\n                new TypeInformationKeyValueSerializationSchema<>(\n                        byte[].class, PojoValue.class, env.getConfig());\n\n        Properties producerProperties =\n                FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n        producerProperties.setProperty(\"retries\", \"3\");\n        producerProperties.putAll(secureProps);\n        kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n\n        env.execute(\"Write deletes to Kafka\");\n\n        \r\n\n        env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n        Properties props = new Properties();\n        props.putAll(standardProps);\n        props.putAll(secureProps);\n        DataStream<Tuple2<byte[], PojoValue>> fromKafka = getStream(env, topic, schema, props);\n\n        fromKafka.flatMap(\n                new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {\n                    long counter = 0;\n\n                    @Override\n                    public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out)\n                            throws Exception {\n                        \r\n                        assertNull(value.f1);\n                        counter++;\n                        if (counter == elementCount) {\n                            \r\n                            throw new SuccessException();\n                        }\n                    }\n                });\n\n        tryExecute(env, \"Read deletes from Kafka\");\n\n        deleteTestTopic(topic);\n    }\n","date":"2021-03-30 07:53:01","endLine":1695,"groupId":"10271","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runAllDeletesTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/93/e0cfef96b5b0f0e877c88de0373924f5a0b515.src","preCode":"    public void runAllDeletesTest() throws Exception {\n        final String topic = \"alldeletestest\";\n        createTestTopic(topic, 1, 1);\n        final int elementCount = 300;\n\n        \r\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n        DataStream<Tuple2<byte[], PojoValue>> kvStream =\n                env.addSource(\n                        new SourceFunction<Tuple2<byte[], PojoValue>>() {\n                            @Override\n                            public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx)\n                                    throws Exception {\n                                Random rnd = new Random(1337);\n                                for (long i = 0; i < elementCount; i++) {\n                                    final byte[] key = new byte[200];\n                                    rnd.nextBytes(key);\n                                    ctx.collect(new Tuple2<>(key, (PojoValue) null));\n                                }\n                            }\n\n                            @Override\n                            public void cancel() {}\n                        });\n\n        TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema =\n                new TypeInformationKeyValueSerializationSchema<>(\n                        byte[].class, PojoValue.class, env.getConfig());\n\n        Properties producerProperties =\n                FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n        producerProperties.setProperty(\"retries\", \"3\");\n        producerProperties.putAll(secureProps);\n        kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null);\n\n        env.execute(\"Write deletes to Kafka\");\n\n        \r\n\n        env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.setParallelism(1);\n        env.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n\n        Properties props = new Properties();\n        props.putAll(standardProps);\n        props.putAll(secureProps);\n        DataStream<Tuple2<byte[], PojoValue>> fromKafka =\n                env.addSource(kafkaServer.getConsumer(topic, schema, props));\n\n        fromKafka.flatMap(\n                new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {\n                    long counter = 0;\n\n                    @Override\n                    public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out)\n                            throws Exception {\n                        \r\n                        assertNull(value.f1);\n                        counter++;\n                        if (counter == elementCount) {\n                            \r\n                            throw new SuccessException();\n                        }\n                    }\n                });\n\n        tryExecute(env, \"Read deletes from Kafka\");\n\n        deleteTestTopic(topic);\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":1623,"status":"M"}],"commitId":"3abf5550a11ac4733799187bf49122417a177b6a","commitMessage":"@@@[FLINK-20114][connector/kafka] Add IT cases for KafkaSource by migrating IT cases from FlinkKafkaConsumer.\n","date":"2021-03-30 07:53:01","modifiedFileCount":"4","status":"M","submitter":"Dong Lin"}]
