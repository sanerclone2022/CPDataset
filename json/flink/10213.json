[{"authorTime":"2020-11-05 14:14:21","codes":[{"authorDate":"2020-11-05 14:14:21","commitOrder":1,"curCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","date":"2020-11-05 21:08:28","endLine":372,"groupId":"47358","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/59/99e3da5b7788d1be8771171ed4cd4a9f88ac11.src","preCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":343,"status":"B"},{"authorDate":"2020-11-05 14:14:21","commitOrder":1,"curCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetFullSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tfinal DynamicTableSink actualSink = createActualSink(SINK_SCHEMA, modifiedOptions);\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSINK_SCHEMA.toPhysicalRowDataType(),\n\t\t\tkeyEncodingFormat,\n\t\t\tvalueEncodingFormat,\n\t\t\tSINK_KEY_FIELDS,\n\t\t\tSINK_VALUE_FIELDS,\n\t\t\tnull,\n\t\t\tSINK_TOPIC,\n\t\t\tUPSERT_KAFKA_SOURCE_PROPERTIES,\n\t\t\t100);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","date":"2020-11-05 21:08:28","endLine":208,"groupId":"20511","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/ae/a46b1be3ce557351b401efdea6b9d5d65a53bf.src","preCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetFullSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tfinal DynamicTableSink actualSink = createActualSink(SINK_SCHEMA, modifiedOptions);\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSINK_SCHEMA.toPhysicalRowDataType(),\n\t\t\tkeyEncodingFormat,\n\t\t\tvalueEncodingFormat,\n\t\t\tSINK_KEY_FIELDS,\n\t\t\tSINK_VALUE_FIELDS,\n\t\t\tnull,\n\t\t\tSINK_TOPIC,\n\t\t\tUPSERT_KAFKA_SOURCE_PROPERTIES,\n\t\t\t100);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":180,"status":"B"}],"commitId":"f504bf576863442bab21f4293d763246cc5fe94e","commitMessage":"@@@[hotfix][upsert-kafka] Support sink parallelism on upsert-kafka sink\n","date":"2020-11-05 21:08:28","modifiedFileCount":"4","status":"B","submitter":"Jark Wu"},{"authorTime":"2020-11-03 22:55:58","codes":[{"authorDate":"2020-11-03 22:55:58","commitOrder":2,"curCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertTrue(sinkFunctionProvider.getParallelism().isPresent());\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","date":"2020-11-07 17:22:09","endLine":437,"groupId":"47358","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/67/77272a8ee0c27ec288108187cdb1d80eb3ca1c.src","preCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":407,"status":"M"},{"authorDate":"2020-11-03 22:55:58","commitOrder":2,"curCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetFullSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tfinal DynamicTableSink actualSink = createActualSink(SINK_SCHEMA, modifiedOptions);\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSINK_SCHEMA.toPhysicalRowDataType(),\n\t\t\tkeyEncodingFormat,\n\t\t\tvalueEncodingFormat,\n\t\t\tSINK_KEY_FIELDS,\n\t\t\tSINK_VALUE_FIELDS,\n\t\t\tnull,\n\t\t\tSINK_TOPIC,\n\t\t\tUPSERT_KAFKA_SINK_PROPERTIES,\n\t\t\t100);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertTrue(sinkFunctionProvider.getParallelism().isPresent());\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","date":"2020-11-07 17:22:09","endLine":210,"groupId":"20511","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/73/4a7883b74fd1847a4bd64a3cb8a84289f692af.src","preCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetFullSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tfinal DynamicTableSink actualSink = createActualSink(SINK_SCHEMA, modifiedOptions);\n\n\t\tEncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\t\tEncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSINK_SCHEMA.toPhysicalRowDataType(),\n\t\t\tkeyEncodingFormat,\n\t\t\tvalueEncodingFormat,\n\t\t\tSINK_KEY_FIELDS,\n\t\t\tSINK_VALUE_FIELDS,\n\t\t\tnull,\n\t\t\tSINK_TOPIC,\n\t\t\tUPSERT_KAFKA_SOURCE_PROPERTIES,\n\t\t\t100);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":181,"status":"M"}],"commitId":"64b96651579d76c718d67ebf2caca526d402a70e","commitMessage":"@@@[FLINK-19276][json][connector-kafka] Support reading Debezium metadata\n\nThis exposes metadata for the Debezium JSON format according to FLIP-107.\n\n- Update the Kafka connector to expose format specific metadata.\n- Reconfigure the internal JsonRowDataDeserializationSchema to read additional fields.\n- Let DebeziumJsonDeserializationSchema access and convert those additional fields to metadata columns.\n\nThis closes #13910.\n","date":"2020-11-07 17:22:09","modifiedFileCount":"13","status":"M","submitter":"Timo Walther"},{"authorTime":"2021-03-18 19:13:17","codes":[{"authorDate":"2020-11-03 22:55:58","commitOrder":3,"curCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertTrue(sinkFunctionProvider.getParallelism().isPresent());\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","date":"2020-11-07 17:22:09","endLine":437,"groupId":"47358","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/67/77272a8ee0c27ec288108187cdb1d80eb3ca1c.src","preCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertTrue(sinkFunctionProvider.getParallelism().isPresent());\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":407,"status":"N"},{"authorDate":"2021-03-18 19:13:17","commitOrder":3,"curCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createTableSink(SINK_SCHEMA, modifiedOptions);\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                new TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                new TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","date":"2021-03-24 04:35:35","endLine":228,"groupId":"20511","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/d0/5845ee2118da97c22951328d6daca8d02cb380.src","preCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createActualSink(SINK_SCHEMA, modifiedOptions);\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                new TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                new TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":198,"status":"M"}],"commitId":"73338e22bd0567169ce2636c8f9e3b87df783385","commitMessage":"@@@[FLINK-21913][table][connectors] Update DynamicTableFactory.Context to use ResolvedCatalogTable\n\nThis closes #15316.\n","date":"2021-03-24 04:35:35","modifiedFileCount":"45","status":"M","submitter":"Timo Walther"},{"authorTime":"2021-03-30 22:40:31","codes":[{"authorDate":"2020-11-03 22:55:58","commitOrder":4,"curCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertTrue(sinkFunctionProvider.getParallelism().isPresent());\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","date":"2020-11-07 17:22:09","endLine":437,"groupId":"47358","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/67/77272a8ee0c27ec288108187cdb1d80eb3ca1c.src","preCode":"\tpublic void testTableSinkWithParallelism() {\n\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n\t\t\tgetBasicSinkOptions(),\n\t\t\toptions -> options.put(\"sink.parallelism\", \"100\"));\n\t\tKafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n\t\tfinal EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n\t\t\tnew EncodingFormatMock(\",\");\n\n\t\tfinal DynamicTableSink expectedSink = createExpectedSink(\n\t\t\tSCHEMA_DATA_TYPE,\n\t\t\tnull,\n\t\t\tvalueEncodingFormat,\n\t\t\tnew int[0],\n\t\t\tnew int[]{0, 1, 2},\n\t\t\tnull,\n\t\t\tTOPIC,\n\t\t\tKAFKA_SINK_PROPERTIES,\n\t\t\tnew FlinkFixedPartitioner<>(),\n\t\t\tKafkaSinkSemantic.EXACTLY_ONCE,\n\t\t\t100\n\t\t);\n\t\tassertEquals(expectedSink, actualSink);\n\n\t\tfinal DynamicTableSink.SinkRuntimeProvider provider =\n\t\t\tactualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n\t\tassertThat(provider, instanceOf(SinkFunctionProvider.class));\n\t\tfinal SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n\t\tassertTrue(sinkFunctionProvider.getParallelism().isPresent());\n\t\tassertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n\t}\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":407,"status":"N"},{"authorDate":"2021-03-30 22:40:31","commitOrder":4,"curCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createTableSink(SINK_SCHEMA, modifiedOptions);\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        SinkBufferFlushMode.DISABLED,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","date":"2021-04-01 10:16:59","endLine":262,"groupId":"24575","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/07/e155d9d748f3c3f17bf0f88980ddf1cd07f1d4.src","preCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createTableSink(SINK_SCHEMA, modifiedOptions);\n\n        EncodingFormat<SerializationSchema<RowData>> keyEncodingFormat =\n                new TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n        EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                new TestFormatFactory.EncodingFormatMock(\",\", ChangelogMode.insertOnly());\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":236,"status":"M"}],"commitId":"ec9b0c5b60290697769415eb3e1b1ed2052460ac","commitMessage":"@@@[FLINK-21191][upsert-kafka] Support buffered sink function for upsert-kafka\n\nThis closes #15434\n","date":"2021-04-01 10:16:59","modifiedFileCount":"7","status":"M","submitter":"Shengkai"},{"authorTime":"2021-03-30 22:40:31","codes":[{"authorDate":"2021-07-14 13:54:56","commitOrder":5,"curCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getBasicSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        KafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n        final EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                new EncodingFormatMock(\",\");\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SCHEMA_DATA_TYPE,\n                        null,\n                        valueEncodingFormat,\n                        new int[0],\n                        new int[] {0, 1, 2},\n                        null,\n                        TOPIC,\n                        KAFKA_SINK_PROPERTIES,\n                        new FlinkFixedPartitioner<>(),\n                        SinkSemantic.EXACTLY_ONCE,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","date":"2021-07-26 22:20:08","endLine":481,"groupId":"47358","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/bb/8337b7bbd971fc3bed7bb90018951c716bc269.src","preCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getBasicSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        KafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n        final EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                new EncodingFormatMock(\",\");\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SCHEMA_DATA_TYPE,\n                        null,\n                        valueEncodingFormat,\n                        new int[0],\n                        new int[] {0, 1, 2},\n                        null,\n                        TOPIC,\n                        KAFKA_SINK_PROPERTIES,\n                        new FlinkFixedPartitioner<>(),\n                        KafkaSinkSemantic.EXACTLY_ONCE,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":451,"status":"M"},{"authorDate":"2021-03-30 22:40:31","commitOrder":5,"curCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createTableSink(SINK_SCHEMA, modifiedOptions);\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        SinkBufferFlushMode.DISABLED,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","date":"2021-04-01 10:16:59","endLine":262,"groupId":"24575","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/07/e155d9d748f3c3f17bf0f88980ddf1cd07f1d4.src","preCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createTableSink(SINK_SCHEMA, modifiedOptions);\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        SinkBufferFlushMode.DISABLED,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":236,"status":"N"}],"commitId":"abbc658f1d1898e63e7bb0d9bfdbbf42fc70ad8b","commitMessage":"@@@[FLINK-23369][connector-kafka] Use enums for options\n\nThis closes #16482.\n","date":"2021-07-26 22:20:08","modifiedFileCount":"8","status":"M","submitter":"Ingo B?rk"},{"authorTime":"2021-08-10 19:34:26","codes":[{"authorDate":"2021-08-10 19:34:26","commitOrder":6,"curCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getBasicSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        KafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n        final EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                new EncodingFormatMock(\",\");\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SCHEMA_DATA_TYPE,\n                        null,\n                        valueEncodingFormat,\n                        new int[0],\n                        new int[] {0, 1, 2},\n                        null,\n                        TOPIC,\n                        KAFKA_SINK_PROPERTIES,\n                        new FlinkFixedPartitioner<>(),\n                        DeliveryGuarantee.EXACTLY_ONCE,\n                        100,\n                        \"kafka-sink\");\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkProvider.class));\n        final SinkProvider sinkProvider = (SinkProvider) provider;\n        assertTrue(sinkProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkProvider.getParallelism().get());\n    }\n","date":"2021-08-14 00:05:49","endLine":532,"groupId":"10213","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/11/5b61266de1ec06e5b3d24038fea9da63ec3581.src","preCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getBasicSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        KafkaDynamicSink actualSink = (KafkaDynamicSink) createTableSink(SCHEMA, modifiedOptions);\n\n        final EncodingFormat<SerializationSchema<RowData>> valueEncodingFormat =\n                new EncodingFormatMock(\",\");\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SCHEMA_DATA_TYPE,\n                        null,\n                        valueEncodingFormat,\n                        new int[0],\n                        new int[] {0, 1, 2},\n                        null,\n                        TOPIC,\n                        KAFKA_SINK_PROPERTIES,\n                        new FlinkFixedPartitioner<>(),\n                        SinkSemantic.EXACTLY_ONCE,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":501,"status":"M"},{"authorDate":"2021-08-10 19:34:26","commitOrder":6,"curCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createTableSink(SINK_SCHEMA, modifiedOptions);\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        DeliveryGuarantee.AT_LEAST_ONCE,\n                        SinkBufferFlushMode.DISABLED,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkProvider.class));\n        final SinkProvider sinkProvider = (SinkProvider) provider;\n        assertTrue(sinkProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkProvider.getParallelism().get());\n    }\n","date":"2021-08-14 00:05:49","endLine":268,"groupId":"10213","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testTableSinkWithParallelism","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0e/af65075c187f7df363d16e8961eff4fc05aa3c.src","preCode":"    public void testTableSinkWithParallelism() {\n        final Map<String, String> modifiedOptions =\n                getModifiedOptions(\n                        getFullSinkOptions(), options -> options.put(\"sink.parallelism\", \"100\"));\n        final DynamicTableSink actualSink = createTableSink(SINK_SCHEMA, modifiedOptions);\n\n        final DynamicTableSink expectedSink =\n                createExpectedSink(\n                        SINK_SCHEMA.toPhysicalRowDataType(),\n                        keyEncodingFormat,\n                        valueEncodingFormat,\n                        SINK_KEY_FIELDS,\n                        SINK_VALUE_FIELDS,\n                        null,\n                        SINK_TOPIC,\n                        UPSERT_KAFKA_SINK_PROPERTIES,\n                        SinkBufferFlushMode.DISABLED,\n                        100);\n        assertEquals(expectedSink, actualSink);\n\n        final DynamicTableSink.SinkRuntimeProvider provider =\n                actualSink.getSinkRuntimeProvider(new SinkRuntimeProviderContext(false));\n        assertThat(provider, instanceOf(SinkFunctionProvider.class));\n        final SinkFunctionProvider sinkFunctionProvider = (SinkFunctionProvider) provider;\n        assertTrue(sinkFunctionProvider.getParallelism().isPresent());\n        assertEquals(100, (long) sinkFunctionProvider.getParallelism().get());\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/UpsertKafkaDynamicTableFactoryTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":241,"status":"M"}],"commitId":"1948446eeda0bcd93471b2aad61a953b6a2e36e3","commitMessage":"@@@[FLINK-23639][connectors/kafka] Migrate Table API Kafka connector to use FLIP-143 KafkaSink\n","date":"2021-08-14 00:05:49","modifiedFileCount":"9","status":"M","submitter":"Fabian Paul"}]
