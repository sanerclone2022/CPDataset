[{"authorTime":"2020-11-06 22:31:02","codes":[{"authorDate":"2020-11-07 10:24:02","commitOrder":2,"curCode":"\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n\t\t\tConfiguration hadoopConfig,\n\t\t\tRowType tableType,\n\t\t\tList<String> partitionKeys,\n\t\t\tPartitionFieldExtractor<SplitT> extractor,\n\t\t\tint[] selectedFields,\n\t\t\tList<OrcFilters.Predicate> conjunctPredicates,\n\t\t\tint batchSize) {\n\t\tString[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n\t\tLogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n\t\tList<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n\t\tint[] orcSelectedFields = getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n\t\tColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator = (SplitT split, VectorizedRowBatch rowBatch) -> {\n\t\t\t\r\n\t\t\tColumnVector[] vectors = new ColumnVector[selectedFields.length];\n\t\t\tfor (int i = 0; i < vectors.length; i++) {\n\t\t\t\tString name = tableFieldNames[selectedFields[i]];\n\t\t\t\tLogicalType type = tableFieldTypes[selectedFields[i]];\n\t\t\t\tvectors[i] = partitionKeys.contains(name) ?\n\t\t\t\t\t\tcreateFlinkVectorFromConstant(\n\t\t\t\t\t\t\t\ttype, extractor.extract(split, name, type), batchSize) :\n\t\t\t\t\t\tcreateFlinkVector(rowBatch.cols[orcFieldNames.indexOf(name)]);\n\t\t\t}\n\t\t\treturn new VectorizedColumnBatch(vectors);\n\t\t};\n\n\t\treturn new OrcColumnarRowFileInputFormat<>(\n\t\t\t\tnew OrcNoHiveShim(),\n\t\t\t\thadoopConfig,\n\t\t\t\tconvertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n\t\t\t\torcSelectedFields,\n\t\t\t\tconjunctPredicates,\n\t\t\t\tbatchSize,\n\t\t\t\tbatchGenerator,\n\t\t\t\tnew RowType(Arrays.stream(selectedFields).mapToObj(i ->\n\t\t\t\t\t\ttableType.getFields().get(i)).collect(Collectors.toList())));\n\t}\n","date":"2020-11-07 10:24:02","endLine":92,"groupId":"18725","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createPartitionedFormat","params":"(ConfigurationhadoopConfig@RowTypetableType@List<String>partitionKeys@PartitionFieldExtractor<SplitT>extractor@int[]selectedFields@List<OrcFilters.Predicate>conjunctPredicates@intbatchSize)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/2e/187e23142817a53430e3c65eec23aec4894892.src","preCode":"\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n\t\t\tConfiguration hadoopConfig,\n\t\t\tRowType tableType,\n\t\t\tList<String> partitionKeys,\n\t\t\tPartitionFieldExtractor<SplitT> extractor,\n\t\t\tint[] selectedFields,\n\t\t\tList<OrcFilters.Predicate> conjunctPredicates,\n\t\t\tint batchSize) {\n\t\tString[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n\t\tLogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n\t\tList<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n\t\tint[] orcSelectedFields = getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n\t\tColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator = (SplitT split, VectorizedRowBatch rowBatch) -> {\n\t\t\t\r\n\t\t\tColumnVector[] vectors = new ColumnVector[selectedFields.length];\n\t\t\tfor (int i = 0; i < vectors.length; i++) {\n\t\t\t\tString name = tableFieldNames[selectedFields[i]];\n\t\t\t\tLogicalType type = tableFieldTypes[selectedFields[i]];\n\t\t\t\tvectors[i] = partitionKeys.contains(name) ?\n\t\t\t\t\t\tcreateFlinkVectorFromConstant(\n\t\t\t\t\t\t\t\ttype, extractor.extract(split, name, type), batchSize) :\n\t\t\t\t\t\tcreateFlinkVector(rowBatch.cols[orcFieldNames.indexOf(name)]);\n\t\t\t}\n\t\t\treturn new VectorizedColumnBatch(vectors);\n\t\t};\n\n\t\treturn new OrcColumnarRowFileInputFormat<>(\n\t\t\t\tnew OrcNoHiveShim(),\n\t\t\t\thadoopConfig,\n\t\t\t\tconvertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n\t\t\t\torcSelectedFields,\n\t\t\t\tconjunctPredicates,\n\t\t\t\tbatchSize,\n\t\t\t\tbatchGenerator,\n\t\t\t\tnew RowType(Arrays.stream(selectedFields).mapToObj(i ->\n\t\t\t\t\t\ttableType.getFields().get(i)).collect(Collectors.toList())));\n\t}\n","realPath":"flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/OrcNoHiveColumnarRowInputFormat.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":55,"status":"B"},{"authorDate":"2020-11-06 22:31:02","commitOrder":2,"curCode":"\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n\t\t\tOrcShim<VectorizedRowBatch> shim,\n\t\t\tConfiguration hadoopConfig,\n\t\t\tRowType tableType,\n\t\t\tList<String> partitionKeys,\n\t\t\tPartitionFieldExtractor<SplitT> extractor,\n\t\t\tint[] selectedFields,\n\t\t\tList<OrcFilters.Predicate> conjunctPredicates,\n\t\t\tint batchSize) {\n\t\tString[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n\t\tLogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n\t\tList<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n\t\tint[] orcSelectedFields = getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n\t\tColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator = (SplitT split, VectorizedRowBatch rowBatch) -> {\n\t\t\t\r\n\t\t\tColumnVector[] vectors = new ColumnVector[selectedFields.length];\n\t\t\tfor (int i = 0; i < vectors.length; i++) {\n\t\t\t\tString name = tableFieldNames[selectedFields[i]];\n\t\t\t\tLogicalType type = tableFieldTypes[selectedFields[i]];\n\t\t\t\tvectors[i] = partitionKeys.contains(name) ?\n\t\t\t\t\t\tcreateFlinkVectorFromConstant(\n\t\t\t\t\t\t\t\ttype, extractor.extract(split, name, type), batchSize) :\n\t\t\t\t\t\tcreateFlinkVector(rowBatch.cols[orcFieldNames.indexOf(name)], type);\n\t\t\t}\n\t\t\treturn new VectorizedColumnBatch(vectors);\n\t\t};\n\n\t\treturn new OrcColumnarRowFileInputFormat<>(\n\t\t\t\tshim,\n\t\t\t\thadoopConfig,\n\t\t\t\tconvertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n\t\t\t\torcSelectedFields,\n\t\t\t\tconjunctPredicates,\n\t\t\t\tbatchSize,\n\t\t\t\tbatchGenerator,\n\t\t\t\tnew RowType(Arrays.stream(selectedFields).mapToObj(i ->\n\t\t\t\t\t\ttableType.getFields().get(i)).collect(Collectors.toList())));\n\t}\n","date":"2020-11-06 22:31:02","endLine":170,"groupId":"18725","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createPartitionedFormat","params":"(OrcShim<VectorizedRowBatch>shim@ConfigurationhadoopConfig@RowTypetableType@List<String>partitionKeys@PartitionFieldExtractor<SplitT>extractor@int[]selectedFields@List<OrcFilters.Predicate>conjunctPredicates@intbatchSize)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/3b/48c278cd38c8e51eb2b8e94f81f53f2c821b03.src","preCode":"\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n\t\t\tOrcShim<VectorizedRowBatch> shim,\n\t\t\tConfiguration hadoopConfig,\n\t\t\tRowType tableType,\n\t\t\tList<String> partitionKeys,\n\t\t\tPartitionFieldExtractor<SplitT> extractor,\n\t\t\tint[] selectedFields,\n\t\t\tList<OrcFilters.Predicate> conjunctPredicates,\n\t\t\tint batchSize) {\n\t\tString[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n\t\tLogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n\t\tList<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n\t\tint[] orcSelectedFields = getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n\t\tColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator = (SplitT split, VectorizedRowBatch rowBatch) -> {\n\t\t\t\r\n\t\t\tColumnVector[] vectors = new ColumnVector[selectedFields.length];\n\t\t\tfor (int i = 0; i < vectors.length; i++) {\n\t\t\t\tString name = tableFieldNames[selectedFields[i]];\n\t\t\t\tLogicalType type = tableFieldTypes[selectedFields[i]];\n\t\t\t\tvectors[i] = partitionKeys.contains(name) ?\n\t\t\t\t\t\tcreateFlinkVectorFromConstant(\n\t\t\t\t\t\t\t\ttype, extractor.extract(split, name, type), batchSize) :\n\t\t\t\t\t\tcreateFlinkVector(rowBatch.cols[orcFieldNames.indexOf(name)], type);\n\t\t\t}\n\t\t\treturn new VectorizedColumnBatch(vectors);\n\t\t};\n\n\t\treturn new OrcColumnarRowFileInputFormat<>(\n\t\t\t\tshim,\n\t\t\t\thadoopConfig,\n\t\t\t\tconvertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n\t\t\t\torcSelectedFields,\n\t\t\t\tconjunctPredicates,\n\t\t\t\tbatchSize,\n\t\t\t\tbatchGenerator,\n\t\t\t\tnew RowType(Arrays.stream(selectedFields).mapToObj(i ->\n\t\t\t\t\t\ttableType.getFields().get(i)).collect(Collectors.toList())));\n\t}\n","realPath":"flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":132,"status":"NB"}],"commitId":"c33e1adbd401a2030d51863c940b8822f06005e5","commitMessage":"@@@[FLINK-19992][hive] Integrate new orc to Hive source\n\nThis closes #13939","date":"2020-11-07 10:24:02","modifiedFileCount":"1","status":"M","submitter":"Jingsong Lee"},{"authorTime":"2020-12-28 21:30:59","codes":[{"authorDate":"2020-12-28 21:30:59","commitOrder":3,"curCode":"            OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n                    Configuration hadoopConfig,\n                    RowType tableType,\n                    List<String> partitionKeys,\n                    PartitionFieldExtractor<SplitT> extractor,\n                    int[] selectedFields,\n                    List<OrcFilters.Predicate> conjunctPredicates,\n                    int batchSize) {\n        String[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n        LogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n        List<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n        int[] orcSelectedFields =\n                getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n        ColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator =\n                (SplitT split, VectorizedRowBatch rowBatch) -> {\n                    \r\n                    ColumnVector[] vectors = new ColumnVector[selectedFields.length];\n                    for (int i = 0; i < vectors.length; i++) {\n                        String name = tableFieldNames[selectedFields[i]];\n                        LogicalType type = tableFieldTypes[selectedFields[i]];\n                        vectors[i] =\n                                partitionKeys.contains(name)\n                                        ? createFlinkVectorFromConstant(\n                                                type,\n                                                extractor.extract(split, name, type),\n                                                batchSize)\n                                        : createFlinkVector(\n                                                rowBatch.cols[orcFieldNames.indexOf(name)]);\n                    }\n                    return new VectorizedColumnBatch(vectors);\n                };\n\n        return new OrcColumnarRowFileInputFormat<>(\n                new OrcNoHiveShim(),\n                hadoopConfig,\n                convertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n                orcSelectedFields,\n                conjunctPredicates,\n                batchSize,\n                batchGenerator,\n                new RowType(\n                        Arrays.stream(selectedFields)\n                                .mapToObj(i -> tableType.getFields().get(i))\n                                .collect(Collectors.toList())));\n    }\n","date":"2020-12-28 21:35:13","endLine":99,"groupId":"102027","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"createPartitionedFormat","params":"(ConfigurationhadoopConfig@RowTypetableType@List<String>partitionKeys@PartitionFieldExtractor<SplitT>extractor@int[]selectedFields@List<OrcFilters.Predicate>conjunctPredicates@intbatchSize)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/47/af6ad91356c2f029970841129653436c99f3fe.src","preCode":"\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n\t\t\tConfiguration hadoopConfig,\n\t\t\tRowType tableType,\n\t\t\tList<String> partitionKeys,\n\t\t\tPartitionFieldExtractor<SplitT> extractor,\n\t\t\tint[] selectedFields,\n\t\t\tList<OrcFilters.Predicate> conjunctPredicates,\n\t\t\tint batchSize) {\n\t\tString[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n\t\tLogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n\t\tList<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n\t\tint[] orcSelectedFields = getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n\t\tColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator = (SplitT split, VectorizedRowBatch rowBatch) -> {\n\t\t\t\r\n\t\t\tColumnVector[] vectors = new ColumnVector[selectedFields.length];\n\t\t\tfor (int i = 0; i < vectors.length; i++) {\n\t\t\t\tString name = tableFieldNames[selectedFields[i]];\n\t\t\t\tLogicalType type = tableFieldTypes[selectedFields[i]];\n\t\t\t\tvectors[i] = partitionKeys.contains(name) ?\n\t\t\t\t\t\tcreateFlinkVectorFromConstant(\n\t\t\t\t\t\t\t\ttype, extractor.extract(split, name, type), batchSize) :\n\t\t\t\t\t\tcreateFlinkVector(rowBatch.cols[orcFieldNames.indexOf(name)]);\n\t\t\t}\n\t\t\treturn new VectorizedColumnBatch(vectors);\n\t\t};\n\n\t\treturn new OrcColumnarRowFileInputFormat<>(\n\t\t\t\tnew OrcNoHiveShim(),\n\t\t\t\thadoopConfig,\n\t\t\t\tconvertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n\t\t\t\torcSelectedFields,\n\t\t\t\tconjunctPredicates,\n\t\t\t\tbatchSize,\n\t\t\t\tbatchGenerator,\n\t\t\t\tnew RowType(Arrays.stream(selectedFields).mapToObj(i ->\n\t\t\t\t\t\ttableType.getFields().get(i)).collect(Collectors.toList())));\n\t}\n","realPath":"flink-formats/flink-orc-nohive/src/main/java/org/apache/flink/orc/nohive/OrcNoHiveColumnarRowInputFormat.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":54,"status":"M"},{"authorDate":"2020-12-28 21:30:59","commitOrder":3,"curCode":"            OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n                    OrcShim<VectorizedRowBatch> shim,\n                    Configuration hadoopConfig,\n                    RowType tableType,\n                    List<String> partitionKeys,\n                    PartitionFieldExtractor<SplitT> extractor,\n                    int[] selectedFields,\n                    List<OrcFilters.Predicate> conjunctPredicates,\n                    int batchSize) {\n        String[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n        LogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n        List<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n        int[] orcSelectedFields =\n                getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n        ColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator =\n                (SplitT split, VectorizedRowBatch rowBatch) -> {\n                    \r\n                    ColumnVector[] vectors = new ColumnVector[selectedFields.length];\n                    for (int i = 0; i < vectors.length; i++) {\n                        String name = tableFieldNames[selectedFields[i]];\n                        LogicalType type = tableFieldTypes[selectedFields[i]];\n                        vectors[i] =\n                                partitionKeys.contains(name)\n                                        ? createFlinkVectorFromConstant(\n                                                type,\n                                                extractor.extract(split, name, type),\n                                                batchSize)\n                                        : createFlinkVector(\n                                                rowBatch.cols[orcFieldNames.indexOf(name)], type);\n                    }\n                    return new VectorizedColumnBatch(vectors);\n                };\n\n        return new OrcColumnarRowFileInputFormat<>(\n                shim,\n                hadoopConfig,\n                convertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n                orcSelectedFields,\n                conjunctPredicates,\n                batchSize,\n                batchGenerator,\n                new RowType(\n                        Arrays.stream(selectedFields)\n                                .mapToObj(i -> tableType.getFields().get(i))\n                                .collect(Collectors.toList())));\n    }\n","date":"2020-12-28 21:35:13","endLine":179,"groupId":"102027","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"createPartitionedFormat","params":"(OrcShim<VectorizedRowBatch>shim@ConfigurationhadoopConfig@RowTypetableType@List<String>partitionKeys@PartitionFieldExtractor<SplitT>extractor@int[]selectedFields@List<OrcFilters.Predicate>conjunctPredicates@intbatchSize)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/d8/8c967edd44769ae43a7d34cef820a33c9ad2a6.src","preCode":"\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n\t\t\tOrcShim<VectorizedRowBatch> shim,\n\t\t\tConfiguration hadoopConfig,\n\t\t\tRowType tableType,\n\t\t\tList<String> partitionKeys,\n\t\t\tPartitionFieldExtractor<SplitT> extractor,\n\t\t\tint[] selectedFields,\n\t\t\tList<OrcFilters.Predicate> conjunctPredicates,\n\t\t\tint batchSize) {\n\t\tString[] tableFieldNames = tableType.getFieldNames().toArray(new String[0]);\n\t\tLogicalType[] tableFieldTypes = tableType.getChildren().toArray(new LogicalType[0]);\n\t\tList<String> orcFieldNames = getNonPartNames(tableFieldNames, partitionKeys);\n\t\tint[] orcSelectedFields = getSelectedOrcFields(tableFieldNames, selectedFields, orcFieldNames);\n\n\t\tColumnBatchFactory<VectorizedRowBatch, SplitT> batchGenerator = (SplitT split, VectorizedRowBatch rowBatch) -> {\n\t\t\t\r\n\t\t\tColumnVector[] vectors = new ColumnVector[selectedFields.length];\n\t\t\tfor (int i = 0; i < vectors.length; i++) {\n\t\t\t\tString name = tableFieldNames[selectedFields[i]];\n\t\t\t\tLogicalType type = tableFieldTypes[selectedFields[i]];\n\t\t\t\tvectors[i] = partitionKeys.contains(name) ?\n\t\t\t\t\t\tcreateFlinkVectorFromConstant(\n\t\t\t\t\t\t\t\ttype, extractor.extract(split, name, type), batchSize) :\n\t\t\t\t\t\tcreateFlinkVector(rowBatch.cols[orcFieldNames.indexOf(name)], type);\n\t\t\t}\n\t\t\treturn new VectorizedColumnBatch(vectors);\n\t\t};\n\n\t\treturn new OrcColumnarRowFileInputFormat<>(\n\t\t\t\tshim,\n\t\t\t\thadoopConfig,\n\t\t\t\tconvertToOrcTypeWithPart(tableFieldNames, tableFieldTypes, partitionKeys),\n\t\t\t\torcSelectedFields,\n\t\t\t\tconjunctPredicates,\n\t\t\t\tbatchSize,\n\t\t\t\tbatchGenerator,\n\t\t\t\tnew RowType(Arrays.stream(selectedFields).mapToObj(i ->\n\t\t\t\t\t\ttableType.getFields().get(i)).collect(Collectors.toList())));\n\t}\n","realPath":"flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":133,"status":"M"}],"commitId":"c6997c97c575d334679915c328792b8a3067cfb5","commitMessage":"@@@[FLINK-20651] Format code with Spotless/google-java-format\n","date":"2020-12-28 21:35:13","modifiedFileCount":"11013","status":"M","submitter":"Rufus Refactor"}]
