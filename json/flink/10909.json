[{"authorTime":"2017-08-22 01:55:57","codes":[{"authorDate":"2017-08-22 01:55:57","commitOrder":1,"curCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tJob job = Job.getInstance();\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), job);\n\t\tjob.getConfiguration().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(job, new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","date":"2017-09-27 16:05:11","endLine":122,"groupId":"21859","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"internalRun","params":"(booleanisTestDeprecatedAPI)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/70782960fcea2f85a95aafb90908f20d8de54d.src","preCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tJob job = Job.getInstance();\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), job);\n\t\tjob.getConfiguration().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(job, new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/test/java/org/apache/flink/test/hadoopcompatibility/mapreduce/WordCountMapreduceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":78,"status":"B"},{"authorDate":"2017-08-22 01:55:57","commitOrder":1,"curCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), new JobConf());\n\t\thadoopOutputFormat.getJobConf().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(), new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","date":"2017-09-27 16:05:11","endLine":122,"groupId":"21859","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"internalRun","params":"(booleanisTestDeprecatedAPI)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/53/927e7e077383d4e8950ca690fc17675e4d068e.src","preCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), new JobConf());\n\t\thadoopOutputFormat.getJobConf().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(), new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/test/java/org/apache/flink/test/hadoopcompatibility/mapred/WordCountMapredITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":78,"status":"B"}],"commitId":"4beff13e07625d8b24119ea15d828481a4bb30d7","commitMessage":"@@@[FLINK-4048] Remove Hadoop from DataSet API\n\nThis removes all Hadoop-related methods from ExecutionEnvironment (there\nare already equivalent methods in flink-hadoop-compatibility (see\nHadoopUtils and HadoopInputs.  etc.). This also removes Hadoop-specific\ntests from flink-tests because these are duplicated by tests in\nflink-hadoop-compatibility.\n\nThis also removes Hadoop-specic example code from flink-examples: the\nDistCp example and related code.\n","date":"2017-09-27 16:05:11","modifiedFileCount":"1","status":"B","submitter":"Aljoscha Krettek"},{"authorTime":"2019-02-22 20:36:48","codes":[{"authorDate":"2017-08-22 01:55:57","commitOrder":2,"curCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tJob job = Job.getInstance();\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), job);\n\t\tjob.getConfiguration().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(job, new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","date":"2017-09-27 16:05:11","endLine":122,"groupId":"21859","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"internalRun","params":"(booleanisTestDeprecatedAPI)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/70782960fcea2f85a95aafb90908f20d8de54d.src","preCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tJob job = Job.getInstance();\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), job);\n\t\tjob.getConfiguration().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(job, new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/test/java/org/apache/flink/test/hadoopcompatibility/mapreduce/WordCountMapreduceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":78,"status":"N"},{"authorDate":"2019-02-22 20:36:48","commitOrder":2,"curCode":"\tprivate void internalRun() throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\n\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\tLongWritable.class, Text.class, textPath));\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), new JobConf());\n\t\thadoopOutputFormat.getJobConf().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(), new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","date":"2019-02-22 20:36:48","endLine":112,"groupId":"21859","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"internalRun","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/91/841fbeb1d4d64f1cbc59d203f2b250b0214f1a.src","preCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), new JobConf());\n\t\thadoopOutputFormat.getJobConf().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(), new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/test/java/org/apache/flink/test/hadoopcompatibility/mapred/WordCountMapredITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"M"}],"commitId":"6f1e9142f85ac21b28410b8a429029549058db40","commitMessage":"@@@[FLINK-11656][hadoop] Remove redundant test path in WordCountMapredITCase\n\n","date":"2019-02-22 20:36:48","modifiedFileCount":"1","status":"M","submitter":"TANG Wen-hui"},{"authorTime":"2019-02-22 20:36:48","codes":[{"authorDate":"2019-02-22 20:40:18","commitOrder":3,"curCode":"\tprivate void internalRun() throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\tLongWritable.class, Text.class, textPath));\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tJob job = Job.getInstance();\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), job);\n\t\tjob.getConfiguration().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(job, new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","date":"2019-02-22 20:40:18","endLine":112,"groupId":"10909","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"internalRun","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/e2/ccd45d4473e1dd11cfc8182556915b21caf03c.src","preCode":"\tprivate void internalRun(boolean isTestDeprecatedAPI) throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\t\tif (isTestDeprecatedAPI) {\n\t\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t} else {\n\t\t\tinput = env.createInput(readHadoopFile(new TextInputFormat(),\n\t\t\t\tLongWritable.class, Text.class, textPath));\n\t\t}\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tJob job = Job.getInstance();\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), job);\n\t\tjob.getConfiguration().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(job, new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/test/java/org/apache/flink/test/hadoopcompatibility/mapreduce/WordCountMapreduceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"M"},{"authorDate":"2019-02-22 20:36:48","commitOrder":3,"curCode":"\tprivate void internalRun() throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\n\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\tLongWritable.class, Text.class, textPath));\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), new JobConf());\n\t\thadoopOutputFormat.getJobConf().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(), new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","date":"2019-02-22 20:36:48","endLine":112,"groupId":"10909","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"internalRun","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/91/841fbeb1d4d64f1cbc59d203f2b250b0214f1a.src","preCode":"\tprivate void internalRun() throws Exception {\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple2<LongWritable, Text>> input;\n\n\t\tinput = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(),\n\t\t\tLongWritable.class, Text.class, textPath));\n\n\t\tDataSet<String> text = input.map(new MapFunction<Tuple2<LongWritable, Text>, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Tuple2<LongWritable, Text> value) throws Exception {\n\t\t\t\treturn value.f1.toString();\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t\r\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t.groupBy(0)\n\t\t\t\t\t\t.sum(1);\n\n\t\tDataSet<Tuple2<Text, LongWritable>> words = counts.map(new MapFunction<Tuple2<String, Integer>, Tuple2<Text, LongWritable>>() {\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<Text, LongWritable> map(Tuple2<String, Integer> value) throws Exception {\n\t\t\t\treturn new Tuple2<Text, LongWritable>(new Text(value.f0), new LongWritable(value.f1));\n\t\t\t}\n\t\t});\n\n\t\t\r\n\t\tHadoopOutputFormat<Text, LongWritable> hadoopOutputFormat =\n\t\t\t\tnew HadoopOutputFormat<Text, LongWritable>(new TextOutputFormat<Text, LongWritable>(), new JobConf());\n\t\thadoopOutputFormat.getJobConf().set(\"mapred.textoutputformat.separator\", \" \");\n\t\tTextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(), new Path(resultPath));\n\n\t\t\r\n\t\twords.output(hadoopOutputFormat);\n\t\tenv.execute(\"Hadoop Compat WordCount\");\n\t}\n","realPath":"flink-connectors/flink-hadoop-compatibility/src/test/java/org/apache/flink/test/hadoopcompatibility/mapred/WordCountMapredITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"N"}],"commitId":"d049d3a3ecb300748bb8c668e4a8f13ca999034e","commitMessage":"@@@[FLINK-11657][hadoop] Remove redundant test path in WordCountMapreduceITCase\n\n","date":"2019-02-22 20:40:18","modifiedFileCount":"1","status":"M","submitter":"TANG Wen-hui"}]
