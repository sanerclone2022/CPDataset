[{"authorTime":"2020-06-03 10:38:29","codes":[{"authorDate":"2020-06-03 10:38:29","commitOrder":1,"curCode":"\tpublic void testPartitionFilter() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 3\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-06-03 10:38:29","endLine":295,"groupId":"41574","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/31/7943ad498b940b0c5e615cca86688171e06bb1.src","preCode":"\tpublic void testPartitionFilter() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 3\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":235,"status":"B"},{"authorDate":"2020-06-03 10:38:29","commitOrder":1,"curCode":"\tpublic void testPartitionFilterDateTimestamp() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\t\t\tSystem.out.println(results);\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-06-03 10:38:29","endLine":326,"groupId":"38894","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/31/7943ad498b940b0c5e615cca86688171e06bb1.src","preCode":"\tpublic void testPartitionFilterDateTimestamp() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\t\t\tSystem.out.println(results);\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":298,"status":"B"}],"commitId":"21cb58625b607cf227cfd298595fdda3ed7c579a","commitMessage":"@@@[FLINK-17937][hive] Change some hive connector tests to IT cases\n\n\nThis closes #12333","date":"2020-06-03 10:38:29","modifiedFileCount":"0","status":"B","submitter":"Rui Li"},{"authorTime":"2020-07-14 14:05:40","codes":[{"authorDate":"2020-07-14 14:05:40","commitOrder":2,"curCode":"\tpublic void testPartitionFilter() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 3\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-07-14 14:05:40","endLine":303,"groupId":"41574","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/b2/badf9765843239e664f97a380a04749d9a0f9d.src","preCode":"\tpublic void testPartitionFilter() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 3\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":235,"status":"M"},{"authorDate":"2020-07-14 14:05:40","commitOrder":2,"curCode":"\tpublic void testPartitionFilterDateTimestamp() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\n\t\t\t\r\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[2]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-07-14 14:05:40","endLine":338,"groupId":"38894","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/b2/badf9765843239e664f97a380a04749d9a0f9d.src","preCode":"\tpublic void testPartitionFilterDateTimestamp() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\t\t\tSystem.out.println(results);\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":306,"status":"M"}],"commitId":"81f41a6d3fda3902e8ee850d4ab8043a0e9bf763","commitMessage":"@@@[FLINK-18529][hive] Query Hive table and filter by timestamp partition can fail\n\nThis closes #12856","date":"2020-07-14 14:05:40","modifiedFileCount":"2","status":"M","submitter":"Rui Li"},{"authorTime":"2020-06-18 11:52:35","codes":[{"authorDate":"2020-06-18 11:52:35","commitOrder":3,"curCode":"\tpublic void testPartitionFilter() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 3\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-09-07 17:37:11","endLine":300,"groupId":"20315","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/52/6a45498a992d05b79faa9039e1e7c21234f28c.src","preCode":"\tpublic void testPartitionFilter() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 3\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":232,"status":"M"},{"authorDate":"2020-06-18 11:52:35","commitOrder":3,"curCode":"\tpublic void testPartitionFilterDateTimestamp() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\n\t\t\t\r\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-09-07 17:37:11","endLine":335,"groupId":"38894","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/52/6a45498a992d05b79faa9039e1e7c21234f28c.src","preCode":"\tpublic void testPartitionFilterDateTimestamp() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tList<Row> results = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\n\t\t\t\r\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n\t\t\tresults = Lists.newArrayList(query.execute().collect());\n\t\t\tassertEquals(\"[2]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":303,"status":"M"}],"commitId":"91d2b628bfe1a2e7beed5111a6d9a572cc6bc310","commitMessage":"@@@[hotfix][table][connector] Use CollectionUtil#iteratorToList instead of Guava Lists\n","date":"2020-09-07 17:37:11","modifiedFileCount":"18","status":"M","submitter":"godfreyhe"},{"authorTime":"2020-10-28 09:54:12","codes":[{"authorDate":"2020-10-28 09:54:12","commitOrder":4,"curCode":"\tpublic void testPartitionFilter() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-10-28 09:54:12","endLine":296,"groupId":"26229","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/64/14bbb01610a799b5cb10b4f7e69b1843a70dcf.src","preCode":"\tpublic void testPartitionFilter() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 3\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 2\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 0\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":224,"status":"M"},{"authorDate":"2020-10-28 09:54:12","commitOrder":4,"curCode":"\tpublic void testPartitionFilterDateTimestamp() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.0}]\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\n\t\t\t\r\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-10-28 09:54:12","endLine":332,"groupId":"38894","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/64/14bbb01610a799b5cb10b4f7e69b1843a70dcf.src","preCode":"\tpublic void testPartitionFilterDateTimestamp() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"PartitionPruned: true, PartitionNums: 1\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\n\t\t\t\r\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":299,"status":"M"}],"commitId":"0a14ad1cc47c4c6d4de0d5c90d3cd9578ca2536c","commitMessage":"@@@[FLINK-19789][hive] Migrate Hive connector to new table source sink interface\n\nThis closes #13771","date":"2020-10-28 09:54:12","modifiedFileCount":"10","status":"M","submitter":"Jingsong Lee"},{"authorTime":"2020-11-24 10:53:43","codes":[{"authorDate":"2020-11-24 10:53:43","commitOrder":5,"curCode":"\tpublic void testPartitionFilter() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-11-24 10:53:43","endLine":288,"groupId":"26229","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/9c/46eb973413634093e227110599b6dfeb859422.src","preCode":"\tpublic void testPartitionFilter() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1=1,p2='a'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1=2,p2='b'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1=3,p2='c'\");\n\t\t\t\r\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{4}).commit(\"p1=4,p2='c:2'\");\n\t\t\tTable query = tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2, 3, 4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 3]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[1, 2]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[4]\", results.toString());\n\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n\t\t\texplain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertFalse(catalog.fallback);\n\t\t\toptimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":216,"status":"M"},{"authorDate":"2020-11-24 10:53:43","commitOrder":5,"curCode":"\tpublic void testPartitionFilterDateTimestamp() throws Exception {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08.1'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09.1'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10.1'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09.1' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.1}]\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\n\t\t\t\r\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","date":"2020-11-24 10:53:43","endLine":324,"groupId":"26230","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/9c/46eb973413634093e227110599b6dfeb859422.src","preCode":"\tpublic void testPartitionFilterDateTimestamp() {\n\t\tTableEnvironment tableEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n\t\tTestPartitionFilterCatalog catalog = new TestPartitionFilterCatalog(\n\t\t\t\thiveCatalog.getName(), hiveCatalog.getDefaultDatabase(), hiveCatalog.getHiveConf(), hiveCatalog.getHiveVersion());\n\t\ttableEnv.registerCatalog(catalog.getName(), catalog);\n\t\ttableEnv.useCatalog(catalog.getName());\n\t\ttableEnv.executeSql(\"create database db1\");\n\t\ttry {\n\t\t\ttableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{1}).commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{2}).commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09'\");\n\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"part\")\n\t\t\t\t\t.addRow(new Object[]{3}).commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10'\");\n\n\t\t\tTable query = tableEnv.sqlQuery(\n\t\t\t\t\t\"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09' as timestamp)\");\n\t\t\tString[] explain = query.explain().split(\"==.*==\\n\");\n\t\t\tassertTrue(catalog.fallback);\n\t\t\tString optimizedPlan = explain[2];\n\t\t\tassertTrue(optimizedPlan, optimizedPlan.contains(\n\t\t\t\t\t\"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.0}]\"));\n\t\t\tList<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[3]\", results.toString());\n\n\t\t\t\r\n\t\t\tquery = tableEnv.sqlQuery(\"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n\t\t\tresults = CollectionUtil.iteratorToList(query.execute().collect());\n\t\t\tassertEquals(\"[2]\", results.toString());\n\t\t} finally {\n\t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n\t\t}\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":291,"status":"M"}],"commitId":"e9b05e723fb02d9a6dae607ef28244eca6e9edc8","commitMessage":"@@@[FLINK-19653][hive] Reduce our dependency on hive runner for tests\n\nThis closes #14123","date":"2020-11-24 10:53:43","modifiedFileCount":"8","status":"M","submitter":"Rui Li"},{"authorTime":"2020-12-18 18:32:55","codes":[{"authorDate":"2020-12-18 18:32:55","commitOrder":6,"curCode":"    public void testPartitionFilter() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1=1,p2='a'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1=2,p2='b'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1=3,p2='c'\");\n            \r\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {4})\n                    .commit(\"p1=4,p2='c:2'\");\n            Table query =\n                    tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2], +I[3], +I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[3]]\", results.toString());\n\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[2]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","date":"2021-01-08 00:17:30","endLine":324,"groupId":"26229","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/dd/1bb79a9390eaafc5cd08289e675b44b548395e.src","preCode":"    public void testPartitionFilter() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1=1,p2='a'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1=2,p2='b'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1=3,p2='c'\");\n            \r\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {4})\n                    .commit(\"p1=4,p2='c:2'\");\n            Table query =\n                    tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[2, 3, 4]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[1, 3]\", results.toString());\n\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[1, 2]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[4]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":226,"status":"M"},{"authorDate":"2020-12-18 18:32:55","commitOrder":6,"curCode":"    public void testPartitionFilterDateTimestamp() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\n                    \"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10.1'\");\n\n            Table query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09.1' as timestamp)\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertTrue(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.1}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[3]]\", results.toString());\n\n            \r\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2]]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","date":"2021-01-08 00:17:30","endLine":374,"groupId":"26230","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/dd/1bb79a9390eaafc5cd08289e675b44b548395e.src","preCode":"    public void testPartitionFilterDateTimestamp() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\n                    \"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10.1'\");\n\n            Table query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09.1' as timestamp)\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertTrue(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.1}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[3]\", results.toString());\n\n            \r\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[2]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":327,"status":"M"}],"commitId":"73cdd3d0d9f6a807b3e47c09eef7983c9aa180c7","commitMessage":"@@@[FLINK-18090] Update tests for new Row.toString\n\nAll tests in modules apart from the Blink planner/runtime\nmodule have been updated.\n\nOtherwise we use a JUnit rule to make the migration of\nthe remaining tests incremental.\n\nThis closes #14568.\n","date":"2021-01-08 00:17:30","modifiedFileCount":"34","status":"M","submitter":"Timo Walther"},{"authorTime":"2021-03-30 21:56:18","codes":[{"authorDate":"2020-12-18 18:32:55","commitOrder":7,"curCode":"    public void testPartitionFilter() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1=1,p2='a'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1=2,p2='b'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1=3,p2='c'\");\n            \r\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {4})\n                    .commit(\"p1=4,p2='c:2'\");\n            Table query =\n                    tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2], +I[3], +I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[3]]\", results.toString());\n\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[2]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","date":"2021-01-08 00:17:30","endLine":324,"groupId":"26229","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/dd/1bb79a9390eaafc5cd08289e675b44b548395e.src","preCode":"    public void testPartitionFilter() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1=1,p2='a'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1=2,p2='b'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1=3,p2='c'\");\n            \r\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {4})\n                    .commit(\"p1=4,p2='c:2'\");\n            Table query =\n                    tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2], +I[3], +I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[3]]\", results.toString());\n\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[2]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":226,"status":"N"},{"authorDate":"2021-03-30 21:56:18","commitOrder":7,"curCode":"    public void testPartitionFilterDateTimestamp() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\n                    \"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10.1'\");\n\n            Table query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09.1' as timestamp)\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertTrue(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.1}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[3]]\", results.toString());\n\n            \r\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where timestamp '2018-08-08 08:08:09.1' = p2\");\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2]]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","date":"2021-04-01 10:31:50","endLine":374,"groupId":"26230","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/21/61f5fb304d29b1ce224604a330667537e59c66.src","preCode":"    public void testPartitionFilterDateTimestamp() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\n                    \"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10.1'\");\n\n            Table query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09.1' as timestamp)\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertTrue(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.1}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[3]]\", results.toString());\n\n            \r\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where timestamp '2018-08-08 08:08:09' = p2\");\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2]]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":327,"status":"M"}],"commitId":"04bbf03a0cdb2f455c1b06569dea95ace6fa7e7c","commitMessage":"@@@[FLINK-21808][hive] Support DQL/DML in HiveParser\n\nThis closes #15253\n","date":"2021-04-01 10:31:50","modifiedFileCount":"21","status":"M","submitter":"Rui Li"},{"authorTime":"2021-06-24 15:56:28","codes":[{"authorDate":"2021-06-24 15:56:28","commitOrder":8,"curCode":"    public void testPartitionFilter() throws Exception {\n        TableEnvironment tableEnv = HiveTestUtils.createTableEnvInBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1=1,p2='a'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1=2,p2='b'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1=3,p2='c'\");\n            \r\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {4})\n                    .commit(\"p1=4,p2='c:2'\");\n            Table query =\n                    tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2], +I[3], +I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[3]]\", results.toString());\n\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[2]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","date":"2021-07-06 19:28:35","endLine":324,"groupId":"101062","id":15,"instanceNumber":1,"isCurCommit":1,"methodName":"testPartitionFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/45/e2d0b138bbfe07d9e8a7dffe9bba21736b9d0d.src","preCode":"    public void testPartitionFilter() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\"create table db1.part(x int) partitioned by (p1 int,p2 string)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1=1,p2='a'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1=2,p2='b'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1=3,p2='c'\");\n            \r\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {4})\n                    .commit(\"p1=4,p2='c:2'\");\n            Table query =\n                    tableEnv.sqlQuery(\"select x from db1.part where p1>1 or p2<>'a' order by x\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2, p2=b}, {p1=3, p2=c}, {p1=4, p2=c:2}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2], +I[3], +I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1>2 and p2<='a' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p1 in (1,3,5) order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=3, p2=c}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[3]]\", results.toString());\n\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where (p1=1 and p2='a') or ((p1=2 and p2='b') or p2='d') order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=1, p2=a}, {p1=2, p2=b}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[1], +I[2]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where p2 = 'c:2' order by x\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=4, p2=c:2}], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[4]]\", results.toString());\n\n            query = tableEnv.sqlQuery(\"select x from db1.part where '' = p2\");\n            explain = query.explain().split(\"==.*==\\n\");\n            assertFalse(catalog.fallback);\n            optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[], project=[x]]]\"));\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":227,"status":"M"},{"authorDate":"2021-06-24 15:56:28","commitOrder":8,"curCode":"    public void testPartitionFilterDateTimestamp() throws Exception {\n        TableEnvironment tableEnv = HiveTestUtils.createTableEnvInBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\n                    \"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10.1'\");\n\n            Table query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09.1' as timestamp)\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertTrue(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.1}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[3]]\", results.toString());\n\n            \r\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where timestamp '2018-08-08 08:08:09.1' = p2\");\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2]]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","date":"2021-07-06 19:28:35","endLine":373,"groupId":"101062","id":16,"instanceNumber":2,"isCurCommit":1,"methodName":"testPartitionFilterDateTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/45/e2d0b138bbfe07d9e8a7dffe9bba21736b9d0d.src","preCode":"    public void testPartitionFilterDateTimestamp() throws Exception {\n        TableEnvironment tableEnv =\n                HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode(SqlDialect.HIVE);\n        TestPartitionFilterCatalog catalog =\n                new TestPartitionFilterCatalog(\n                        hiveCatalog.getName(),\n                        hiveCatalog.getDefaultDatabase(),\n                        hiveCatalog.getHiveConf(),\n                        hiveCatalog.getHiveVersion());\n        tableEnv.registerCatalog(catalog.getName(), catalog);\n        tableEnv.useCatalog(catalog.getName());\n        tableEnv.executeSql(\"create database db1\");\n        try {\n            tableEnv.executeSql(\n                    \"create table db1.part(x int) partitioned by (p1 date,p2 timestamp)\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {1})\n                    .commit(\"p1='2018-08-08',p2='2018-08-08 08:08:08.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {2})\n                    .commit(\"p1='2018-08-09',p2='2018-08-08 08:08:09.1'\");\n            HiveTestUtils.createTextTableInserter(hiveCatalog, \"db1\", \"part\")\n                    .addRow(new Object[] {3})\n                    .commit(\"p1='2018-08-10',p2='2018-08-08 08:08:10.1'\");\n\n            Table query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where p1>cast('2018-08-09' as date) and p2<>cast('2018-08-08 08:08:09.1' as timestamp)\");\n            String[] explain = query.explain().split(\"==.*==\\n\");\n            assertTrue(catalog.fallback);\n            String optimizedPlan = explain[2];\n            assertTrue(\n                    optimizedPlan,\n                    optimizedPlan.contains(\n                            \"table=[[test-catalog, db1, part, partitions=[{p1=2018-08-10, p2=2018-08-08 08:08:10.1}]\"));\n            List<Row> results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[3]]\", results.toString());\n\n            \r\n            query =\n                    tableEnv.sqlQuery(\n                            \"select x from db1.part where timestamp '2018-08-08 08:08:09.1' = p2\");\n            results = CollectionUtil.iteratorToList(query.execute().collect());\n            assertEquals(\"[+I[2]]\", results.toString());\n        } finally {\n            tableEnv.executeSql(\"drop database db1 cascade\");\n        }\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":327,"status":"M"}],"commitId":"312fe4c1ce17ac6e5515bcab560b86456993daf5","commitMessage":"@@@[FLINK-22880][table] Remove 'blink' term from code base\n\nThis removes all mentionings of the term \"blink\" in the code\nbase. In order to reduce user confusion.  do not use this term\nanymore but refer to as \"Flink SQL\" or \"Flink Table API\".\n\nThis closes #16374.\n","date":"2021-07-06 19:28:35","modifiedFileCount":"73","status":"M","submitter":"Timo Walther"}]
