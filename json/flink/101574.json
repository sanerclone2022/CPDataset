[{"authorTime":"2016-10-02 22:56:41","codes":[{"authorDate":"2016-10-02 22:56:41","commitOrder":1,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-07 20:14:27","endLine":224,"groupId":"53398","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/84/8a5793827a72b84b66ab1b06d0c9846f668cdc.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":127,"status":"B"},{"authorDate":"2016-10-02 22:56:41","commitOrder":1,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, false);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, false);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof SuppressRestartsException) {\n\t\t\t\tSuppressRestartsException suppressRestartsException = (SuppressRestartsException) exception.getCause();\n\n\t\t\t\tif (suppressRestartsException.getCause() instanceof IllegalStateException) {\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t} else {\n\t\t\t\t\tthrow exception;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-07 20:14:27","endLine":322,"groupId":"45928","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/84/8a5793827a72b84b66ab1b06d0c9846f668cdc.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, false);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, false);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof SuppressRestartsException) {\n\t\t\t\tSuppressRestartsException suppressRestartsException = (SuppressRestartsException) exception.getCause();\n\n\t\t\t\tif (suppressRestartsException.getCause() instanceof IllegalStateException) {\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t} else {\n\t\t\t\t\tthrow exception;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"B"}],"commitId":"8d953bf2626012e3e497334641962bd8f96098de","commitMessage":"@@@[FLINK-4731] Fix HeapKeyedStateBackend Scale-In\n\nAdds additional tests in RescalingITCase for scale-in\n\nThis closes #2584.\n","date":"2016-10-07 20:14:27","modifiedFileCount":"11","status":"B","submitter":"Stefan Richter"},{"authorTime":"2016-10-06 22:43:42","codes":[{"authorDate":"2016-10-06 22:43:42","commitOrder":2,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":223,"groupId":"53398","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":126,"status":"M"},{"authorDate":"2016-10-06 22:43:42","commitOrder":2,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, false);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, false);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof SuppressRestartsException) {\n\t\t\t\tSuppressRestartsException suppressRestartsException = (SuppressRestartsException) exception.getCause();\n\n\t\t\t\tif (suppressRestartsException.getCause() instanceof IllegalStateException) {\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t} else {\n\t\t\t\t\tthrow exception;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":320,"groupId":"45928","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, false);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, false);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof SuppressRestartsException) {\n\t\t\t\tSuppressRestartsException suppressRestartsException = (SuppressRestartsException) exception.getCause();\n\n\t\t\t\tif (suppressRestartsException.getCause() instanceof IllegalStateException) {\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t} else {\n\t\t\t\t\tthrow exception;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":232,"status":"M"}],"commitId":"fd410d9f6d1c8d53fb721752528ebd77fd78db57","commitMessage":"@@@[FLINK-4512] [FLIP-10] Add option to persist periodic checkpoints\n\n[FLINK-4509] [FLIP-10] Specify savepoint directory per savepoint\n[FLINK-4507] [FLIP-10] Deprecate savepoint backend config\n\nThis closes #2608.\n","date":"2016-10-14 16:05:06","modifiedFileCount":"50","status":"M","submitter":"Ufuk Celebi"},{"authorTime":"2016-09-14 20:27:27","codes":[{"authorDate":"2016-10-06 22:43:42","commitOrder":3,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":223,"groupId":"53398","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":126,"status":"N"},{"authorDate":"2016-09-14 20:27:27","commitOrder":3,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, false);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, false);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-19 17:41:26","endLine":313,"groupId":"45928","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/0e/513fa00c662aa51a430f164dba25d9ebc332d3.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, false);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, false);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof SuppressRestartsException) {\n\t\t\t\tSuppressRestartsException suppressRestartsException = (SuppressRestartsException) exception.getCause();\n\n\t\t\t\tif (suppressRestartsException.getCause() instanceof IllegalStateException) {\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\t\t\t\t} else {\n\t\t\t\t\tthrow exception;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":231,"status":"M"}],"commitId":"b05c3c1b01fc48674d01e138e5e3e628c823974f","commitMessage":"@@@[FLINK-4619] Answer with JobResultFailure if savepoint restore fails during submission\n\nThis closes #2498.\n","date":"2016-10-19 17:41:26","modifiedFileCount":"3","status":"M","submitter":"Maciek Pr?chniak"},{"authorTime":"2016-10-04 16:59:38","codes":[{"authorDate":"2016-10-06 22:43:42","commitOrder":4,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":223,"groupId":"53398","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":126,"status":"N"},{"authorDate":"2016-10-04 16:59:38","commitOrder":4,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-20 22:14:21","endLine":321,"groupId":"45928","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/95/115d65b56d0638f1dd1ef61cf5c1082fdde3c1.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, false);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, false);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":239,"status":"M"}],"commitId":"cab9cd44eca83ef8cbcd2a2d070d8c79cb037977","commitMessage":"@@@[FLINK-4844] Partitionable Raw Keyed/Operator State\n","date":"2016-10-20 22:14:21","modifiedFileCount":"87","status":"M","submitter":"Stefan Richter"},{"authorTime":"2016-10-31 07:36:58","codes":[{"authorDate":"2016-10-06 22:43:42","commitOrder":5,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":223,"groupId":"53398","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":126,"status":"N"},{"authorDate":"2016-10-31 07:36:58","commitOrder":5,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\tassertTrue(String.valueOf(savepointResponse), savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-31 21:02:59","endLine":315,"groupId":"45928","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/da/25ae641618d675da72dd285da22db63b2603b5.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":239,"status":"M"}],"commitId":"094b747a39906f01f6a8b92233a5a8011618e641","commitMessage":"@@@[hotfix] Improved test stability of RescalingITCase\n\nThis closes #2728.\n","date":"2016-10-31 21:02:59","modifiedFileCount":"1","status":"M","submitter":"Stefan Richter"},{"authorTime":"2016-10-27 00:05:26","codes":[{"authorDate":"2016-10-27 00:05:26","commitOrder":6,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-11-02 14:34:21","endLine":231,"groupId":"17456","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5a/6417329010ded63de2a627feec65dd6d6eb3d8.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":134,"status":"M"},{"authorDate":"2016-10-27 00:05:26","commitOrder":6,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\tassertTrue(String.valueOf(savepointResponse), savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-11-02 14:34:21","endLine":316,"groupId":"45928","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5a/6417329010ded63de2a627feec65dd6d6eb3d8.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\tassertTrue(String.valueOf(savepointResponse), savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":240,"status":"M"}],"commitId":"c0e620f0ace0aa3500a5642e7165cf9f05e81f6a","commitMessage":"@@@[FLINK-4445] [checkpointing] Add option to allow non restored checkpoint state\n\nAllows to skip checkpoint state that cannot be mapped to a job vertex when\nrestoring.\n\nThis closes #2712.\n","date":"2016-11-02 14:34:21","modifiedFileCount":"12","status":"M","submitter":"Ufuk Celebi"},{"authorTime":"2016-10-27 00:05:26","codes":[{"authorDate":"2017-01-16 21:31:22","commitOrder":7,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2017-01-24 22:51:35","endLine":247,"groupId":"45929","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/07/3632ad00d86ebd05b0618eecf5e0447688e5b7.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":148,"status":"M"},{"authorDate":"2016-10-27 00:05:26","commitOrder":7,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\tassertTrue(String.valueOf(savepointResponse), savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-11-02 14:34:21","endLine":316,"groupId":"45928","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5a/6417329010ded63de2a627feec65dd6d6eb3d8.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\tassertTrue(String.valueOf(savepointResponse), savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":240,"status":"N"}],"commitId":"acfeeaf5e337e56300d10a3a991e79edc827ac7a","commitMessage":"@@@[FLINK-5473] Limit max parallelism to 1 for non-parallel operators\n\n[FLINK-5473] Better default behaviours for unspecified maximum parallelism\n\nThis closes #3182.\n","date":"2017-01-24 22:51:35","modifiedFileCount":"34","status":"M","submitter":"Stefan Richter"},{"authorTime":"2018-03-19 18:36:39","codes":[{"authorDate":"2018-03-19 18:36:39","commitOrder":8,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2018-03-24 02:11:49","endLine":246,"groupId":"48539","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/e4/f4389bb6abc56bd4f91e9dc2cce1e0784167b8.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":171,"status":"M"},{"authorDate":"2018-03-19 18:36:39","commitOrder":8,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","date":"2018-03-24 02:11:49","endLine":302,"groupId":"32274","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/e4/f4389bb6abc56bd4f91e9dc2cce1e0784167b8.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\tassertTrue(String.valueOf(savepointResponse), savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":255,"status":"M"}],"commitId":"edb6f7fef8c5df6af43bbe28f96d8c6bb3332d00","commitMessage":"@@@[FLINK-8956][tests] Port RescalingITCase to flip6\n\nThis closes #5715.\n","date":"2018-03-24 02:11:49","modifiedFileCount":"1","status":"M","submitter":"zentol"},{"authorTime":"2019-11-01 14:51:28","codes":[{"authorDate":"2019-11-01 14:51:28","commitOrder":9,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2019-11-01 14:51:28","endLine":249,"groupId":"48539","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cf/1b00a5d2fefb67d804f6f6c5eefe557d94c8d3.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"},{"authorDate":"2019-11-01 14:51:28","commitOrder":9,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","date":"2019-11-01 14:51:28","endLine":303,"groupId":"32274","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cf/1b00a5d2fefb67d804f6f6c5eefe557d94c8d3.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":258,"status":"M"}],"commitId":"bf5235e340543b9c4551d2131e8a405bd1e9e0c0","commitMessage":"@@@[FLINK-14496][client] Exclude detach flag from ClusterClient\n\nThis closes #9972 .","date":"2019-11-01 14:51:28","modifiedFileCount":"37","status":"M","submitter":"tison"},{"authorTime":"2019-11-08 10:23:59","codes":[{"authorDate":"2019-11-08 10:23:59","commitOrder":10,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2019-11-08 10:23:59","endLine":249,"groupId":"48539","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/92/3933fbfc07fbdb138584bed9c14182277a926d.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"},{"authorDate":"2019-11-08 10:23:59","commitOrder":10,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","date":"2019-11-08 10:23:59","endLine":303,"groupId":"32274","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/92/3933fbfc07fbdb138584bed9c14182277a926d.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":258,"status":"M"}],"commitId":"d938c19480c220344827271ff8da729cd91735b3","commitMessage":"@@@[FLINK-14593][client] Port ClusterClient to asynchronous interface version\n\nThis closes #10069 .\n","date":"2019-11-08 10:23:59","modifiedFileCount":"27","status":"M","submitter":"tison"},{"authorTime":"2019-11-08 10:23:59","codes":[{"authorDate":"2020-02-04 03:26:55","commitOrder":11,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tassertTrue(SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2020-03-05 01:44:42","endLine":249,"groupId":"48539","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/80/d0cb57bf07883a0eabc0a52673827a0290bb47.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"},{"authorDate":"2019-11-08 10:23:59","commitOrder":11,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","date":"2019-11-08 10:23:59","endLine":303,"groupId":"32274","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/92/3933fbfc07fbdb138584bed9c14182277a926d.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":258,"status":"N"}],"commitId":"b098ce505176720ba37da8f6d6c23096b1d3a260","commitMessage":"@@@[FLINK-15838] Dangling CountDownLatch.await(timeout)\n\nThis closes #11005.\n","date":"2020-03-05 01:44:42","modifiedFileCount":"7","status":"M","submitter":"Ayush Saxena"},{"authorTime":"2020-08-15 08:29:49","codes":[{"authorDate":"2020-08-15 08:29:49","commitOrder":12,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tassertTrue(SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2020-08-20 07:30:49","endLine":246,"groupId":"18138","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/58fd7331dde7f5275411e70b2fe077b385f6e9.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tassertTrue(SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"},{"authorDate":"2020-08-15 08:29:49","commitOrder":12,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","date":"2020-08-20 07:30:49","endLine":300,"groupId":"32723","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/58fd7331dde7f5275411e70b2fe077b385f6e9.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":255,"status":"M"}],"commitId":"dfb8a3be7f0d113032a28cf6a1b296725e5562f5","commitMessage":"@@@[FLINK-15299][test] Move ClientUtils#submitJob & ClientUtils#submitJobAndWaitForResult to test scope\n\nThis closes #11469 .\n","date":"2020-08-20 07:30:49","modifiedFileCount":"28","status":"M","submitter":"tison"},{"authorTime":"2020-08-15 08:29:49","codes":[{"authorDate":"2021-03-31 16:38:24","commitOrder":13,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-03-31 16:38:24","endLine":272,"groupId":"31403","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/26/de8ec8f181742c22c718176d63b212d6d079a2.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"M"},{"authorDate":"2020-08-15 08:29:49","commitOrder":13,"curCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","date":"2020-08-20 07:30:49","endLine":300,"groupId":"32723","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/58fd7331dde7f5275411e70b2fe077b385f6e9.src","preCode":"\tpublic void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n\t\tfinal int parallelism = numSlots / 2;\n\t\tfinal int parallelism2 = numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\t\t} catch (JobExecutionException exception) {\n\t\t\tif (exception.getCause() instanceof IllegalStateException) {\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t} else {\n\t\t\t\tthrow exception;\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":255,"status":"N"}],"commitId":"4597d5557c640e0ef5a526cbb6d46686be5dd813","commitMessage":"@@@[FLINK-21844][runtime] Do not auto-configure maxParallelism in REACTIVE scheduling mode\n\nThis moves the configuration and management of vertex parallelism into the control of the scheduler.  instead of the ExecutionGraphVertex. This gives the Adaptive scheduler assurances about the task resources when scheduling.\n","date":"2021-03-31 16:38:24","modifiedFileCount":"25","status":"M","submitter":"Austin Cawley-Edwards"},{"authorTime":"2021-04-14 19:54:22","codes":[{"authorDate":"2021-03-31 16:38:24","commitOrder":14,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-03-31 16:38:24","endLine":272,"groupId":"31403","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/26/de8ec8f181742c22c718176d63b212d6d079a2.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"N"},{"authorDate":"2021-04-14 19:54:22","commitOrder":14,"curCode":"    public void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n        final int parallelism = numSlots / 2;\n        final int parallelism2 = numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            \r\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n        } catch (JobExecutionException exception) {\n            if (exception.getCause() instanceof IllegalStateException) {\n                \r\n                \r\n                \r\n            } else {\n                throw exception;\n            }\n        }\n    }\n","date":"2021-04-15 21:01:09","endLine":336,"groupId":"33740","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/ae/da6994fe5fca74412c9d3d8d94c1a655244179.src","preCode":"    public void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n        final int parallelism = numSlots / 2;\n        final int parallelism2 = numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            \r\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n        } catch (JobExecutionException exception) {\n            if (exception.getCause() instanceof IllegalStateException) {\n                \r\n                \r\n                \r\n            } else {\n                throw exception;\n            }\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":282,"status":"M"}],"commitId":"87efae4d3180a52e16240a0b4bbb197f85acd22c","commitMessage":"@@@[FLINK-21941] Make sure jobs do not finish before taking savepoint in RescalingITCase\n","date":"2021-04-15 21:01:09","modifiedFileCount":"1","status":"M","submitter":"Dawid Wysakowicz"},{"authorTime":"2021-06-25 23:31:35","codes":[{"authorDate":"2021-06-25 23:31:35","commitOrder":15,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-06-30 17:43:35","endLine":275,"groupId":"33738","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5e/444689b56eae3385aa4f32f714fd391ef86d25.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"M"},{"authorDate":"2021-06-25 23:31:35","commitOrder":15,"curCode":"    public void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n        final int parallelism = numSlots / 2;\n        final int parallelism2 = numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            \r\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n        } catch (JobExecutionException exception) {\n            if (exception.getCause() instanceof IllegalStateException) {\n                \r\n                \r\n                \r\n            } else {\n                throw exception;\n            }\n        }\n    }\n","date":"2021-06-30 17:43:35","endLine":340,"groupId":"33740","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5e/444689b56eae3385aa4f32f714fd391ef86d25.src","preCode":"    public void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n        final int parallelism = numSlots / 2;\n        final int parallelism2 = numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            \r\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n        } catch (JobExecutionException exception) {\n            if (exception.getCause() instanceof IllegalStateException) {\n                \r\n                \r\n                \r\n            } else {\n                throw exception;\n            }\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":284,"status":"M"}],"commitId":"3b92d67017173547268673d71652a6bc98167abb","commitMessage":"@@@[FLINK-22593][tests] Waiting all tasks running before triggerSavepoint\n","date":"2021-06-30 17:43:35","modifiedFileCount":"3","status":"M","submitter":"Anton Kalashnikov"},{"authorTime":"2021-08-13 08:54:07","codes":[{"authorDate":"2021-08-13 08:54:07","commitOrder":16,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID(), false);\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-08-23 20:24:35","endLine":289,"groupId":"101574","id":31,"instanceNumber":1,"isCurCommit":1,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/55/114557496406880db793aaf65bda729c91c226.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":191,"status":"M"},{"authorDate":"2021-08-13 08:54:07","commitOrder":16,"curCode":"    public void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n        final int parallelism = numSlots / 2;\n        final int parallelism2 = numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID(), false);\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            \r\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n        } catch (JobExecutionException exception) {\n            if (exception.getCause() instanceof IllegalStateException) {\n                \r\n                \r\n                \r\n            } else {\n                throw exception;\n            }\n        }\n    }\n","date":"2021-08-23 20:24:35","endLine":354,"groupId":"101574","id":32,"instanceNumber":2,"isCurCommit":1,"methodName":"testSavepointRescalingNonPartitionedStateCausesException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/55/114557496406880db793aaf65bda729c91c226.src","preCode":"    public void testSavepointRescalingNonPartitionedStateCausesException() throws Exception {\n        final int parallelism = numSlots / 2;\n        final int parallelism2 = numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            \r\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(\n                            parallelism2, maxParallelism, OperatorCheckpointMethod.NON_PARTITIONED);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n        } catch (JobExecutionException exception) {\n            if (exception.getCause() instanceof IllegalStateException) {\n                \r\n                \r\n                \r\n            } else {\n                throw exception;\n            }\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":298,"status":"M"}],"commitId":"136bb52326922dfe70b3cc79fd463f9d6cfecc32","commitMessage":"@@@[FLINK-23811][tests] Handle finished subtasks in CommonTestUtils.waitForAllTaskRunning\n\nCommonTestUtils.waitForAllTaskRunning returns when all the subtasks are running AND\nthe job is running and not finished. However.  with FLIP-147.  subtasks may finish and\nthe job will still be running. So the method won't return and instead timeout.\n\nThis commit adds a flag to indicate whether to fail or proceed\nwhen a finished sub-task is encountered.\n","date":"2021-08-23 20:24:35","modifiedFileCount":"7","status":"M","submitter":"Roman Khachatryan"}]
