[{"authorTime":"2016-10-07 06:21:42","codes":[{"authorDate":"2016-10-07 06:21:42","commitOrder":1,"curCode":"\tpublic static void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":96,"groupId":"4687","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/02/83f68178013da87b98a2223506d11540630117.src","preCode":"\tpublic static void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"B"},{"authorDate":"2016-10-07 06:21:42","commitOrder":1,"curCode":"\tpublic void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":93,"groupId":"4687","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/32/11a208a36275137bfe88099bfd6873d33ca246.src","preCode":"\tpublic void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":74,"status":"B"}],"commitId":"b410c393c960f55c09fadd4f22732d06f801b938","commitMessage":"@@@[FLINK-4800] Introduce the TimestampedFileInputSplit for Continuous File Processing\n\nThis commit mainly introduces the TimestampedFileInputSplit. \nwhich extends the class FileInputSplit and also contains:\ni) the modification time of the file it belongs to and also.  and\nii) when checkpointing.  the point the reader is currently reading\n    from in the split the reader.\n\nThis will be useful for rescaling. With this addition.  the\nContinuousFileMonitoringFunction sends TimestampedFileInputSplit\nto the Readers.  and the Readers' state now contain only\nTimestampedFileInputSplit.\n\nIn addition.  it refactors the code of the ContinuousFileMonitoringFunction\nand that of the ContinuousFileReaderOperator along with the related\ntests.\n\nThis closes #2618.\n","date":"2016-10-27 20:22:02","modifiedFileCount":"5","status":"B","submitter":"kl0u"},{"authorTime":"2016-10-07 06:21:42","codes":[{"authorDate":"2017-02-10 22:14:34","commitOrder":2,"curCode":"\tpublic static void createHDFS() {\n\t\ttry {\n\t\t\tFile hdfsDir = tempFolder.newFolder();\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","date":"2017-02-11 00:07:39","endLine":103,"groupId":"4687","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic static void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"M"},{"authorDate":"2016-10-07 06:21:42","commitOrder":2,"curCode":"\tpublic void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":93,"groupId":"4687","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/32/11a208a36275137bfe88099bfd6873d33ca246.src","preCode":"\tpublic void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":74,"status":"N"}],"commitId":"f6709b4a48a843a0a1818fd59b98d32f82d6184f","commitMessage":"@@@[FLINK-5415] Harden ContinuousFileProcessingTest\n\n- Use TemporaryFolder @ClassRule instead of manually managing HDFS base\ndir.\n- Place files for each test in own sub-directory\n- Harden completeness condition in testFunctionRestore()\n","date":"2017-02-11 00:07:39","modifiedFileCount":"1","status":"M","submitter":"Aljoscha Krettek"},{"authorTime":"2016-10-07 06:21:42","codes":[{"authorDate":"2017-05-13 18:56:56","commitOrder":3,"curCode":"\tpublic static void createHDFS() {\n\t\tAssume.assumeTrue(\"HDFS cluster cannot be start on Windows without extensions.\", !OperatingSystem.isWindows());\n\n\t\ttry {\n\t\t\tFile hdfsDir = tempFolder.newFolder();\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() + \"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch (Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","date":"2017-07-01 15:33:42","endLine":112,"groupId":"4687","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5d/5a1c3d1105413df24cfc671f6f65dd60246363.src","preCode":"\tpublic static void createHDFS() {\n\t\ttry {\n\t\t\tFile hdfsDir = tempFolder.newFolder();\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() + \"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch (Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":92,"status":"M"},{"authorDate":"2016-10-07 06:21:42","commitOrder":3,"curCode":"\tpublic void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":93,"groupId":"4687","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/32/11a208a36275137bfe88099bfd6873d33ca246.src","preCode":"\tpublic void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() +\"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch(Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":74,"status":"N"}],"commitId":"4cac6f4e64d3a9e5ad0e13e9d6dc29f915b52a18","commitMessage":"@@@[FLINK-6575] [tests] Disable tests on Windows that use HDFS\n\nThis closes #6575.\n","date":"2017-07-01 15:33:42","modifiedFileCount":"9","status":"M","submitter":"zentol"},{"authorTime":"2019-12-11 18:21:52","codes":[{"authorDate":"2017-05-13 18:56:56","commitOrder":4,"curCode":"\tpublic static void createHDFS() {\n\t\tAssume.assumeTrue(\"HDFS cluster cannot be start on Windows without extensions.\", !OperatingSystem.isWindows());\n\n\t\ttry {\n\t\t\tFile hdfsDir = tempFolder.newFolder();\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() + \"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch (Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","date":"2017-07-01 15:33:42","endLine":112,"groupId":"101","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5d/5a1c3d1105413df24cfc671f6f65dd60246363.src","preCode":"\tpublic static void createHDFS() {\n\t\tAssume.assumeTrue(\"HDFS cluster cannot be start on Windows without extensions.\", !OperatingSystem.isWindows());\n\n\t\ttry {\n\t\t\tFile hdfsDir = tempFolder.newFolder();\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() + \"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch (Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":92,"status":"N"},{"authorDate":"2019-12-11 18:21:52","commitOrder":4,"curCode":"\tpublic void createHDFS() throws IOException {\n\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\tFileUtil.fullyDelete(baseDir);\n\n\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\thdfsCluster = builder.build();\n\n\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() + \"/\";\n\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\t}\n","date":"2019-12-17 18:44:09","endLine":94,"groupId":"101","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"createHDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/b3/0309610bec1ea32a495017145652b0c367f635.src","preCode":"\tpublic void createHDFS() {\n\t\ttry {\n\t\t\tbaseDir = new File(\"./target/hdfs/hdfsTesting\").getAbsoluteFile();\n\t\t\tFileUtil.fullyDelete(baseDir);\n\n\t\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());\n\t\t\thdConf.set(\"dfs.block.size\", String.valueOf(1048576)); \r\n\n\t\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\t\thdfsCluster = builder.build();\n\n\t\t\thdfsURI = \"hdfs://\" + hdfsCluster.getURI().getHost() + \":\" + hdfsCluster.getNameNodePort() + \"/\";\n\t\t\thdfs = new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);\n\n\t\t} catch (Throwable e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test failed \" + e.getMessage());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":81,"status":"M"}],"commitId":"ce214d2deb763d4596ba60b24cd0783c31050af9","commitMessage":"@@@[hotfix][tests] Remove unnecessary try-catch usage\n","date":"2019-12-17 18:44:09","modifiedFileCount":"1","status":"M","submitter":"Gary Yao"}]
