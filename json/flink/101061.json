[{"authorTime":"2020-06-03 10:38:29","codes":[{"authorDate":"2020-06-03 10:38:29","commitOrder":1,"curCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = Lists.newArrayList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","date":"2020-06-03 10:38:29","endLine":200,"groupId":"45963","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testReadPartitionTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/31/7943ad498b940b0c5e615cca86688171e06bb1.src","preCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = Lists.newArrayList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":180,"status":"B"},{"authorDate":"2020-06-03 10:38:29","commitOrder":1,"curCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tString physicalExecutionPlan = explain[3];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\tassertTrue(physicalExecutionPlan, physicalExecutionPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\t\r\n\t\tList<Row> rows = Lists.newArrayList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","date":"2020-06-03 10:38:29","endLine":232,"groupId":"41572","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionPrunning","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/31/7943ad498b940b0c5e615cca86688171e06bb1.src","preCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tString physicalExecutionPlan = explain[3];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\tassertTrue(physicalExecutionPlan, physicalExecutionPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\t\r\n\t\tList<Row> rows = Lists.newArrayList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":203,"status":"B"}],"commitId":"21cb58625b607cf227cfd298595fdda3ed7c579a","commitMessage":"@@@[FLINK-17937][hive] Change some hive connector tests to IT cases\n\n\nThis closes #12333","date":"2020-06-03 10:38:29","modifiedFileCount":"0","status":"B","submitter":"Rui Li"},{"authorTime":"2020-06-18 11:52:35","codes":[{"authorDate":"2020-06-18 11:52:35","commitOrder":2,"curCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","date":"2020-09-07 17:37:11","endLine":197,"groupId":"33230","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testReadPartitionTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/52/6a45498a992d05b79faa9039e1e7c21234f28c.src","preCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = Lists.newArrayList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"M"},{"authorDate":"2020-06-18 11:52:35","commitOrder":2,"curCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tString physicalExecutionPlan = explain[3];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\tassertTrue(physicalExecutionPlan, physicalExecutionPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\t\r\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","date":"2020-09-07 17:37:11","endLine":229,"groupId":"41572","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionPrunning","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/52/6a45498a992d05b79faa9039e1e7c21234f28c.src","preCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tString physicalExecutionPlan = explain[3];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\tassertTrue(physicalExecutionPlan, physicalExecutionPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\t\r\n\t\tList<Row> rows = Lists.newArrayList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":200,"status":"M"}],"commitId":"91d2b628bfe1a2e7beed5111a6d9a572cc6bc310","commitMessage":"@@@[hotfix][table][connector] Use CollectionUtil#iteratorToList instead of Guava Lists\n","date":"2020-09-07 17:37:11","modifiedFileCount":"18","status":"M","submitter":"godfreyhe"},{"authorTime":"2020-10-28 09:54:12","codes":[{"authorDate":"2020-06-18 11:52:35","commitOrder":3,"curCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","date":"2020-09-07 17:37:11","endLine":197,"groupId":"33230","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testReadPartitionTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/52/6a45498a992d05b79faa9039e1e7c21234f28c.src","preCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"N"},{"authorDate":"2020-10-28 09:54:12","commitOrder":3,"curCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"table=[[hive, source_db, test_table_pt_1, partitions=[{pt=0}], project=[year, value]]]\"));\n\t\t\r\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","date":"2020-10-28 09:54:12","endLine":221,"groupId":"41572","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionPrunning","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/64/14bbb01610a799b5cb10b4f7e69b1843a70dcf.src","preCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tString physicalExecutionPlan = explain[3];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\tassertTrue(physicalExecutionPlan, physicalExecutionPlan.contains(\n\t\t\t\t\"HiveTableSource(year, value, pt) TablePath: source_db.test_table_pt_1, PartitionPruned: true, PartitionNums: 1\"));\n\t\t\r\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":195,"status":"M"}],"commitId":"0a14ad1cc47c4c6d4de0d5c90d3cd9578ca2536c","commitMessage":"@@@[FLINK-19789][hive] Migrate Hive connector to new table source sink interface\n\nThis closes #13771","date":"2020-10-28 09:54:12","modifiedFileCount":"10","status":"M","submitter":"Jingsong Lee"},{"authorTime":"2020-11-24 10:53:43","codes":[{"authorDate":"2020-11-24 10:53:43","commitOrder":4,"curCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tbatchTableEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = batchTableEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","date":"2020-11-24 10:53:43","endLine":185,"groupId":"5789","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testReadPartitionTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/9c/46eb973413634093e227110599b6dfeb859422.src","preCode":"\tpublic void testReadPartitionTable() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\n\t\tassertEquals(4, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":166,"status":"M"},{"authorDate":"2020-11-24 10:53:43","commitOrder":4,"curCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tbatchTableEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = batchTableEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"table=[[hive, source_db, test_table_pt_1, partitions=[{pt=0}], project=[year, value]]]\"));\n\t\t\r\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","date":"2020-11-24 10:53:43","endLine":213,"groupId":"41572","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionPrunning","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/9c/46eb973413634093e227110599b6dfeb859422.src","preCode":"\tpublic void testPartitionPrunning() throws Exception {\n\t\tfinal String dbName = \"source_db\";\n\t\tfinal String tblName = \"test_table_pt_1\";\n\t\tTableEnvironment tEnv = createTableEnv();\n\t\ttEnv.executeSql(\"CREATE TABLE source_db.test_table_pt_1 \" +\n\t\t\t\t\t\t\"(`year` STRING, `value` INT) partitioned by (pt int)\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2014\", 3})\n\t\t\t\t.addRow(new Object[]{\"2014\", 4})\n\t\t\t\t.commit(\"pt=0\");\n\t\tHiveTestUtils.createTextTableInserter(hiveShell, dbName, tblName)\n\t\t\t\t.addRow(new Object[]{\"2015\", 2})\n\t\t\t\t.addRow(new Object[]{\"2015\", 5})\n\t\t\t\t.commit(\"pt=1\");\n\t\tTable src = tEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n\t\t\r\n\t\tString[] explain = src.explain().split(\"==.*==\\n\");\n\t\tassertEquals(4, explain.length);\n\t\tString optimizedLogicalPlan = explain[2];\n\t\tassertTrue(optimizedLogicalPlan, optimizedLogicalPlan.contains(\n\t\t\t\t\"table=[[hive, source_db, test_table_pt_1, partitions=[{pt=0}], project=[year, value]]]\"));\n\t\t\r\n\t\tList<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\t\tassertEquals(2, rows.size());\n\t\tObject[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n\t\tassertArrayEquals(new String[]{\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n\t}\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":188,"status":"M"}],"commitId":"e9b05e723fb02d9a6dae607ef28244eca6e9edc8","commitMessage":"@@@[FLINK-19653][hive] Reduce our dependency on hive runner for tests\n\nThis closes #14123","date":"2020-11-24 10:53:43","modifiedFileCount":"8","status":"M","submitter":"Rui Li"},{"authorTime":"2020-12-18 18:32:55","codes":[{"authorDate":"2020-12-18 18:32:55","commitOrder":5,"curCode":"    public void testReadPartitionTable() throws Exception {\n        final String dbName = \"source_db\";\n        final String tblName = \"test_table_pt\";\n        batchTableEnv.executeSql(\n                \"CREATE TABLE source_db.test_table_pt \"\n                        + \"(`year` STRING, `value` INT) partitioned by (pt int)\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2014\", 3})\n                .addRow(new Object[] {\"2014\", 4})\n                .commit(\"pt=0\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2015\", 2})\n                .addRow(new Object[] {\"2015\", 5})\n                .commit(\"pt=1\");\n        Table src = batchTableEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n        List<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\n        assertEquals(4, rows.size());\n        Object[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n        assertArrayEquals(\n                new String[] {\n                    \"+I[2014, 3, 0]\", \"+I[2014, 4, 0]\", \"+I[2015, 2, 1]\", \"+I[2015, 5, 1]\"\n                },\n                rowStrings);\n    }\n","date":"2021-01-08 00:17:30","endLine":191,"groupId":"101061","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testReadPartitionTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/dd/1bb79a9390eaafc5cd08289e675b44b548395e.src","preCode":"    public void testReadPartitionTable() throws Exception {\n        final String dbName = \"source_db\";\n        final String tblName = \"test_table_pt\";\n        batchTableEnv.executeSql(\n                \"CREATE TABLE source_db.test_table_pt \"\n                        + \"(`year` STRING, `value` INT) partitioned by (pt int)\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2014\", 3})\n                .addRow(new Object[] {\"2014\", 4})\n                .commit(\"pt=0\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2015\", 2})\n                .addRow(new Object[] {\"2015\", 5})\n                .commit(\"pt=1\");\n        Table src = batchTableEnv.sqlQuery(\"select * from hive.source_db.test_table_pt\");\n        List<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n\n        assertEquals(4, rows.size());\n        Object[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n        assertArrayEquals(\n                new String[] {\"2014,3,0\", \"2014,4,0\", \"2015,2,1\", \"2015,5,1\"}, rowStrings);\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":167,"status":"M"},{"authorDate":"2020-12-18 18:32:55","commitOrder":5,"curCode":"    public void testPartitionPrunning() throws Exception {\n        final String dbName = \"source_db\";\n        final String tblName = \"test_table_pt_1\";\n        batchTableEnv.executeSql(\n                \"CREATE TABLE source_db.test_table_pt_1 \"\n                        + \"(`year` STRING, `value` INT) partitioned by (pt int)\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2014\", 3})\n                .addRow(new Object[] {\"2014\", 4})\n                .commit(\"pt=0\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2015\", 2})\n                .addRow(new Object[] {\"2015\", 5})\n                .commit(\"pt=1\");\n        Table src =\n                batchTableEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n        \r\n        String[] explain = src.explain().split(\"==.*==\\n\");\n        assertEquals(4, explain.length);\n        String optimizedLogicalPlan = explain[2];\n        assertTrue(\n                optimizedLogicalPlan,\n                optimizedLogicalPlan.contains(\n                        \"table=[[hive, source_db, test_table_pt_1, partitions=[{pt=0}], project=[year, value]]]\"));\n        \r\n        List<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n        assertEquals(2, rows.size());\n        Object[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n        assertArrayEquals(new String[] {\"+I[2014, 3, 0]\", \"+I[2014, 4, 0]\"}, rowStrings);\n    }\n","date":"2021-01-08 00:17:30","endLine":223,"groupId":"101061","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartitionPrunning","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/dd/1bb79a9390eaafc5cd08289e675b44b548395e.src","preCode":"    public void testPartitionPrunning() throws Exception {\n        final String dbName = \"source_db\";\n        final String tblName = \"test_table_pt_1\";\n        batchTableEnv.executeSql(\n                \"CREATE TABLE source_db.test_table_pt_1 \"\n                        + \"(`year` STRING, `value` INT) partitioned by (pt int)\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2014\", 3})\n                .addRow(new Object[] {\"2014\", 4})\n                .commit(\"pt=0\");\n        HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)\n                .addRow(new Object[] {\"2015\", 2})\n                .addRow(new Object[] {\"2015\", 5})\n                .commit(\"pt=1\");\n        Table src =\n                batchTableEnv.sqlQuery(\"select * from hive.source_db.test_table_pt_1 where pt = 0\");\n        \r\n        String[] explain = src.explain().split(\"==.*==\\n\");\n        assertEquals(4, explain.length);\n        String optimizedLogicalPlan = explain[2];\n        assertTrue(\n                optimizedLogicalPlan,\n                optimizedLogicalPlan.contains(\n                        \"table=[[hive, source_db, test_table_pt_1, partitions=[{pt=0}], project=[year, value]]]\"));\n        \r\n        List<Row> rows = CollectionUtil.iteratorToList(src.execute().collect());\n        assertEquals(2, rows.size());\n        Object[] rowStrings = rows.stream().map(Row::toString).sorted().toArray();\n        assertArrayEquals(new String[] {\"2014,3,0\", \"2014,4,0\"}, rowStrings);\n    }\n","realPath":"flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":194,"status":"M"}],"commitId":"73cdd3d0d9f6a807b3e47c09eef7983c9aa180c7","commitMessage":"@@@[FLINK-18090] Update tests for new Row.toString\n\nAll tests in modules apart from the Blink planner/runtime\nmodule have been updated.\n\nOtherwise we use a JUnit rule to make the migration of\nthe remaining tests incremental.\n\nThis closes #14568.\n","date":"2021-01-08 00:17:30","modifiedFileCount":"34","status":"M","submitter":"Timo Walther"}]
