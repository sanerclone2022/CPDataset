[{"authorTime":"2016-10-06 22:43:42","codes":[{"authorDate":"2016-10-06 22:43:42","commitOrder":2,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":223,"groupId":"53398","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":126,"status":"MB"},{"authorDate":"2016-10-06 22:43:42","commitOrder":2,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, true);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, true);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\tsumExp += c;\n\t\t\t}\n\n\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\tsumAct += c;\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":548,"groupId":"46801","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, true);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, true);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\tsumExp += c;\n\t\t\t}\n\n\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\tsumAct += c;\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":456,"status":"MB"}],"commitId":"fd410d9f6d1c8d53fb721752528ebd77fd78db57","commitMessage":"@@@[FLINK-4512] [FLIP-10] Add option to persist periodic checkpoints\n\n[FLINK-4509] [FLIP-10] Specify savepoint directory per savepoint\n[FLINK-4507] [FLIP-10] Deprecate savepoint backend config\n\nThis closes #2608.\n","date":"2016-10-14 16:05:06","modifiedFileCount":"50","status":"M","submitter":"Ufuk Celebi"},{"authorTime":"2016-10-04 16:59:38","codes":[{"authorDate":"2016-10-06 22:43:42","commitOrder":3,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-14 16:05:06","endLine":223,"groupId":"53398","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/48/d720a0e0c0c46bf1b54d675144ceb5beceddea.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":126,"status":"N"},{"authorDate":"2016-10-04 16:59:38","commitOrder":3,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif(checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-10-20 22:14:21","endLine":574,"groupId":"21856","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/95/115d65b56d0638f1dd1ef61cf5c1082fdde3c1.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, true);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, true);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\tsumExp += c;\n\t\t\t}\n\n\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\tsumAct += c;\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":467,"status":"M"}],"commitId":"cab9cd44eca83ef8cbcd2a2d070d8c79cb037977","commitMessage":"@@@[FLINK-4844] Partitionable Raw Keyed/Operator State\n","date":"2016-10-20 22:14:21","modifiedFileCount":"87","status":"M","submitter":"Stefan Richter"},{"authorTime":"2016-10-27 00:05:26","codes":[{"authorDate":"2016-10-27 00:05:26","commitOrder":4,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-11-02 14:34:21","endLine":231,"groupId":"17456","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5a/6417329010ded63de2a627feec65dd6d6eb3d8.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":134,"status":"M"},{"authorDate":"2016-10-27 00:05:26","commitOrder":4,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif(checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-11-02 14:34:21","endLine":569,"groupId":"21856","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5a/6417329010ded63de2a627feec65dd6d6eb3d8.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif(checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointPath(savepointPath);\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":462,"status":"M"}],"commitId":"c0e620f0ace0aa3500a5642e7165cf9f05e81f6a","commitMessage":"@@@[FLINK-4445] [checkpointing] Add option to allow non restored checkpoint state\n\nAllows to skip checkpoint state that cannot be mapped to a job vertex when\nrestoring.\n\nThis closes #2712.\n","date":"2016-11-02 14:34:21","modifiedFileCount":"12","status":"M","submitter":"Ufuk Celebi"},{"authorTime":"2016-12-03 09:42:25","codes":[{"authorDate":"2016-10-27 00:05:26","commitOrder":5,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2016-11-02 14:34:21","endLine":231,"groupId":"17456","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5a/6417329010ded63de2a627feec65dd6d6eb3d8.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":134,"status":"N"},{"authorDate":"2016-12-03 09:42:25","commitOrder":5,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tSystem.out.println(savepointResponse);\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2017-01-14 04:29:19","endLine":593,"groupId":"52863","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/45/fcc2533f942890da33322c9c2bb6c52870d632.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif(checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"M"}],"commitId":"1020ba2c9cfc1d01703e97c72e20a922bae0732d","commitMessage":"@@@[FLINK-5265] Introduce state handle replication mode for CheckpointCoordinator\n","date":"2017-01-14 04:29:19","modifiedFileCount":"17","status":"M","submitter":"Stefan Richter"},{"authorTime":"2016-12-03 09:42:25","codes":[{"authorDate":"2017-01-16 21:31:22","commitOrder":6,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2017-01-24 22:51:35","endLine":247,"groupId":"45929","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/07/3632ad00d86ebd05b0618eecf5e0447688e5b7.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, maxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":148,"status":"M"},{"authorDate":"2016-12-03 09:42:25","commitOrder":6,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tSystem.out.println(savepointResponse);\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2017-01-14 04:29:19","endLine":593,"groupId":"52863","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/45/fcc2533f942890da33322c9c2bb6c52870d632.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tSystem.out.println(savepointResponse);\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"N"}],"commitId":"acfeeaf5e337e56300d10a3a991e79edc827ac7a","commitMessage":"@@@[FLINK-5473] Limit max parallelism to 1 for non-parallel operators\n\n[FLINK-5473] Better default behaviours for unspecified maximum parallelism\n\nThis closes #3182.\n","date":"2017-01-24 22:51:35","modifiedFileCount":"34","status":"M","submitter":"Stefan Richter"},{"authorTime":"2017-05-31 03:40:47","codes":[{"authorDate":"2017-01-16 21:31:22","commitOrder":7,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2017-01-24 22:51:35","endLine":247,"groupId":"45929","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/07/3632ad00d86ebd05b0618eecf5e0447688e5b7.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":148,"status":"N"},{"authorDate":"2017-05-31 03:40:47","commitOrder":7,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tSystem.out.println(savepointResponse);\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2017-07-13 06:37:47","endLine":629,"groupId":"52863","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/ca/d669364fdbe99dd80339cdbc2656123c7c7c9b.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSource.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tSystem.out.println(savepointResponse);\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_SNAPSHOT) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.CHECK_CORRECT_RESTORE) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"M"}],"commitId":"9bd491e05120915cbde36d4452e3982fe5d0975f","commitMessage":"@@@[FLINK-6731] [tests] Activate strict checkstyle for flink-tests\n\nThis closes #4295\n","date":"2017-07-13 06:37:47","modifiedFileCount":"185","status":"M","submitter":"Greg Hogan"},{"authorTime":"2018-02-06 21:44:01","codes":[{"authorDate":"2017-01-16 21:31:22","commitOrder":8,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2017-01-24 22:51:35","endLine":247,"groupId":"45929","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/07/3632ad00d86ebd05b0618eecf5e0447688e5b7.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":148,"status":"N"},{"authorDate":"2018-02-06 21:44:01","commitOrder":8,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","date":"2018-02-25 22:14:21","endLine":626,"groupId":"0","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/a2/3c679e65f2b37331aa2c6dde179ea2dbc73d46.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tSystem.out.println(savepointResponse);\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":508,"status":"M"}],"commitId":"df3e6bb7627db03635febd48eff4c10032b668ef","commitMessage":"@@@[FLINK-8360][checkpointing] Implement state storage for local recovery and integrate with task lifecycle\n","date":"2018-02-25 22:14:21","modifiedFileCount":"107","status":"M","submitter":"Stefan Richter"},{"authorTime":"2018-03-19 18:36:39","codes":[{"authorDate":"2018-03-19 18:36:39","commitOrder":9,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2018-03-24 02:11:49","endLine":246,"groupId":"48539","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/e4/f4389bb6abc56bd4f91e9dc2cce1e0784167b8.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tActorGateway jobManager = null;\n\t\tJobID jobID = null;\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess)\n\t\t\t\t\tAwait.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\tjobID = null;\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tjobID = null;\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":171,"status":"M"},{"authorDate":"2018-03-19 18:36:39","commitOrder":9,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\treturn client.triggerSavepoint(jobID, null);\n\t\t\t\t\t} catch (FlinkException e) {\n\t\t\t\t\t\treturn FutureUtils.completedExceptionally(e);\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","date":"2018-03-24 02:11:49","endLine":530,"groupId":"32275","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/e4/f4389bb6abc56bd4f91e9dc2cce1e0784167b8.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tFiniteDuration timeout = new FiniteDuration(3, TimeUnit.MINUTES);\n\t\tDeadline deadline = timeout.fromNow();\n\n\t\tJobID jobID = null;\n\t\tActorGateway jobManager = null;\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tjobManager = cluster.getLeaderGateway(deadline.timeLeft());\n\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tjobID = jobGraph.getJobID();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\n\t\t\tObject savepointResponse = null;\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\twhile (deadline.hasTimeLeft()) {\n\n\t\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID, Option.<String>empty()), deadline.timeLeft());\n\t\t\t\tFiniteDuration waitingTime = new FiniteDuration(10, TimeUnit.SECONDS);\n\t\t\t\tsavepointResponse = Await.result(savepointPathFuture, waitingTime);\n\n\t\t\t\tif (savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertTrue(savepointResponse instanceof JobManagerMessages.TriggerSavepointSuccess);\n\n\t\t\tfinal String savepointPath = ((JobManagerMessages.TriggerSavepointSuccess) savepointResponse).savepointPath();\n\n\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), deadline.timeLeft());\n\n\t\t\tFuture<Object> cancellationResponseFuture = jobManager.ask(new JobManagerMessages.CancelJob(jobID), deadline.timeLeft());\n\n\t\t\tObject cancellationResponse = Await.result(cancellationResponseFuture, deadline.timeLeft());\n\n\t\t\tassertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);\n\n\t\t\tAwait.ready(jobRemovedFuture, deadline.timeLeft());\n\n\t\t\t\r\n\t\t\tjobID = null;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tjobID = scaledJobGraph.getJobID();\n\n\t\t\tcluster.submitJobAndWait(scaledJobGraph, false);\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t\tjobID = null;\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tif (jobID != null && jobManager != null) {\n\t\t\t\tFuture<Object> jobRemovedFuture = jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID), timeout);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.ready(jobRemovedFuture, timeout);\n\t\t\t\t} catch (TimeoutException | InterruptedException ie) {\n\t\t\t\t\tfail(\"Failed while cleaning up the cluster.\");\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":435,"status":"M"}],"commitId":"edb6f7fef8c5df6af43bbe28f96d8c6bb3332d00","commitMessage":"@@@[FLINK-8956][tests] Port RescalingITCase to flip6\n\nThis closes #5715.\n","date":"2018-03-24 02:11:49","modifiedFileCount":"1","status":"M","submitter":"zentol"},{"authorTime":"2019-11-01 14:51:28","codes":[{"authorDate":"2019-11-01 14:51:28","commitOrder":10,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2019-11-01 14:51:28","endLine":249,"groupId":"48539","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cf/1b00a5d2fefb67d804f6f6c5eefe557d94c8d3.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"},{"authorDate":"2019-11-01 14:51:28","commitOrder":10,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\treturn client.triggerSavepoint(jobID, null);\n\t\t\t\t\t} catch (FlinkException e) {\n\t\t\t\t\t\treturn FutureUtils.completedExceptionally(e);\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","date":"2019-11-01 14:51:28","endLine":527,"groupId":"2022","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cf/1b00a5d2fefb67d804f6f6c5eefe557d94c8d3.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.setDetached(true);\n\t\t\tclient.submitJob(jobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\treturn client.triggerSavepoint(jobID, null);\n\t\t\t\t\t} catch (FlinkException e) {\n\t\t\t\t\t\treturn FutureUtils.completedExceptionally(e);\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tclient.setDetached(false);\n\t\t\tclient.submitJob(scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":434,"status":"M"}],"commitId":"bf5235e340543b9c4551d2131e8a405bd1e9e0c0","commitMessage":"@@@[FLINK-14496][client] Exclude detach flag from ClusterClient\n\nThis closes #9972 .","date":"2019-11-01 14:51:28","modifiedFileCount":"37","status":"M","submitter":"tison"},{"authorTime":"2019-11-08 10:23:59","codes":[{"authorDate":"2019-11-08 10:23:59","commitOrder":11,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2019-11-08 10:23:59","endLine":249,"groupId":"48539","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/92/3933fbfc07fbdb138584bed9c14182277a926d.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"},{"authorDate":"2019-11-08 10:23:59","commitOrder":11,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\treturn client.triggerSavepoint(jobID, null);\n\t\t\t\t\t} catch (FlinkException e) {\n\t\t\t\t\t\treturn FutureUtils.completedExceptionally(e);\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","date":"2019-11-08 10:23:59","endLine":527,"groupId":"0","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/92/3933fbfc07fbdb138584bed9c14182277a926d.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\treturn client.triggerSavepoint(jobID, null);\n\t\t\t\t\t} catch (FlinkException e) {\n\t\t\t\t\t\treturn FutureUtils.completedExceptionally(e);\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID);\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":434,"status":"M"}],"commitId":"d938c19480c220344827271ff8da729cd91735b3","commitMessage":"@@@[FLINK-14593][client] Port ClusterClient to asynchronous interface version\n\nThis closes #10069 .\n","date":"2019-11-08 10:23:59","modifiedFileCount":"27","status":"M","submitter":"tison"},{"authorTime":"2019-11-29 09:55:23","codes":[{"authorDate":"2019-11-08 10:23:59","commitOrder":12,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2019-11-08 10:23:59","endLine":249,"groupId":"48539","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/92/3933fbfc07fbdb138584bed9c14182277a926d.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"N"},{"authorDate":"2019-11-29 09:55:23","commitOrder":12,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> client.triggerSavepoint(jobID, null),\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","date":"2019-11-29 21:51:53","endLine":520,"groupId":"2022","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/f0/f0507374a86fb5d1b00c549d9a0b2740ab6132.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\treturn client.triggerSavepoint(jobID, null);\n\t\t\t\t\t} catch (FlinkException e) {\n\t\t\t\t\t\treturn FutureUtils.completedExceptionally(e);\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":433,"status":"M"}],"commitId":"cf4de13474804e84535cba2dd93b1fa980d71652","commitMessage":"@@@[FLINK-14762][client] Implement JobClient#triggerSavepoint\n","date":"2019-11-29 21:51:53","modifiedFileCount":"6","status":"M","submitter":"tison"},{"authorTime":"2019-11-29 09:55:23","codes":[{"authorDate":"2020-02-04 03:26:55","commitOrder":13,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tassertTrue(SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2020-03-05 01:44:42","endLine":249,"groupId":"48539","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/80/d0cb57bf07883a0eabc0a52673827a0290bb47.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tSubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"},{"authorDate":"2019-11-29 09:55:23","commitOrder":13,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> client.triggerSavepoint(jobID, null),\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","date":"2019-11-29 21:51:53","endLine":520,"groupId":"2022","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/f0/f0507374a86fb5d1b00c549d9a0b2740ab6132.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> client.triggerSavepoint(jobID, null),\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":433,"status":"N"}],"commitId":"b098ce505176720ba37da8f6d6c23096b1d3a260","commitMessage":"@@@[FLINK-15838] Dangling CountDownLatch.await(timeout)\n\nThis closes #11005.\n","date":"2020-03-05 01:44:42","modifiedFileCount":"7","status":"M","submitter":"Ayush Saxena"},{"authorTime":"2020-08-15 08:29:49","codes":[{"authorDate":"2020-08-15 08:29:49","commitOrder":14,"curCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tassertTrue(SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","date":"2020-08-20 07:30:49","endLine":246,"groupId":"18138","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/58fd7331dde7f5275411e70b2fe077b385f6e9.src","preCode":"\tpublic void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism) throws Exception {\n\t\tfinal int numberKeys = 42;\n\t\tfinal int numberElements = 1000;\n\t\tfinal int numberElements2 = 500;\n\t\tfinal int parallelism = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithKeyedState(parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tassertTrue(SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n\t\t\t\r\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n\t\t\t\texpectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism, keyGroupIndex), numberElements * key));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult, actualResult);\n\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tint restoreMaxParallelism = deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithKeyedState(parallelism2, restoreMaxParallelism, numberKeys, numberElements2, true, 100);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tSet<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n\t\t\tSet<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n\t\t\tfor (int key = 0; key < numberKeys; key++) {\n\t\t\t\tint keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\t\t\t\texpectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism, parallelism2, keyGroupIndex), key * (numberElements + numberElements2)));\n\t\t\t}\n\n\t\t\tassertEquals(expectedResult2, actualResult2);\n\n\t\t} finally {\n\t\t\t\r\n\t\t\tCollectionSink.clearElementsSet();\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"},{"authorDate":"2020-08-15 08:29:49","commitOrder":14,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> client.triggerSavepoint(jobID, null),\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","date":"2020-08-20 07:30:49","endLine":518,"groupId":"32721","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/58fd7331dde7f5275411e70b2fe077b385f6e9.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tClientUtils.submitJob(client, jobGraph);\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> client.triggerSavepoint(jobID, null),\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tClientUtils.submitJobAndWaitForResult(client, scaledJobGraph, RescalingITCase.class.getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":431,"status":"M"}],"commitId":"dfb8a3be7f0d113032a28cf6a1b296725e5562f5","commitMessage":"@@@[FLINK-15299][test] Move ClientUtils#submitJob & ClientUtils#submitJobAndWaitForResult to test scope\n\nThis closes #11469 .\n","date":"2020-08-20 07:30:49","modifiedFileCount":"28","status":"M","submitter":"tison"},{"authorTime":"2020-08-15 08:29:49","codes":[{"authorDate":"2021-03-31 16:38:24","commitOrder":15,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-03-31 16:38:24","endLine":272,"groupId":"31403","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/26/de8ec8f181742c22c718176d63b212d6d079a2.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? ExecutionJobVertex.VALUE_NOT_SET : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"M"},{"authorDate":"2020-08-15 08:29:49","commitOrder":15,"curCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> client.triggerSavepoint(jobID, null),\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","date":"2020-08-20 07:30:49","endLine":518,"groupId":"32721","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/be/58fd7331dde7f5275411e70b2fe077b385f6e9.src","preCode":"\tpublic void testSavepointRescalingPartitionedOperatorState(boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n\t\tfinal int parallelism = scaleOut ? numSlots : numSlots / 2;\n\t\tfinal int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n\t\tfinal int maxParallelism = 13;\n\n\t\tDuration timeout = Duration.ofMinutes(3);\n\t\tDeadline deadline = Deadline.now().plus(timeout);\n\n\t\tClusterClient<?> client = cluster.getClusterClient();\n\n\t\tint counterSize = Math.max(parallelism, parallelism2);\n\n\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION ||\n\t\t\t\tcheckpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\tPartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSource.checkCorrectRestore = new int[counterSize];\n\t\t} else {\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n\t\t\tPartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n\t\t}\n\n\t\ttry {\n\t\t\tJobGraph jobGraph = createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n\t\t\tfinal JobID jobID = jobGraph.getJobID();\n\n\t\t\tclient.submitJob(jobGraph).get();\n\n\t\t\t\r\n\t\t\tStateSourceBase.workStartedLatch.await();\n\n\t\t\tCompletableFuture<String> savepointPathFuture = FutureUtils.retryWithDelay(\n\t\t\t\t() -> client.triggerSavepoint(jobID, null),\n\t\t\t\t(int) deadline.timeLeft().getSeconds() / 10,\n\t\t\t\tTime.seconds(10),\n\t\t\t\t(throwable) -> true,\n\t\t\t\tTestingUtils.defaultScheduledExecutor()\n\t\t\t);\n\n\t\t\tfinal String savepointPath = savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n\t\t\tclient.cancel(jobID).get();\n\n\t\t\twhile (!getRunningJobs(client).isEmpty()) {\n\t\t\t\tThread.sleep(50);\n\t\t\t}\n\n\t\t\tJobGraph scaledJobGraph = createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n\t\t\tscaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tsubmitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n\t\t\tint sumExp = 0;\n\t\t\tint sumAct = 0;\n\n\t\t\tif (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t} else if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSource.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\n\t\t\t\tsumExp *= parallelism2;\n\t\t\t} else {\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n\t\t\t\t\tsumExp += c;\n\t\t\t\t}\n\n\t\t\t\tfor (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n\t\t\t\t\tsumAct += c;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassertEquals(sumExp, sumAct);\n\t\t} finally {\n\t\t}\n\t}\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":431,"status":"N"}],"commitId":"4597d5557c640e0ef5a526cbb6d46686be5dd813","commitMessage":"@@@[FLINK-21844][runtime] Do not auto-configure maxParallelism in REACTIVE scheduling mode\n\nThis moves the configuration and management of vertex parallelism into the control of the scheduler.  instead of the ExecutionGraphVertex. This gives the Adaptive scheduler assurances about the task resources when scheduling.\n","date":"2021-03-31 16:38:24","modifiedFileCount":"25","status":"M","submitter":"Austin Cawley-Edwards"},{"authorTime":"2021-04-14 19:54:22","codes":[{"authorDate":"2021-03-31 16:38:24","commitOrder":16,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-03-31 16:38:24","endLine":272,"groupId":"31403","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/26/de8ec8f181742c22c718176d63b212d6d079a2.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"N"},{"authorDate":"2021-04-14 19:54:22","commitOrder":16,"curCode":"    public void testSavepointRescalingPartitionedOperatorState(\n            boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n        final int parallelism = scaleOut ? numSlots : numSlots / 2;\n        final int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        int counterSize = Math.max(parallelism, parallelism2);\n\n        if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION\n                || checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n            PartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSource.checkCorrectRestore = new int[counterSize];\n        } else {\n            PartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n        }\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture =\n                    FutureUtils.retryWithDelay(\n                            () -> client.triggerSavepoint(jobID, null),\n                            (int) deadline.timeLeft().getSeconds() / 10,\n                            Time.seconds(10),\n                            (throwable) -> true,\n                            TestingUtils.defaultScheduledExecutor());\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            int sumExp = 0;\n            int sumAct = 0;\n\n            if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            } else if (checkpointMethod\n                    == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n\n                sumExp *= parallelism2;\n            } else {\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            }\n\n            assertEquals(sumExp, sumAct);\n        } finally {\n        }\n    }\n","date":"2021-04-15 21:01:09","endLine":589,"groupId":"31404","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/ae/da6994fe5fca74412c9d3d8d94c1a655244179.src","preCode":"    public void testSavepointRescalingPartitionedOperatorState(\n            boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n        final int parallelism = scaleOut ? numSlots : numSlots / 2;\n        final int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        int counterSize = Math.max(parallelism, parallelism2);\n\n        if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION\n                || checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n            PartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSource.checkCorrectRestore = new int[counterSize];\n        } else {\n            PartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n        }\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture =\n                    FutureUtils.retryWithDelay(\n                            () -> client.triggerSavepoint(jobID, null),\n                            (int) deadline.timeLeft().getSeconds() / 10,\n                            Time.seconds(10),\n                            (throwable) -> true,\n                            TestingUtils.defaultScheduledExecutor());\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            int sumExp = 0;\n            int sumAct = 0;\n\n            if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            } else if (checkpointMethod\n                    == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n\n                sumExp *= parallelism2;\n            } else {\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            }\n\n            assertEquals(sumExp, sumAct);\n        } finally {\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":493,"status":"M"}],"commitId":"87efae4d3180a52e16240a0b4bbb197f85acd22c","commitMessage":"@@@[FLINK-21941] Make sure jobs do not finish before taking savepoint in RescalingITCase\n","date":"2021-04-15 21:01:09","modifiedFileCount":"1","status":"M","submitter":"Dawid Wysakowicz"},{"authorTime":"2021-06-25 23:31:35","codes":[{"authorDate":"2021-06-25 23:31:35","commitOrder":17,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-06-30 17:43:35","endLine":275,"groupId":"33738","id":31,"instanceNumber":1,"isCurCommit":0,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5e/444689b56eae3385aa4f32f714fd391ef86d25.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"M"},{"authorDate":"2021-06-25 23:31:35","commitOrder":17,"curCode":"    public void testSavepointRescalingPartitionedOperatorState(\n            boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n        final int parallelism = scaleOut ? numSlots : numSlots / 2;\n        final int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        int counterSize = Math.max(parallelism, parallelism2);\n\n        if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION\n                || checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n            PartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSource.checkCorrectRestore = new int[counterSize];\n        } else {\n            PartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n        }\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture =\n                    FutureUtils.retryWithDelay(\n                            () -> client.triggerSavepoint(jobID, null),\n                            (int) deadline.timeLeft().getSeconds() / 10,\n                            Time.seconds(10),\n                            (throwable) -> true,\n                            TestingUtils.defaultScheduledExecutor());\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            int sumExp = 0;\n            int sumAct = 0;\n\n            if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            } else if (checkpointMethod\n                    == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n\n                sumExp *= parallelism2;\n            } else {\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            }\n\n            assertEquals(sumExp, sumAct);\n        } finally {\n        }\n    }\n","date":"2021-06-30 17:43:35","endLine":596,"groupId":"23512","id":32,"instanceNumber":2,"isCurCommit":0,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/5e/444689b56eae3385aa4f32f714fd391ef86d25.src","preCode":"    public void testSavepointRescalingPartitionedOperatorState(\n            boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n        final int parallelism = scaleOut ? numSlots : numSlots / 2;\n        final int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        int counterSize = Math.max(parallelism, parallelism2);\n\n        if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION\n                || checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n            PartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSource.checkCorrectRestore = new int[counterSize];\n        } else {\n            PartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n        }\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture =\n                    FutureUtils.retryWithDelay(\n                            () -> client.triggerSavepoint(jobID, null),\n                            (int) deadline.timeLeft().getSeconds() / 10,\n                            Time.seconds(10),\n                            (throwable) -> true,\n                            TestingUtils.defaultScheduledExecutor());\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            int sumExp = 0;\n            int sumAct = 0;\n\n            if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            } else if (checkpointMethod\n                    == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n\n                sumExp *= parallelism2;\n            } else {\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            }\n\n            assertEquals(sumExp, sumAct);\n        } finally {\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":498,"status":"M"}],"commitId":"3b92d67017173547268673d71652a6bc98167abb","commitMessage":"@@@[FLINK-22593][tests] Waiting all tasks running before triggerSavepoint\n","date":"2021-06-30 17:43:35","modifiedFileCount":"3","status":"M","submitter":"Anton Kalashnikov"},{"authorTime":"2021-08-13 08:54:07","codes":[{"authorDate":"2021-08-13 08:54:07","commitOrder":18,"curCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID(), false);\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","date":"2021-08-23 20:24:35","endLine":289,"groupId":"101576","id":33,"instanceNumber":1,"isCurCommit":1,"methodName":"testSavepointRescalingKeyedState","params":"(booleanscaleOut@booleanderiveMaxParallelism)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/55/114557496406880db793aaf65bda729c91c226.src","preCode":"    public void testSavepointRescalingKeyedState(boolean scaleOut, boolean deriveMaxParallelism)\n            throws Exception {\n        final int numberKeys = 42;\n        final int numberElements = 1000;\n        final int numberElements2 = 500;\n        final int parallelism = scaleOut ? numSlots / 2 : numSlots;\n        final int parallelism2 = scaleOut ? numSlots : numSlots / 2;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism, maxParallelism, numberKeys, numberElements, false, 100);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            \r\n            assertTrue(\n                    SubtaskIndexFlatMapper.workCompletedLatch.await(\n                            deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS));\n\n            \r\n\n            Set<Tuple2<Integer, Integer>> actualResult = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n\n                expectedResult.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism, keyGroupIndex),\n                                numberElements * key));\n            }\n\n            assertEquals(expectedResult, actualResult);\n\n            \r\n            CollectionSink.clearElementsSet();\n\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            CompletableFuture<String> savepointPathFuture = client.triggerSavepoint(jobID, null);\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            client.cancel(jobID).get();\n\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            int restoreMaxParallelism =\n                    deriveMaxParallelism ? JobVertex.MAX_PARALLELISM_DEFAULT : maxParallelism;\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithKeyedState(\n                            parallelism2,\n                            restoreMaxParallelism,\n                            numberKeys,\n                            numberElements2,\n                            true,\n                            100);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            Set<Tuple2<Integer, Integer>> actualResult2 = CollectionSink.getElementsSet();\n\n            Set<Tuple2<Integer, Integer>> expectedResult2 = new HashSet<>();\n\n            for (int key = 0; key < numberKeys; key++) {\n                int keyGroupIndex = KeyGroupRangeAssignment.assignToKeyGroup(key, maxParallelism);\n                expectedResult2.add(\n                        Tuple2.of(\n                                KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(\n                                        maxParallelism, parallelism2, keyGroupIndex),\n                                key * (numberElements + numberElements2)));\n            }\n\n            assertEquals(expectedResult2, actualResult2);\n\n        } finally {\n            \r\n            CollectionSink.clearElementsSet();\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":191,"status":"M"},{"authorDate":"2021-08-13 08:54:07","commitOrder":18,"curCode":"    public void testSavepointRescalingPartitionedOperatorState(\n            boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n        final int parallelism = scaleOut ? numSlots : numSlots / 2;\n        final int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        int counterSize = Math.max(parallelism, parallelism2);\n\n        if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION\n                || checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n            PartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSource.checkCorrectRestore = new int[counterSize];\n        } else {\n            PartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n        }\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID(), false);\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture =\n                    FutureUtils.retryWithDelay(\n                            () -> client.triggerSavepoint(jobID, null),\n                            (int) deadline.timeLeft().getSeconds() / 10,\n                            Time.seconds(10),\n                            (throwable) -> true,\n                            TestingUtils.defaultScheduledExecutor());\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            int sumExp = 0;\n            int sumAct = 0;\n\n            if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            } else if (checkpointMethod\n                    == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n\n                sumExp *= parallelism2;\n            } else {\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            }\n\n            assertEquals(sumExp, sumAct);\n        } finally {\n        }\n    }\n","date":"2021-08-23 20:24:35","endLine":610,"groupId":"101576","id":34,"instanceNumber":2,"isCurCommit":1,"methodName":"testSavepointRescalingPartitionedOperatorState","params":"(booleanscaleOut@OperatorCheckpointMethodcheckpointMethod)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/55/114557496406880db793aaf65bda729c91c226.src","preCode":"    public void testSavepointRescalingPartitionedOperatorState(\n            boolean scaleOut, OperatorCheckpointMethod checkpointMethod) throws Exception {\n        final int parallelism = scaleOut ? numSlots : numSlots / 2;\n        final int parallelism2 = scaleOut ? numSlots / 2 : numSlots;\n        final int maxParallelism = 13;\n\n        Duration timeout = Duration.ofMinutes(3);\n        Deadline deadline = Deadline.now().plus(timeout);\n\n        ClusterClient<?> client = cluster.getClusterClient();\n\n        int counterSize = Math.max(parallelism, parallelism2);\n\n        if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION\n                || checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n            PartitionedStateSource.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSource.checkCorrectRestore = new int[counterSize];\n        } else {\n            PartitionedStateSourceListCheckpointed.checkCorrectSnapshot = new int[counterSize];\n            PartitionedStateSourceListCheckpointed.checkCorrectRestore = new int[counterSize];\n        }\n\n        try {\n            JobGraph jobGraph =\n                    createJobGraphWithOperatorState(parallelism, maxParallelism, checkpointMethod);\n            \r\n            StateSourceBase.canFinishLatch = new CountDownLatch(1);\n\n            final JobID jobID = jobGraph.getJobID();\n\n            client.submitJob(jobGraph).get();\n\n            \r\n            waitForAllTaskRunning(cluster.getMiniCluster(), jobGraph.getJobID());\n            \r\n            StateSourceBase.workStartedLatch.await();\n\n            CompletableFuture<String> savepointPathFuture =\n                    FutureUtils.retryWithDelay(\n                            () -> client.triggerSavepoint(jobID, null),\n                            (int) deadline.timeLeft().getSeconds() / 10,\n                            Time.seconds(10),\n                            (throwable) -> true,\n                            TestingUtils.defaultScheduledExecutor());\n\n            final String savepointPath =\n                    savepointPathFuture.get(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n\n            \r\n            StateSourceBase.canFinishLatch.countDown();\n            client.cancel(jobID).get();\n            while (!getRunningJobs(client).isEmpty()) {\n                Thread.sleep(50);\n            }\n\n            JobGraph scaledJobGraph =\n                    createJobGraphWithOperatorState(parallelism2, maxParallelism, checkpointMethod);\n\n            scaledJobGraph.setSavepointRestoreSettings(\n                    SavepointRestoreSettings.forPath(savepointPath));\n\n            submitJobAndWaitForResult(client, scaledJobGraph, getClass().getClassLoader());\n\n            int sumExp = 0;\n            int sumAct = 0;\n\n            if (checkpointMethod == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            } else if (checkpointMethod\n                    == OperatorCheckpointMethod.CHECKPOINTED_FUNCTION_BROADCAST) {\n                for (int c : PartitionedStateSource.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSource.checkCorrectRestore) {\n                    sumAct += c;\n                }\n\n                sumExp *= parallelism2;\n            } else {\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectSnapshot) {\n                    sumExp += c;\n                }\n\n                for (int c : PartitionedStateSourceListCheckpointed.checkCorrectRestore) {\n                    sumAct += c;\n                }\n            }\n\n            assertEquals(sumExp, sumAct);\n        } finally {\n        }\n    }\n","realPath":"flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":512,"status":"M"}],"commitId":"136bb52326922dfe70b3cc79fd463f9d6cfecc32","commitMessage":"@@@[FLINK-23811][tests] Handle finished subtasks in CommonTestUtils.waitForAllTaskRunning\n\nCommonTestUtils.waitForAllTaskRunning returns when all the subtasks are running AND\nthe job is running and not finished. However.  with FLIP-147.  subtasks may finish and\nthe job will still be running. So the method won't return and instead timeout.\n\nThis commit adds a flag to indicate whether to fail or proceed\nwhen a finished sub-task is encountered.\n","date":"2021-08-23 20:24:35","modifiedFileCount":"7","status":"M","submitter":"Roman Khachatryan"}]
