[{"authorTime":"2020-07-21 12:42:42","codes":[{"authorDate":"2020-07-21 12:42:42","commitOrder":1,"curCode":"    public void testGetEtlJobStatus(@Mocked BrokerUtil brokerUtil, @Mocked YarnClient client,\n                                    @Injectable ApplicationReport report)\n            throws IOException, YarnException, UserException {\n        new Expectations() {\n            {\n                YarnClient.createYarnClient();\n                result = client;\n                client.getApplicationReport((ApplicationId) any);\n                result = report;\n                report.getYarnApplicationState();\n                returns(YarnApplicationState.RUNNING, YarnApplicationState.FINISHED, YarnApplicationState.FINISHED);\n                report.getFinalApplicationStatus();\n                returns(FinalApplicationStatus.UNDEFINED, FinalApplicationStatus.FAILED, FinalApplicationStatus.SUCCEEDED);\n                report.getTrackingUrl();\n                result = trackingUrl;\n                report.getProgress();\n                returns(0.5f, 1f, 1f);\n                BrokerUtil.readFile(anyString, (BrokerDesc) any);\n                result = \"{'normal_rows': 10, 'abnormal_rows': 0, 'failed_reason': 'etl job failed'}\";\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n\n        \r\n        EtlStatus status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.RUNNING, status.getState());\n        Assert.assertEquals(50, status.getProgress());\n\n        \r\n        status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.CANCELLED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(\"etl job failed\", status.getDppResult().failedReason);\n\n        \r\n        status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.FINISHED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n        Assert.assertEquals(10, status.getDppResult().normalRows);\n        Assert.assertEquals(0, status.getDppResult().abnormalRows);\n    }\n","date":"2020-07-21 12:42:42","endLine":191,"groupId":"1044","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testGetEtlJobStatus","params":"(@MockedBrokerUtilbrokerUtil@@MockedYarnClientclient@@InjectableApplicationReportreport)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/02/a690bd3c72f24d0502cafe68e5d154e138cea4.src","preCode":"    public void testGetEtlJobStatus(@Mocked BrokerUtil brokerUtil, @Mocked YarnClient client,\n                                    @Injectable ApplicationReport report)\n            throws IOException, YarnException, UserException {\n        new Expectations() {\n            {\n                YarnClient.createYarnClient();\n                result = client;\n                client.getApplicationReport((ApplicationId) any);\n                result = report;\n                report.getYarnApplicationState();\n                returns(YarnApplicationState.RUNNING, YarnApplicationState.FINISHED, YarnApplicationState.FINISHED);\n                report.getFinalApplicationStatus();\n                returns(FinalApplicationStatus.UNDEFINED, FinalApplicationStatus.FAILED, FinalApplicationStatus.SUCCEEDED);\n                report.getTrackingUrl();\n                result = trackingUrl;\n                report.getProgress();\n                returns(0.5f, 1f, 1f);\n                BrokerUtil.readFile(anyString, (BrokerDesc) any);\n                result = \"{'normal_rows': 10, 'abnormal_rows': 0, 'failed_reason': 'etl job failed'}\";\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n\n        \r\n        EtlStatus status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.RUNNING, status.getState());\n        Assert.assertEquals(50, status.getProgress());\n\n        \r\n        status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.CANCELLED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(\"etl job failed\", status.getDppResult().failedReason);\n\n        \r\n        status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.FINISHED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n        Assert.assertEquals(10, status.getDppResult().normalRows);\n        Assert.assertEquals(0, status.getDppResult().abnormalRows);\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":143,"status":"B"},{"authorDate":"2020-07-21 12:42:42","commitOrder":1,"curCode":"    public void testKillEtlJob(@Mocked YarnClient client) throws IOException, YarnException {\n        new Expectations() {\n            {\n                YarnClient.createYarnClient();\n                result = client;\n                client.killApplication((ApplicationId) any);\n                times = 1;\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        try {\n            handler.killEtlJob(null, appId, loadJobId, resource);\n        } catch (Exception e) {\n            Assert.fail(e.getMessage());\n        }\n    }\n","date":"2020-07-21 12:42:42","endLine":215,"groupId":"8099","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testKillEtlJob","params":"(@MockedYarnClientclient)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/02/a690bd3c72f24d0502cafe68e5d154e138cea4.src","preCode":"    public void testKillEtlJob(@Mocked YarnClient client) throws IOException, YarnException {\n        new Expectations() {\n            {\n                YarnClient.createYarnClient();\n                result = client;\n                client.killApplication((ApplicationId) any);\n                times = 1;\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        try {\n            handler.killEtlJob(null, appId, loadJobId, resource);\n        } catch (Exception e) {\n            Assert.fail(e.getMessage());\n        }\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":194,"status":"B"}],"commitId":"ad17afef9139a9aeedeb2e92638e95886d515f14","commitMessage":"@@@[CodeRefactor] #4098 Make FE multi module (#4099)\n\nThis PR change the FE code structure to maven multi module structure. \nSee ISSUE: #4098 for more info.  such as How to resolve conflicts.","date":"2020-07-21 12:42:42","modifiedFileCount":"0","status":"B","submitter":"Mingyu Chen"},{"authorTime":"2020-08-27 12:08:55","codes":[{"authorDate":"2020-08-27 12:08:55","commitOrder":2,"curCode":"    public void testGetEtlJobStatus(@Mocked BrokerUtil brokerUtil, @Mocked Util util, @Mocked CommandResult commandResult,\n                                    @Mocked SparkYarnConfigFiles sparkYarnConfigFiles, @Mocked SparkLoadAppHandle handle)\n            throws IOException, UserException {\n\n        new Expectations() {\n            {\n                sparkYarnConfigFiles.prepare();\n                sparkYarnConfigFiles.getConfigDir();\n                result = \"./yarn_config\";\n\n                commandResult.getReturnCode();\n                result = 0;\n                commandResult.getStdout();\n                returns(runningReport, runningReport, failedReport, failedReport, finishReport, finishReport);\n\n                handle.getUrl();\n                result = trackingUrl;\n            }\n        };\n\n        new Expectations() {\n            {\n                Util.executeCommand(anyString, (String[]) any);\n                minTimes = 0;\n                result = commandResult;\n\n                BrokerUtil.readFile(anyString, (BrokerDesc) any);\n                result = \"{'normal_rows': 10, 'abnormal_rows': 0, 'failed_reason': 'etl job failed'}\";\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        new Expectations(resource) {\n            {\n                resource.getYarnClientPath();\n                result = Config.yarn_client_path;\n            }\n        };\n\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n\n        \r\n        EtlStatus status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.RUNNING, status.getState());\n        Assert.assertEquals(50, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n\n        \r\n        status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.CANCELLED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(\"etl job failed\", status.getDppResult().failedReason);\n\n        \r\n        status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.FINISHED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n        Assert.assertEquals(10, status.getDppResult().normalRows);\n        Assert.assertEquals(0, status.getDppResult().abnormalRows);\n    }\n","date":"2020-08-27 12:08:55","endLine":275,"groupId":"2150","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testGetEtlJobStatus","params":"(@MockedBrokerUtilbrokerUtil@@MockedUtilutil@@MockedCommandResultcommandResult@@MockedSparkYarnConfigFilessparkYarnConfigFiles@@MockedSparkLoadAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/90/4a8b1718ba000ea22d2065820fc378c2f9f4a1.src","preCode":"    public void testGetEtlJobStatus(@Mocked BrokerUtil brokerUtil, @Mocked YarnClient client,\n                                    @Injectable ApplicationReport report)\n            throws IOException, YarnException, UserException {\n        new Expectations() {\n            {\n                YarnClient.createYarnClient();\n                result = client;\n                client.getApplicationReport((ApplicationId) any);\n                result = report;\n                report.getYarnApplicationState();\n                returns(YarnApplicationState.RUNNING, YarnApplicationState.FINISHED, YarnApplicationState.FINISHED);\n                report.getFinalApplicationStatus();\n                returns(FinalApplicationStatus.UNDEFINED, FinalApplicationStatus.FAILED, FinalApplicationStatus.SUCCEEDED);\n                report.getTrackingUrl();\n                result = trackingUrl;\n                report.getProgress();\n                returns(0.5f, 1f, 1f);\n                BrokerUtil.readFile(anyString, (BrokerDesc) any);\n                result = \"{'normal_rows': 10, 'abnormal_rows': 0, 'failed_reason': 'etl job failed'}\";\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n\n        \r\n        EtlStatus status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.RUNNING, status.getState());\n        Assert.assertEquals(50, status.getProgress());\n\n        \r\n        status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.CANCELLED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(\"etl job failed\", status.getDppResult().failedReason);\n\n        \r\n        status = handler.getEtlJobStatus(null, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.FINISHED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n        Assert.assertEquals(10, status.getDppResult().normalRows);\n        Assert.assertEquals(0, status.getDppResult().abnormalRows);\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":210,"status":"M"},{"authorDate":"2020-08-27 12:08:55","commitOrder":2,"curCode":"    public void testKillEtlJob(@Mocked Util util, @Mocked CommandResult commandResult,\n                               @Mocked SparkYarnConfigFiles sparkYarnConfigFiles) throws IOException, UserException {\n        new Expectations() {\n            {\n                sparkYarnConfigFiles.prepare();\n                sparkYarnConfigFiles.getConfigDir();\n                result = \"./yarn_config\";\n            }\n        };\n\n        new Expectations() {\n            {\n                commandResult.getReturnCode();\n                result = 0;\n            }\n        };\n\n        new Expectations() {\n            {\n                Util.executeCommand(anyString, (String[]) any);\n                minTimes = 0;\n                result = commandResult;\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        new Expectations(resource) {\n            {\n                resource.getYarnClientPath();\n                result = Config.yarn_client_path;\n            }\n        };\n\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        try {\n            handler.killEtlJob(null, appId, loadJobId, resource);\n        } catch (Exception e) {\n            Assert.fail(e.getMessage());\n        }\n    }\n","date":"2020-08-27 12:08:55","endLine":365,"groupId":"2151","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testKillEtlJob","params":"(@MockedUtilutil@@MockedCommandResultcommandResult@@MockedSparkYarnConfigFilessparkYarnConfigFiles)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/90/4a8b1718ba000ea22d2065820fc378c2f9f4a1.src","preCode":"    public void testKillEtlJob(@Mocked YarnClient client) throws IOException, YarnException {\n        new Expectations() {\n            {\n                YarnClient.createYarnClient();\n                result = client;\n                client.killApplication((ApplicationId) any);\n                times = 1;\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        try {\n            handler.killEtlJob(null, appId, loadJobId, resource);\n        } catch (Exception e) {\n            Assert.fail(e.getMessage());\n        }\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":322,"status":"M"}],"commitId":"8c38c79104d8b3721bd7588e1de49ade95fd1e46","commitMessage":"@@@[SparkLoad]Use the yarn command to get status and kill the application (#4383)\n\nThis cl will use yarn command as follows to kill or get status of application running on YARN.\n\n```\nyarn --config confdir application <-kill | -status> <Application ID>\n```","date":"2020-08-27 12:08:55","modifiedFileCount":"8","status":"M","submitter":"xy720"},{"authorTime":"2020-08-27 12:08:55","codes":[{"authorDate":"2021-07-18 22:14:42","commitOrder":3,"curCode":"    public void testGetEtlJobStatus(@Mocked BrokerUtil brokerUtil, @Mocked Util util, @Mocked CommandResult commandResult,\n                                    @Mocked SparkYarnConfigFiles sparkYarnConfigFiles, @Mocked SparkLoadAppHandle handle)\n            throws IOException, UserException {\n\n        new Expectations() {\n            {\n                sparkYarnConfigFiles.prepare();\n                sparkYarnConfigFiles.getConfigDir();\n                result = \"./yarn_config\";\n\n                commandResult.getReturnCode();\n                result = 0;\n                commandResult.getStdout();\n                returns(runningReport, runningReport, failedReport, failedReport, finishReport, finishReport);\n\n                handle.getUrl();\n                result = trackingUrl;\n            }\n        };\n\n        new Expectations() {\n            {\n                Util.executeCommand(anyString, (String[]) any);\n                minTimes = 0;\n                result = commandResult;\n\n                BrokerUtil.readFile(anyString, (BrokerDesc) any, anyLong);\n                result = \"{'normal_rows': 10, 'abnormal_rows': 0, 'failed_reason': 'etl job failed'}\";\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        new Expectations(resource) {\n            {\n                resource.getYarnClientPath();\n                result = Config.yarn_client_path;\n            }\n        };\n\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n\n        \r\n        EtlStatus status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.RUNNING, status.getState());\n        Assert.assertEquals(50, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n\n        \r\n        status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.CANCELLED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(\"etl job failed\", status.getDppResult().failedReason);\n\n        \r\n        status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.FINISHED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n        Assert.assertEquals(10, status.getDppResult().normalRows);\n        Assert.assertEquals(0, status.getDppResult().abnormalRows);\n    }\n","date":"2021-07-18 22:14:42","endLine":276,"groupId":"10443","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"testGetEtlJobStatus","params":"(@MockedBrokerUtilbrokerUtil@@MockedUtilutil@@MockedCommandResultcommandResult@@MockedSparkYarnConfigFilessparkYarnConfigFiles@@MockedSparkLoadAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/a4/6263028699dd6bb47581a7a3afc1f4658111e5.src","preCode":"    public void testGetEtlJobStatus(@Mocked BrokerUtil brokerUtil, @Mocked Util util, @Mocked CommandResult commandResult,\n                                    @Mocked SparkYarnConfigFiles sparkYarnConfigFiles, @Mocked SparkLoadAppHandle handle)\n            throws IOException, UserException {\n\n        new Expectations() {\n            {\n                sparkYarnConfigFiles.prepare();\n                sparkYarnConfigFiles.getConfigDir();\n                result = \"./yarn_config\";\n\n                commandResult.getReturnCode();\n                result = 0;\n                commandResult.getStdout();\n                returns(runningReport, runningReport, failedReport, failedReport, finishReport, finishReport);\n\n                handle.getUrl();\n                result = trackingUrl;\n            }\n        };\n\n        new Expectations() {\n            {\n                Util.executeCommand(anyString, (String[]) any);\n                minTimes = 0;\n                result = commandResult;\n\n                BrokerUtil.readFile(anyString, (BrokerDesc) any);\n                result = \"{'normal_rows': 10, 'abnormal_rows': 0, 'failed_reason': 'etl job failed'}\";\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        new Expectations(resource) {\n            {\n                resource.getYarnClientPath();\n                result = Config.yarn_client_path;\n            }\n        };\n\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n\n        \r\n        EtlStatus status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.RUNNING, status.getState());\n        Assert.assertEquals(50, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n\n        \r\n        status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.CANCELLED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(\"etl job failed\", status.getDppResult().failedReason);\n\n        \r\n        status = handler.getEtlJobStatus(handle, appId, loadJobId, etlOutputPath, resource, brokerDesc);\n        Assert.assertEquals(TEtlState.FINISHED, status.getState());\n        Assert.assertEquals(100, status.getProgress());\n        Assert.assertEquals(trackingUrl, status.getTrackingUrl());\n        Assert.assertEquals(10, status.getDppResult().normalRows);\n        Assert.assertEquals(0, status.getDppResult().abnormalRows);\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":211,"status":"M"},{"authorDate":"2020-08-27 12:08:55","commitOrder":3,"curCode":"    public void testKillEtlJob(@Mocked Util util, @Mocked CommandResult commandResult,\n                               @Mocked SparkYarnConfigFiles sparkYarnConfigFiles) throws IOException, UserException {\n        new Expectations() {\n            {\n                sparkYarnConfigFiles.prepare();\n                sparkYarnConfigFiles.getConfigDir();\n                result = \"./yarn_config\";\n            }\n        };\n\n        new Expectations() {\n            {\n                commandResult.getReturnCode();\n                result = 0;\n            }\n        };\n\n        new Expectations() {\n            {\n                Util.executeCommand(anyString, (String[]) any);\n                minTimes = 0;\n                result = commandResult;\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        new Expectations(resource) {\n            {\n                resource.getYarnClientPath();\n                result = Config.yarn_client_path;\n            }\n        };\n\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        try {\n            handler.killEtlJob(null, appId, loadJobId, resource);\n        } catch (Exception e) {\n            Assert.fail(e.getMessage());\n        }\n    }\n","date":"2020-08-27 12:08:55","endLine":365,"groupId":"10443","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testKillEtlJob","params":"(@MockedUtilutil@@MockedCommandResultcommandResult@@MockedSparkYarnConfigFilessparkYarnConfigFiles)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/90/4a8b1718ba000ea22d2065820fc378c2f9f4a1.src","preCode":"    public void testKillEtlJob(@Mocked Util util, @Mocked CommandResult commandResult,\n                               @Mocked SparkYarnConfigFiles sparkYarnConfigFiles) throws IOException, UserException {\n        new Expectations() {\n            {\n                sparkYarnConfigFiles.prepare();\n                sparkYarnConfigFiles.getConfigDir();\n                result = \"./yarn_config\";\n            }\n        };\n\n        new Expectations() {\n            {\n                commandResult.getReturnCode();\n                result = 0;\n            }\n        };\n\n        new Expectations() {\n            {\n                Util.executeCommand(anyString, (String[]) any);\n                minTimes = 0;\n                result = commandResult;\n            }\n        };\n\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        new Expectations(resource) {\n            {\n                resource.getYarnClientPath();\n                result = Config.yarn_client_path;\n            }\n        };\n\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        try {\n            handler.killEtlJob(null, appId, loadJobId, resource);\n        } catch (Exception e) {\n            Assert.fail(e.getMessage());\n        }\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":322,"status":"N"}],"commitId":"a4b1622cebb10e9fbce0c9e5b4c4598f0bc08e50","commitMessage":"@@@[HttpV2] Add more httpv2 APIs (#6210)\n\n1. /api/cluster_overview to view some statistic info of the cluster\n2. /api/meta/ to view the database/table schema\n3. /api/import/file_review to review the file content with format CSV or PARQUET.","date":"2021-07-18 22:14:42","modifiedFileCount":"5","status":"M","submitter":"Mingyu Chen"}]
