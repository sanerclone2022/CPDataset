[{"authorTime":"2020-07-21 12:42:42","codes":[{"authorDate":"2020-07-21 12:42:42","commitOrder":1,"curCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                 @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                returns(null, null, appId);\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.RUNNING);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n        Assert.assertEquals(handle, attachment.getHandle());\n    }\n","date":"2020-07-21 12:42:42","endLine":114,"groupId":"6087","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSubmitEtlJob","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableSparkAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/02/a690bd3c72f24d0502cafe68e5d154e138cea4.src","preCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                 @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                returns(null, null, appId);\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.RUNNING);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n        Assert.assertEquals(handle, attachment.getHandle());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":87,"status":"B"},{"authorDate":"2020-07-21 12:42:42","commitOrder":1,"curCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                       @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                result = null;\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.FAILED);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n    }\n","date":"2020-07-21 12:42:42","endLine":140,"groupId":"6087","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSubmitEtlJobFailed","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableSparkAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/02/a690bd3c72f24d0502cafe68e5d154e138cea4.src","preCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                       @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                result = null;\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.FAILED);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"B"}],"commitId":"ad17afef9139a9aeedeb2e92638e95886d515f14","commitMessage":"@@@[CodeRefactor] #4098 Make FE multi module (#4099)\n\nThis PR change the FE code structure to maven multi module structure. \nSee ISSUE: #4098 for more info.  such as How to resolve conflicts.","date":"2020-07-21 12:42:42","modifiedFileCount":"0","status":"B","submitter":"Mingyu Chen"},{"authorTime":"2020-07-27 09:48:41","codes":[{"authorDate":"2020-07-27 09:48:41","commitOrder":2,"curCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                 @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                returns(null, null, appId);\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.RUNNING);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n        Assert.assertEquals(handle, attachment.getHandle());\n    }\n","date":"2020-07-27 09:48:41","endLine":133,"groupId":"6087","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSubmitEtlJob","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableSparkAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/e0/11062fbbc0e0c4b6df9afc20a9e6eb05938087.src","preCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                 @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                returns(null, null, appId);\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.RUNNING);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n        Assert.assertEquals(handle, attachment.getHandle());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"M"},{"authorDate":"2020-07-27 09:48:41","commitOrder":2,"curCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                       @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                result = null;\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.FAILED);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n    }\n","date":"2020-07-27 09:48:41","endLine":166,"groupId":"6087","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSubmitEtlJobFailed","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableSparkAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/e0/11062fbbc0e0c4b6df9afc20a9e6eb05938087.src","preCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                       @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                result = null;\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.FAILED);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":136,"status":"M"}],"commitId":"f2c9e1e53432ccf5f763c5e28934b4797c90f9bd","commitMessage":"@@@[Spark Load]Create spark load's repository in HDFS for dependencies (#4163)\n\n\n Resume\nWhen users use spark load.  they have to upload the dependent jars to hdfs every time.\nThis cl will add a self-generated repository under working_dir folder in hdfs for saving dependecies of spark dpp programe and spark platform.\nNote that.  the dependcies we upload to repository include:\n1?`spark-dpp.jar`\n2?`spark2x.zip`\n1 is the dpp library which built with spark-dpp submodule. See details about spark-dpp submodule in pr #4146 .\n2 is the spark2.x.x platform library which contains all jars in $SPARK_HOME/jars\n\n**The repository structure** will be like this:\n\n```\n__spark_repository__/\n    |-__archive_1_0_0/\n    |        |-__lib_990325d2c0d1d5e45bf675e54e44fb16_spark-dpp.jar\n    |        |-__lib_7670c29daf535efe3c9b923f778f61fc_spark-2x.zip\n    |-__archive_2_2_0/\n    |        |-__lib_64d5696f99c379af2bee28c1c84271d5_spark-dpp.jar\n    |        |-__lib_1bbb74bb6b264a270bc7fca3e964160f_spark-2x.zip\n    |-__archive_3_2_0/\n    |        |-...\n```\n\nThe followinng conditions will force fe to upload dependencies:\n1?When fe find its dppVersion is absent in repository.\n2?The MD5 value of remote file does not match the local file.\nBefore Fe uploads the dependencies.  it will create an archive directory with name `__archive_{dppVersion}` under the repository.","date":"2020-07-27 09:48:41","modifiedFileCount":"5","status":"M","submitter":"xy720"},{"authorTime":"2020-08-27 12:08:55","codes":[{"authorDate":"2020-08-27 12:08:55","commitOrder":3,"curCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n                                 @Mocked SparkLoadAppHandle handle ) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.launch();\n                result = process;\n                handle.getAppId();\n                result = appId;\n                handle.getState();\n                result = SparkLoadAppHandle.State.RUNNING;\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n    }\n","date":"2020-08-27 12:08:55","endLine":174,"groupId":"6087","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSubmitEtlJob","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableProcessprocess@@MockedSparkLoadAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/90/4a8b1718ba000ea22d2065820fc378c2f9f4a1.src","preCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                 @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                returns(null, null, appId);\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.RUNNING);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n        Assert.assertEquals(handle, attachment.getHandle());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"},{"authorDate":"2020-08-27 12:08:55","commitOrder":3,"curCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n                                       @Mocked SparkLoadAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.launch();\n                result = process;\n                handle.getAppId();\n                result = appId;\n                handle.getState();\n                result = SparkLoadAppHandle.State.FAILED;\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n    }\n","date":"2020-08-27 12:08:55","endLine":207,"groupId":"6087","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testSubmitEtlJobFailed","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableProcessprocess@@MockedSparkLoadAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/90/4a8b1718ba000ea22d2065820fc378c2f9f4a1.src","preCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n                                       @Injectable SparkAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.startApplication((SparkAppHandle.Listener) any);\n                result = handle;\n                handle.getAppId();\n                result = null;\n                handle.getState();\n                returns(State.CONNECTED, State.SUBMITTED, State.FAILED);\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"M"}],"commitId":"8c38c79104d8b3721bd7588e1de49ade95fd1e46","commitMessage":"@@@[SparkLoad]Use the yarn command to get status and kill the application (#4383)\n\nThis cl will use yarn command as follows to kill or get status of application running on YARN.\n\n```\nyarn --config confdir application <-kill | -status> <Application ID>\n```","date":"2020-08-27 12:08:55","modifiedFileCount":"8","status":"M","submitter":"xy720"},{"authorTime":"2020-09-06 20:32:47","codes":[{"authorDate":"2020-09-06 20:32:47","commitOrder":4,"curCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n                                 @Mocked SparkLoadAppHandle handle ) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.launch();\n                result = process;\n                handle.getAppId();\n                result = appId;\n                handle.getState();\n                result = SparkLoadAppHandle.State.RUNNING;\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, handle, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n    }\n","date":"2020-09-06 20:32:47","endLine":174,"groupId":"10441","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testSubmitEtlJob","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableProcessprocess@@MockedSparkLoadAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/73/df62834bab547d0307458fba713f6377d11124.src","preCode":"    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n                                 @Mocked SparkLoadAppHandle handle ) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.launch();\n                result = process;\n                handle.getAppId();\n                result = appId;\n                handle.getState();\n                result = SparkLoadAppHandle.State.RUNNING;\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n\n        \r\n        Assert.assertEquals(appId, attachment.getAppId());\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"},{"authorDate":"2020-09-06 20:32:47","commitOrder":4,"curCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n                                       @Mocked SparkLoadAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.launch();\n                result = process;\n                handle.getAppId();\n                result = appId;\n                handle.getState();\n                result = SparkLoadAppHandle.State.FAILED;\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, handle, attachment);\n    }\n","date":"2020-09-06 20:32:47","endLine":207,"groupId":"10441","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testSubmitEtlJobFailed","params":"(@MockedBrokerUtilbrokerUtil@@MockedSparkLauncherlauncher@@InjectableProcessprocess@@MockedSparkLoadAppHandlehandle)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-incubatordoris-10-0.7/blobInfo/CC_OUT/blobs/73/df62834bab547d0307458fba713f6377d11124.src","preCode":"    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n                                       @Mocked SparkLoadAppHandle handle) throws IOException, LoadException {\n        new Expectations() {\n            {\n                launcher.launch();\n                result = process;\n                handle.getAppId();\n                result = appId;\n                handle.getState();\n                result = SparkLoadAppHandle.State.FAILED;\n            }\n        };\n\n        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n        SparkResource resource = new SparkResource(resourceName);\n        new Expectations(resource) {\n            {\n                resource.prepareArchive();\n                result = archive;\n            }\n        };\n\n        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n        sparkConfigs.put(\"spark.master\", \"yarn\");\n        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n    }\n","realPath":"fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java","repoName":"incubatordoris","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"M"}],"commitId":"8f10317e0d111aa6c1b5cb63b888a0965c1d49f5","commitMessage":"@@@[Spark load][Bug] fix that cancelling a spark load in the `PENDING` phase will not succeed (#4536)\n\n","date":"2020-09-06 20:32:47","modifiedFileCount":"6","status":"M","submitter":"xy720"}]
